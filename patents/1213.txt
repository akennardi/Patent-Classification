                                         ABSTRACT
The present     invention        provides     a   method   for managing        the  wiring
and  growth    of    a   direct      interconnect        network    implemented       on    a
torus   or    higher       radix      interconnect        structure       based     on     an
architecture that        replaces the Network Interface Card                    (NIC) with
PCIe  switching      cards      housed    in   the   server.     Also    provided      is   a
passive   patch      panel       for    use     in   the    implementation         of     the
interconnect,      comprising:        a   passive     backplane     that     houses    node
to  node   connectivity          for    the    interconnect;      and     at    least     one
connector    board     plugged       into    the   passive    backplane        comprising
multiple    connectors.          The   multiple       connectors       are    capable      of
receiving an     interconnecting           plug to maintain        the    continuity       of
the  torus   or   higher       radix   topology      when   not  fully     enabled.       The
PCIe   card   for     use     in   the    implementation       of    the    interconnect
comprises:     at    least       4   electrical       or   optical      ports     for     the
interconnect;      a    local      switch;      a  processor      with     RAM    and     ROM
memory;  and a PCI interface.

                      9/16
              Top of the Rack Patch Panel
                 Server
   31
   336
       SERVER
       Front                              Back
                  FIGURE 14
33
                                               38
                  FIGURE 15

   METHOD AND APPARATUS TO MANAGE THE DIRECT INTERCONNECT SWITCH
   WIRING AND GROWTH IN COMPUTER NETWORKS
 5 FIELD OF THE INVENTION
   The  present     invention      relates      to  computer       network      topology        and
   architecture.       In   particular,      the present         invention      relates       to a
   method   and    apparatus      for    managing      the    wiring     and     growth       of   a
 0 direct   interconnect       switch implemented            on,   for   example,         a torus
   or higher radix wiring structure.
   BACKGROUND OF THE INVENTION
 5 Each document,        reference,      patent    application        or patent          cited in
   this text is      expressly      incorporated herein            in   their entirety by
   reference,     which means that         it    should be read and considered by
   the reader as part         of this text.          That the document, reference,
   patent  application        or patent       cited in     this text is          not repeated
 0 in this text is merely for reasons of conciseness.
   The  following       discussion     of the background            to the      invention        is
   intended to facilitate an understanding of the present invention
   only.     It   should be      appreciated       that     the    discussion         is   not an
25 acknowledgement        or admission        that any      of the      material         referred
   to was published,         known or part of the common                general knowledge
   of the person skilled in            the art      in   any     jurisdiction         as at     the
   priority date of the invention.
30 The  term Data       Centers     (DC)   generally       refers     to   facilities         used
   to house     large     computer    systems      (often contained           on racks        that
   house   the      equipment)       and     their     associated        components,            all
   connected     by   an   enormous      amount    of    structured       cabling.           Cloud
                                             1

   Data      Centers       (CDC)      is     a   term     used     to   refer       to      large,         generally
   off-premise facilities that similarly store an entity's data.
   Network        switches         are       computer         networking           apparatus              that        link
 5 network        devices       for      communication/processing                    purposes.               In     other
   words,       a  switch is         a telecommunication                 device        that        is    capable of
   receiving          a    message           from      any      device        connected               to      it,       and
   transmitting            the     message          to    a    specific          device         for       which         the
   message        was     to   be     relayed.          A   network         switch        is      also        commonly
 0 referred         to   as    a    multi-port           network       bridge        that         processes             and
   routes       data.        Here,        by    port,      we    are    referring            to      an    interface
    (outlet        for     a     cable         or     plug)       between         the        switch           and       the
   computer/server/CPU to which it is attached.
 5 Today,       DCs     and   CDCs        generally         implement          data      center          networking
   using      a    set    of   layer         two     switches.        Layer       two       switches            process
   and     route      data     at    layer        2,    the    data      link      layer,           which        is     the
   protocol        layer     that       transfers         data between             nodes         (e.g.        servers)
   on the same           local      area       network      or adjacent           nodes         in     a   wide area
 0 network.           A   key     problem         to    solve,       however,         is      how       to     build       a
   large      capacity        computer           network        that     is      able      to      carry        a     very
   large      aggregate          bandwidth            (hundreds        of      TB)     containing               a     very
   large        number       of      ports           (thousands),            that        requires              minimal
   structure         and    space        (i.e.      minimizing         the      need      for       a    large        room
25 to    house numerous            cabinets          with     racks     of      cards),         that       is     easily
   scalable, and that may assist in minimizing power consumption.
   The      traditional           network          topology          implementation                 is      based         on
   totally        independent            switches         organized          in    a     hierarchical                 tree
   structure         as    shown       in      Figure      1.    Core     switch        2     is      a   very        high
30 speed, low count port with a very large switching capacity.                                                          The
   second layer is implemented using Aggregation switch                                                 4, a medium
   capacity        switch      with       a    larger      number       of ports,            while        the       third
                                                        2

   layer         is      implemented              using          lower       speed,          large         port         count
    (forty/forty-eight),                     low capacity            Edge switches               6.    Typically          the
   Edge       switches           are     layer      two,       whereas        the     Aggregation             ports       are
   layer        two     and/or         three,       and      the    Core      switch        is     typically            layer
 5 three.          This        implementation                provides          any       server         8    to       server
   connectivity                in      a   maximum          of     six     hop       links        in      the     example
   provided          (three hops              up    to    the     core     switch         2   and     three       down     to
   the destination server 8).                            Such a hierarchical                   structure is also
   usually           duplicated               for      redundancy-reliability                        purposes.            For
 0 example,          with       reference          to Figure          1,    without         duplication              if   the
   right-most Edge                  switch 6 fails, then there is no                                connectivity to
   the        right-most             servers         8.      In     the     least,          core        switch          2  is
   duplicated since the                       failure of the core switch 2 would generate
   a    total       data       center         connectivity           failure.           For     reasons         that      are
 5 apparent,           this       method        has     significant            limitations            in     addressing
   the      challenges             of     the    future         DC   or    CDC.       For      instance,          because
   each       switch        is     completely           self-contained,                this      adds      complexity,
   significant              floor-space            utilization,            complex          cabling          and     manual
   switches           configuration/provisioning                            that         is     prone         to        human
 0 error, and increased energy costs.
   Many        attempts           have       been       made,       however,           to     improve          switching
   scalability,               reliability,             capacity        and       latency        in    data       centers.
   For      instance,            efforts        have      been      made      to    implement           more      complex
25 switching           solutions            by    using       a   unified         control         plane       (e.g.       the
   QFabric          System         switch       from Juniper            Networks;           see,       for    instance,
   http://www.juniper.net/us/en/products
   services/switching/qfabric-system/),                                    but        such       a     system           still
   uses       and maintains the traditional                            hierarchical architecture.                          In
30 addition, given the exponential increase in the number of                                                          system
   users       and data to be stored,                        accessed,          and processed,               processing
   power       has     become          the most         important         factor         when      determining            the
                                                             3

   performance         requirements              of    a     computer        network        system.             While
   server       performance            has    continually             improved,       one     server         is    not
   powerful        enough         to     meet     the       needs.         This    is     why     the       use      of
   parallel        processing            has     become         of     paramount        importance.             As     a
 5 result,       what      was       predominantly             north-south          traffic         flows,         has
   now      primarily       become         east-west         traffic        flows,      in    many       cases       up
   to      80%.    Despite           this      change        in      traffic       flows,         the       network
   architectures           haven't          evolved        to    be     optimal      for    this      model.         It
   is      therefore       still         the    topology           of     the    communication              network
 0  (which       interconnects                the      computing             nodes        (servers))             that
   determines          the        speed        of     interactions               between         CPUs         during
   parallel processing communication.
   The      need   for     increased          east-west           traffic        communications              led     to
 5 the       creation       of       newer,        flatter           network       architectures,                e.g.
   toroidal/torus              networks.             A      torus         interconnect           system         is     a
   network       topology           for     connecting           network         nodes      (servers)           in     a
   mesh-like manner in parallel computer                                  systems.        A torus topology
   can have nodes            arranged in           2,     3,    or more        (N)  dimensions           that      can
 0 be       visualized           as      an     array        wherein          processors/servers                   are
   connected         to     their          nearest         neighbor          processors/servers,                   and
   wherein       processors/servers                 on      opposite         edges     of    the      array        are
   connected.           In     this       way,    each       node      has    2N   connections            in     a   N
   dimensional torus configuration                            (Figure 2 provides an example of
25 a 3-D torus interconnect).                      Because each node in a torus topology
   is     connected       to      adjacent        ones       via     short      cabling,        there        is    low
   network        latency          during       parallel           processing.           Indeed,          a     torus
   topology        provides           access      to     any      node      (server)        with      a    minimum
   number of hops.              For example, a four dimension torus implementing
30 a   3    x  3  x  3    x    4    structure         (108      nodes)       requires       on     average         2.5
   hops in order to provide any to any                                 connectivity. Unfortunately,
   large      torus    network          implementations                have not      been      practical           for
                                                       4

   commercial           deployment            in       DCs      or       CDCs         because        large
   implementations            can take years           to build,        cabling        can    be  complex
   (2N    connections           for   each      node),       and    they      can      be    costly    and
   cumbersome       to    modify      if     expansion       is    necessary.          However,     where
 5 the    need     for      processing         power      has     outweighed          the     commercial
   drawbacks,           the       implementation               of      torus          topologies         in
   supercomputers           has   been     very      successful.        In   this       respect,     IBM's
   Blue     Gene    supercomputer             provides        an    example         of    a    3-D  torus
   interconnect          network       wherein         64    cabinets        house        65,536    nodes
 0 (131,072     CPUs)       to provide petaFLOPs               processing power              (see Figure
   3    for     an       illustration),               while       Fujitsu's           PRIMEHPC        FX10
   supercomputer          system     is     an   example       of   a   6-D    torus       interconnect
   housed in       1,024      racks     comprising        98,304      nodes).        While     the  above
   examples        dealt        with      a     torus       topology,          they       are     equally
 5 applicable to other flat network topologies.
   Embodiments of the present invention seek to overcome the
   deficiencies in such prior art network topologies by providing a
   system and architecture that may be beneficial and practical for
 0 commercial deployment in DCs and CDCs.
   SUMMARY OF THE INVENTION
   According       to     a     first     principal         aspect,        there       is   provided       a
25 Peripheral       Component        Interconnect           Express        (PCIe)      card    housed    in
   a  server     and     having      connectivity          by    cable      to    a   connector       on   a
   passive     patch panel          for    use    in    the   implementation             of a    torus   or
   higher radix interconnect, said card comprising:
30 at  least     4   electrical          or optical        ports      for    the     torus     or  higher
   radix interconnect;
                                                   5

   a local switch;
   a processor       with Random Access         Memory    (RAM)    and Read Only Memory
    (ROM) memory;       and
 5
   a Peripheral Component Interconnect                (PCI) interface,
   and    wherein      the   PCIe    card    has   connectivity         to    an     electronic
   device on the passive patch panel whereby information about                                 said
 0 patch     panel    or   the  connector      may  be   relayed       at   the      request      of
   software running on said card.
   In   one     embodiment,     the    information     relates       to    one     or     more    of
   manufacturing         date   of    the   patch   panel,      version        of      the    patch
 5 panel,      configuration       of   the   patch   panel,      patch      panel        ID,    and
   connector ID.
   In    another      embodiment,        the    local    switch       is     electrical           or
   optical.
 0
   In  a   further     embodiment,      the   PCIe  card is      capable       of supporting
   port    to    Peripheral     Component      Interconnect         (PCI)     traffic,         hair
   pinning traffic, and transit with add/drop traffic.
25 In one embodiment, the PCIe              card is   further capable of combining
   multiple optical wavelengths on a same fiber.
   According       to   a   second     principal    aspect,       there      is     provided        a
   Peripheral       Component    Interconnect       Express      (PCIe)     card        housed    in
30 a   server      having     connectivity       by  cable      to    a   connector           on    a
   passive       patch    panel   in    a  torus    or   higher       radix      interconnect
   structure, said card comprising:
                                             6

         a Peripheral Component Interconnect             (PCI) connection;
         a processor with Random Access Memory             (RAM) and Read Only
 5       Memory   (ROM) memory;
         a switch;
         at least one Ethernet PHY interface device;              and
 0
         at least eight interface ports to provide for the
         implementation of up to a four dimension torus direct
         interconnect network,
 5       and wherein the PCIe card has connectivity to an electronic
         device on the passive patch panel whereby information about
         said patch panel or the connector may be relayed at the
         request of software running on said card.
 0 In  one   embodiment,   the    information    relates      to   one    or    more    of
   manufacturing    date   of    the   patch   panel,      version    of      the    patch
   panel,    configuration    of   the   patch  panel,      patch   panel        ID,   and
   connector ID.
25 In  another   embodiment,     the   PCIe  card    is    capable   of      supporting
   port   to  Peripheral   Component      Interconnect       (PCI)   traffic,         hair
   pinning traffic, and transit with add/drop traffic.
   In a further embodiment, the switch is electrical.
30
   In one embodiment, the switch is optical.
                                        7

   According    to  a    third       principal    aspect,      there      is     provided      a
   Peripheral Component Interconnect Express                 (PCIe) card for use in
   optical wavelength multiplexing comprising:
 5 a Peripheral Component Interconnect               (PCI) connection;
   a processor with Random Access Memory               (RAM) and Read Only Memory
    (ROM) memory;
 0 a photonic switch;
   at least one Ethernet PHY interface device;
   a   first    passive        optical      mux-demux     comprising             fibers      and
 5 combining   multiple      optical     wavelengths     on   the    fibers,        said mux
   demux being linked to optical receivers;               and
   a   second   passive       optical      mux-demux     comprising           fibers,       said
   second   passive   optical        mux-demux   linked    to   optical        transmitters
 0 on    different     wavelengths          and     combining        multiple           optical
   wavelengths on the fibers,
   said    fibers    interconnecting           a    plurality       of       mux-demux        to
   implement an optical direct interconnect torus network.
25
   In  one  embodiment,       the    PCIe   card  is   capable     of    supporting         port
   to Peripheral    Component Interconnect             (PCI) traffic, hair pinning
   traffic, and transit with add/drop traffic.
30 According to a     fourth principal         aspect, there is provided use of
   any   embodiment    of     a   Peripheral     Component      Interconnect            Express
    (PCIe)  card   arranged         in  accordance      with    the     third       principal
                                            8

   aspect       wherein         the     PCIe      card    is    capable           of     supporting           port      to
   Peripheral            Component          Interconnect            (PCI)         traffic,           hair      pinning
   traffic,          and     transit         with      add/drop          traffic,             or     as     described
   herein,        with     a   top     of a      rack    switch       in    a     torus       structure          suited
 5 to       implement            hybrid         compute         servers             and        storage           server
   configurations,
   wherein said top of the                      rack switch provides                    connectivity to the
   servers        and     connectivity            to other top            of a        rack      switches        in    the
 0 torus       structure          where      the     top     of    the       rack        switches         are      torus
   nodes.
   In      one     aspect,         the     present        invention            provides            a    method        for
   managing         the wiring           and growth          of a     direct          interconnect             network
 5 implemented            on    a    torus      or  higher        radix        interconnect              structure,
   comprising:           populating          a passive patch panel                      comprising           at    least
   one       connector             board        having        multiple              connectors              with        an
   interconnect             plug       at     each      of      said        connectors;               removing          an
   interconnect            plug      from a       connector        and      replacing            said plug with
 0 a connecting cable attached to a PCIe card housed in a server to
   add      said       server         to     the     interconnect               structure;              discovering
   connectivity            of      the    server       to    the     interconnect                structure;           and
   discovering           topology         of the       interconnect             structure            based      on    the
   servers added to the interconnect structure.
25
   In      another        aspect,        the      present        invention              provides          a    passive
   patch      panel       for     use    in    the    implementation                of     a   torus       or   higher
   radix      interconnect,             comprising:           a  passive            backplane          that     houses
   node      to       node        connectivity            for     the        torus           or     higher         radix
30 interconnect;             and     at   least     one      connector          board plugged                into     the
   passive         backplane          comprising          multiple           connectors.              The      passive
   patch        panel        may       be     electrical,            optical,              or      a     hybrid         of
                                                        9

   electrical      and     optical.     The     optical         passive      patch      panel     is
   capable    of   combining       multiple       optical       wavelengths         on   the    same
   fiber.   Each     of     the   multiple      connectors          of   the     at    least     one
   connector     board     is  capable    of   receiving          an interconnecting            plug
 5 that  may    be  electrical       or  optical,          as   appropriate,         to maintain
   the continuity of the torus or higher radix topology.
   In yet another aspect, the present invention provides a PCIe
   card for use in the implementation of a torus or higher radix
 0 interconnect, comprising:           at least 4 electrical or optical ports
   for the torus or higher radix interconnect;                       a local switch;          a
   processor with RAM and ROM memory;                  and a PCI interface. The
   local switch may be electrical or optical. The PCIe card is
   capable of supporting port to PCI traffic, hair pinning traffic,
 5 and transit with add/drop traffic. The PCIe card is                            further
   capable of combining multiple optical wavelengths on the same
   fiber.
   BRIEF DESCRIPTION OF THE DRAWINGS
 0
   The embodiment of the           invention will now be described, by way of
   example, with reference to the accompanying drawings in which:
   FIGURE   1    displays       a  high   level        view     of   the   traditional          data
25 center network implementation              (Prior art);
   FIGURE     2    displays        a    diagram           of    a     3-dimensional           torus
   interconnect having 8 nodes             (Prior Art);
30 FIGURE  3    displays       a  diagram     showing         the   hierarchy        of   the    IBM
   Blue  Gene processing units           employing a torus architecture                      (Prior
   Art);
                                            10

   FIGURE  4   displays    a   high  level    diagram      of   a  3D  and     4D    torus
   structure according to an embodiment of the present invention;
 5 FIGURE 5 displays a diagram for a 36             node 2-D torus according to
   an  embodiment    of  the    present    invention       as   an   easy    to    follow
   example of the network interconnect;
   FIGURE  6  displays    a   three  dimensional         configuration     of    the    2-D
 0 configuration     shown     in   Fig.    5   replicated        three     times       and
   interconnected on the third dimension;
   FIGURE 7   displays   a   wiring   diagram of        the node   connectivity         for
   the 2-D torus shown in Fig. 5;
 5
   FIGURE 8   displays   a   wiring   diagram of        the node   connectivity         for
   the 3-D torus shown in Fig. 6;
   FIGURE 9   displays   a   diagram of the passive           backplane    of the Top
 0 of the Rack Patch Panel        (TPP) that    implements the wiring for the
   direct  interconnect       network    of   an    embodiment      of   the     present
   invention;
   FIGURE   10   displays     the   TPP   and    interconnecting         plug      of     an
25 embodiment of the present invention;
   FIGURE 11   displays the rear view of the passive backplane of the
   TPP with   the unpowered      integrated    circuits       used  to  identify        the
   connector ID and the patch panel ID,            and the PCIe       card connected
30 to the TPP;
                                       11

   FIGURE    12    displays     an   alternative        embodiment     of   the    passive
   backplane of the TPP;
   FIGURE     13    displays     a   high    level       view    of   an    optical    TPP
 5 implementation of an embodiment of the present invention;
   FIGURE   14    displays    a   high   level      view   of  a   data   center    server
   rack  with    a  TPP  implementation       in     accordance    with   an   embodiment
   of the present invention;
 0
   FIGURE 15     displays   a   high   level    view of a       hybrid implementation
   of  a  torus     toplogy    with   nodes    implemented       by   Top   of   the  Rack
   switches and PCIe cards housed in the server;
 5 FIGURE 16 displays a block diagram of a PCIe                    card implementation
   in accordance with an embodiment of the present invention;
   FIGURE 17     displays   the packet      traffic       flow suported      by the   PCIe
   card shown in Figure 16;
 0
   FIGURE   18   displays    a   block   diagram of        a  PCIe  card   with    optical
   multiwavelengths      in   accordance     with an embodiment          of the present
   invention;
25 FIGURE 19 displays a high level view of a TPP having a passive
   optical multiwavelengths implementation of an embodiment of the
   present invention;
   FIGURES     20a   to   20c    displays     the     pseudocode      to   generate    the
30 netlist for the wiring of a 4D torus structure;
   FIGURE 21 displays the connectors installed on the TPP;                      and
                                          12

   FIGURE  22  is  the   rear view  of  the connector  board  of  the  TPP
   with unpowered integrated circuits used to      identify connector   ID
   and patch panel ID.
 5
   DETAILED DESCRIPTION OF THE INVENTION
   An embodiment of the present invention uses a torus mesh or
 0 higher radix wiring to implement direct interconnect switching
   for data center applications. Such architecture is capable of
   providing a high performance    flat layer 2/3 network to
   interconnect tens of thousands of servers in a single switching
   domain.
 5
   With reference to Figure 4, the torus used is multidimensional
   (i.e. 3D, 4D,   etc.),  in order to promote efficiency of routing
   packets across the structure     (although even a single dimensional
   torus can be used in certain deployments).      In this respect,
 0 there is a minimum number of hops for any to any connectivity
   (e.g. a four dimension torus implementing a 3 x 3 x 3 x 4
   structure  (108 nodes) requires on average only 2.5 hops in order
   to provide any to any connectivity).     Each node 10  (server) can
   be visualized as being connected on each dimension in a ring
25 connection   (12, 14, 16, and 18) because the nodes 10    (servers)
   are connected to their nearest neighbor nodes 10      (servers), as
   well as nodes 10    (servers) on opposite edges of the structure.
   Each node 10 thereby has 2N connections in the N-dimensional
   torus configuration. The ring connection itself can be
30 implemented as an electrical interconnect or as an optical
   interconnect, or a combination of both electrical and optical
   interconnect.
                                   13

   One problem to be addressed in such a topology, however, is how
   to reduce deployment complexity by promoting wiring
   simplification and simplicity when adding new nodes in the
 5 network without impacting the existing implementation. This      is
   one aspect of the present invention, and this disclosure
   addresses the wiring issues when implementing large torus or
   higher radix structures.
 0 Figure 5 displays a simple 2D torus wiring diagram for a 6 x 6
   thirty-six node configuration for ease of explanation.      As
   shown, the structure is a folded 2D torus wherein the length of
   each connection  (12, 13) is equivalent throughout. Each node 10
   in this diagram represents a server interconnected via a PCIe
 5 switch card 41  (shown in Figure 16 for instance) that is housed
   in the server.
   Figure 6 displays a three dimensional configuration build using
   the 2D configuration of Figure 5, but replicated three times and
 0 interconnected on the third dimension.
   Figure 7 displays the wiring diagram for the two dimensional
   torus structure shown in Figure 5.  In the implementation shown,
   each of the 36 nodes 10 has connectors 21    (which can, for
25 instance, be a Very High Density Cable Interconnect VHDCI
   connector supplied by Molex or National Instruments, etc.)      with
   four connections  (north (N), south  (S), east  (E),  west (W))  that
   provide the switch wiring when the cable from the PCIe card 41
   (not shown) is plugged in. In order to simplify the wiring, the
30 connectors 21 are interconnected in a passive backplane 200       (as
   shown in Figure 9) that is housed by a Top of the rack Patch
   Panel  (TPP) 31  (as shown in Figures 10 and 14).    The passive
                                 14

   backplane 200 presented in Figure 9 shows three fields: the main
   field  (as shown in the middle of the diagram in dashed lines)
   populated with the 42 connectors 21 implementing a 2D 7 x 6
   torus configuration, the field on the left      (in dashed lines)
 5 populated with the 2 groups of 6 connectors 21 for expansion on
   the third dimension, and the field on the right       (in dashed
   lines) with 2 groups of 6 connectors 21 to allow for expansion
   on the fourth dimension. The 3D expansion is implemented by
   connecting the 6 cables   (same type as the cables connecting the
 0 PCIe card 41 to the TPP connector 21) from the TPP to the TPP on
   a different rack 33 of servers. The TPP patch panel backplane
   implementation can even be modified if desired, and with a
   simple printed circuit board replacement      (backplane 200)a person
   skilled in the art can change the wiring as required to
 5 implement different torus structures     (e.g. 5D,  6D, etc.).   In
   order to provide the ability to grow the structure without any
   restrictions or rules to follow when adding new servers in the
   rack 33, a small interconnecting plug 25 may be utilized. This
   plug 25 can be populated at TPP manufacture for every connector
 0 21. This way, every ring connection is initially closed and by
   replacing the plug 25 as needed with PCIe cable from the server
   the torus interconnect is built.
   Figure 8 presents the wiring diagram for a three dimensional
25 torus structure. Note for instance the 6 connections       shown at
   the nodes at the top left of the diagram to attach the PCIe
   cables to the 3D structure:   +X,  -X, +Y,  -Y,  +Z and -Z.  The TPP
   implementation to accommodate the 3D torus cabling is designed
   to connect any connector 21 to every other connector 21
30 following the wiring diagram shown in Figure 8.
                                   15

   The novel method of generating a netlist of the connectivity of
   the TPP is explained with the aid of pseudocode as shown at
   Figures 20a to 20c for a 4D torus wiring implementation       (that
   can easily be modified for a 3D, 5D, etc. implementation or
 5 otherwise). For the 3D torus   (Z, Y, X) each node 10 will be at
   the intersection of the three rings -    ringZ, ringY and ringX.
   If a person skilled in the art of network architecture desires
   to interconnect all the servers in a rack 33     (up to 42 servers;
   see the middle section of Figure 9 as discussed above) at once,
 0 there are no restrictions -  the servers can be wired in random
   fashion. This approach greatly simplifies the deployment -       you
   add the server, connect the cable to the TPP without any special
   connectivity rules, and the integrity of the torus structure is
   maintained. The network management    system that a person skilled
 5 in the art would know how to implement will maintain a complete
   image of the data center network including the TPP and all the
   interconnected servers, which provides connectivity status and
   all the information required for each node.
   As shown in Figure 11,  each PCIe card 41    (housed in a node
 0 server) has connectivity by cable 36 to the TPP. The cable 36
   connecting the PCIe card 41 to the TPP provides connectivity to
   the 8 ports 40  (see Figure 16) and also provides connectivity to
   the TPP for management purposes. The backplane 200 includes
   unpowered electronic devices / integrated circuit      (IC) 230
25 attached to every connector 21. Devices 230 are interrogated by
   the software running on the PCIe card 41 in order to get the
   connector ID where it is connected. Every device 230 attached to
   the connector uses a passive resistor combination that uniquely
   identifies every connector.
30
   The TPP identification mechanism    (patch panel ID) is also
   implemented using the electronic device 240 which may be
                                  16

   programmed at installation. The local persistent memory of
   device 240 may also hold other information -                                 such as
   manufacturing date, version, configuration and ID. The
   connectivity of device 240 to the PCIe cards permits the
 5 transfer of this information at software request.
   At    the     card initialization              the     software       applies power          to  the  IC
   230      and     reads      the      connector      21     ID.    A  practical        implementation
   requires          wire     connectivity          -   two     for    power      and   ground     and  the
 0 third to read the connector 21 ID using "1-Wire" technology.
   In      a     similar         fashion,        the     patch        panel       ID,    programmed      at
   installation            with the management                software,       can be     read using the
   same       wiring       as   with      IC   230.    The     unpowered        device     240   has   non
   volatile            memory          with     the      ability         to       support      read/write
 5 transactions               under         software          control.         IC     240      may     hold
   manufacturing information, TPP version, and TPP ID.
   Figure         12    displays         another      passive       patch      panel     implementation
   option using a separate printed circuit board 26 as a backplane.
 0 This       implementation              can   increase          significantly         the    number    of
   servers           in      the        rack     and      also        provides        flexibility        in
   connector/wiring selection.
   The      printed        circuit        board     23    supporting         the     connectors      21  is
   plugged via high capacity connectors 22 to the backplane 26. The
25 printed         circuit         board     24  also     has      high    capacity       connectors     22
   and        is      also       plugged        into      the       backplane         26    to     provide
   connectivity to the connector board 23.
   The     high      capacity          connectors      21    on the     board      24   can   be   used  to
   interconnect the TPPs rack 33 to rack 33.
30 The     direct        interconnect          wiring      is     implemented        on   the   backplane
   26. Any time the wiring changes                          (for different reasons) the only
   device        to    change       is   the backplane          26.    For example,        where    a  very
                                                      17

   large      torus       implementation               needs       to     change        (e.g.        for      a    10,000
   server configuration the most efficient                                    4D torus would be a 10 x
   10 x 10 x 10 configuration as opposed to trying to use a 6 x 7 x
   16   x   15;     and     for a        160,000       server deployment                 the most            efficient
 5 configuration would be a 20                         x 20 x 20 x 20),                 you can accommodate
   these      configurations               by    simply        changing         the     backplane             26    while
   maintaining the connector boards 23 and 24 the same.
   Figure       13    displays           an   optical        patch        panel       implementation.                  Such
 0 implementation                assumes       port     to     port       fiber       interconnect                as    per
   the wiring diagram presented in Figures 5 or 6                                           (2D or 3D torus).
   The     optical        connectors           on     boards        28     and     29    are      interconnected
   using       optical           fiber      27      (e.g.      high       density         FlexPlane              optical
   circuitry           from         Molex,       which         provides            high       density            optical
 5 routing         on    PCBs       or    backplanes).            The      optical        TPP      is      preferably
   fibered        at   manufacturing               time     and     the      optical        plugs          250     should
   populate         the      TPP      during      manufacturing.                The     connectors              and     the
   optical         plugs          250     are    preferably             low      loss.        The         connector's
   optical        loss      is     determined          by    the     connector           type       (e.g.        whether
 0 or    not     it    uses        micro      optical         lenses        for      collimation)               and     the
   wavelength            (e.g.       single       mod     fiber        in     C    band      introduces              lower
   optical loss than multimode fiber at 1340 nm).
   Another         implementation             option        for     the      optical        TPP       is     presented
25 in Figure 19. This                   implementation drastically reduces the number
   of      physical            connections             (fibers)           using         optical            wavelength
   multiplexing.             The new        component          added to the              TPP     is      the     passive
   optical       mux-demux           220 that         combines        multiple          optical          wavelengths
   on    the     same      fiber.         The   fibers         27    interconnects              the       outputs        of
30 the     mux-demux           220     to   implement          the      optical        direct          interconnect
   torus      structure.             To    connect       two      different           racks        (TPP       to    TPP),
   connector          222      is    used.     This      implementation                requires            a  modified
                                                        18

   version         of    the PCIe          card 41         as     shown     in     Figure        18.     The    card 41
   includes          the     optical         mux-demux          220,      optical         transmitters            225  on
   different wavelengths, and optical receivers 224.
 5 The       TPP     can     also       be     deployed          as    an     electrical/optical                  hybrid
   implementation.                 In      such       a    case,       the       torus        nodes       would      have
   optical          ports        and       electrical            ports.         A    hybrid         implementation
   would usually be used to provide connectivity to very large data
   centers.          You      could      use     the      electrical           connectivity              at   the    rack
 0 level         and        optical          connectivity                in      all       rack        to     rack     or
   geographical                 distributed               data          center           interconnects.               The
   electrical            cables        are     frequently            used    for      low     rate      connectivity
   (e.g. 1Gbps or               lower rate 10/100Mbps).                       Special electrical cables
   can       be    used       at    higher        rate       connectivity               (e.g.       10    Gbps) .     The
 5 higher         rate      interconnect              network         may    use      optical         transmission,
   as     it     can     offer        longer       reach        and     can      support         very     high      rates
   (e.g. 100 Gbps or 400 Gbps).
   Figure         15    shows       a   combined          deployment            using       a    Top     of   the    Rack
 0 (ToR)        switch       38    and a       PCIe      card 41         based       server       interconnect         in
   a   torus         structure          that       is     suited        to     implement           hybrid       compute
   servers         and       storage         server         configurations.                 The    PCIe       41    based
   implementation has the advantage of increased add/drop bandwidth
   since        the      PCI     port       in    a     server        can     accommodate             substantially
25 more       bandwidth          than      a    fixed       switch       port      bandwidth            (e.g.     1  Gbps
   or        10    Gbps).          The       PCIe        card        41     supporting             the       4D     torus
   implementation                can     accommodate             up     to    8    times       the      interconnect
   badwidth of the torus links.
30 The       ToR    switch        38    is     an    ordinary          layer       2   Ethernet          switch.      The
   switch        provides          connectivity             to    the     servers         and     connectivity         to
   other ToR switches in a torus configuration where the                                                    ToR switch
                                                           19

   is     a   torus       node.    According             to    this     embodiment            of     the     invention
   the      ToR     switches       38       and      the      PCIe     cards       41     are      interconnected
   further using a modified version of the TPP 31.
 5 Figure         16       displays           the        block        diagram          of       the       PCIe        card
   implementation             for     the present               invention.          This      card can be             seen
   as     a   multiport         Network          Interface           Card     (NIC) .        The      PCIe       card     41
   includes         a    processor         46 with          RAM 47 and ROM              48 memory,             a  packet
   switch 44 and the Ethernet PHY interface devices 45. The card 41
 0 as     shown       has    a    PCIe        connection            42   and      8    interface           ports        40,
   meaning        the      card as       shown        can provide          for the           implementation               of
   up to a four dimension torus direct interconnect network.
   Figure 17 displays the packet traffic                                  flows suported by the card
 5 41.      Each     port     40   has        access         to   the    PCI     port      42.       Therefore,           in
   the      case     of     port    to      PCI      traffic         (as   shown        by      400),       the     total
   bandwidth          is    eight     times         the      port    capacity         given that            the total
   number        of     ports     40      is      8.     The     number       of     ports        determines            the
   torus       mesh      connectivity.              An eight         port     PCIe      Card       implementation
 0 enables       up to a        four dimension torus                     (x+,     x-, y+, y-,             z+, z-        and
   w+, w-).
   A    second       type     of    traffic            suported        by   the       card       41    is     the     hair
   pinning        traffic        (as     shown by            410).     This    occurs         where       traffic         is
   switched          from    one     port        to     another        port;       the     traffic          is     simply
25 transiting            the   node.         A   third        type     of   traffic           supported           by    the
   card 41 is transit with add/drop traffic                                      (as shown at 420).                   This
   occurs when incomming traffic                              from one port            is partially dropped
   to     the    PCI       port    and       partially           redirected           to     another          port,       or
   where       the     incoming        traffic           is    merged with           the      traffic          from     the
30 PCI port and redirected to another port.
                                                           20

  The      transit    and      add/drop     traffic     capability         implements         the
  direct     interconnect        network,    whereby    each node       can be     a  traffic
  add/drop node.
5 Throughout        the      specification,        unless      the    context        requires
  otherwise,      the word "comprise"           or variations      such as       "comprises"
  or     "comprising",      will    be understood      to   imply   the     inclusion       of a
  stated integer or group of integers but not the exclusion of any
  other integer or group of integers.
0
  Furthermore,       throughout        the    specification,       unless       the   context
  requires      otherwise,        the   word    "include"     or   variations        such      as
  "includes"       or   "including",         will    be   understood         to   imply       the
  inclusion      of  a   stated      integer    or  group    of  integers       but   not     the
5 exclusion of any other integer or group of integers.
  Modifications        and      variations     such   as    would    be     apparent       to    a
  skilled      addressee        are   deemed    to   be   within     the      scope    of     the
  present invention.
0
  The      present     application        is    divided      from   Australian          patent
  application       2014311217,         the   content     of   which      is    incorporated
  herein in its entirety by reference.
                                             21

   THE CLAIMS DEFINING THE              INVENTION ARE AS FOLLOWS:
      1. A   Peripheral         Component         Interconnect       Express     (PCIe)    card
 5       housed     in   a   server        and   having     connectivity     by   cable    to  a
         connector        on      a     passive      patch     panel     for    use    in    the
         implementation           of     a   torus    or    higher    radix    interconnect,
         said card comprising:
 0       at   least    4    electrical          or  optical      ports   for   the   torus    or
         higher radix interconnect;
         a local switch;
 5       a  processor       with       Random Access        Memory    (RAM)   and   Read   Only
         Memory     (ROM) memory;           and
         a Peripheral Component Interconnect                      (PCI) interface,
 0       and wherein the PCIe card has connectivity to an electronic
         device on the passive patch panel whereby information about
         said    patch     panel        or   the   connector      may   be  relayed     at   the
         request of software running on said card.
25    2. The    Peripheral         Component       Interconnect       Express    (PCIe)    card
         of   claim 1, wherein the               information relates to one or more
         of   manufacturing            date    of  the     patch   panel,   version     of   the
         patch panel,         configuration         of the      patch panel,     patch    panel
         ID,   and connector ID.
30
      3. The    Peripheral         Component       Interconnect       Express    (PCIe)    card
         of    claim    2,     wherein        the    local     switch    is  electrical       or
         optical.
                                               22

   4.  The  Peripheral    Component    Interconnect      Express      (PCIe)       card
       of  claim 3, wherein the      PCIe card is      capable of       supporting
       port  to  Peripheral     Component    Interconnect         (PCI)   traffic,
 5     hair pinning traffic, and transit with add/drop traffic.
   5. The   Peripheral    Component    Interconnect      Express      (PCIe)       card
       of  claim   4,   wherein  the   PCIe   card  is    further      capable       of
       combining multiple optical wavelengths on a same fiber.
 0
   6. A   Peripheral     Component    Interconnect      Express      (PCIe)        card
       housed   in    a  server   having    connectivity        by   cable      to    a
       connector    on  a  passive   patch   panel   in    a   torus    or    higher
       radix interconnect structure, said card comprising:
 5
      a Peripheral Component Interconnect          (PCI) connection;
      a processor with Random Access Memory           (RAM) and Read Only
      Memory   (ROM) memory;
 0
      a switch;
      at least one Ethernet PHY interface device;              and
25    at least eight interface ports to provide for the
      implementation of up to a four dimension torus direct
      interconnect network,
      and wherein the PCIe card has connectivity to an electronic
30    device on the passive patch panel whereby information about
      said patch panel or the connector may be relayed at the
      request of software running on said card.
                                   23

   7.  The  Peripheral    Component   Interconnect      Express   (PCIe)       card
       of  claim 6,   wherein the   information    relates    to one or more
       of  manufacturing    date of   the  patch    panel,   version     of     the
 5     patch panel,    configuration   of the   patch panel,     patch      panel
       ID,  and connector ID.
   8. The   Peripheral    Component   Interconnect      Express   (PCIe)       card
       of  claim 7, wherein the     PCIe card is     capable of     supporting
 0     port   to   Peripheral  Component    Interconnect      (PCI)   traffic,
       hair pinning traffic, and transit with add/drop traffic.
   9.  The  Peripheral    Component   Interconnect      Express   (PCIe)       card
       of claim 8, wherein the switch is electrical.
 5
   10.The   Peripheral    Component   Interconnect      Express   (PCIe)       card
       of claim 8, wherein the switch is optical.
   11.  A  Peripheral    Component   Interconnect       Express  (PCIe)        card
 0     for use in optical wavelength multiplexing comprising:
       a Peripheral Component Interconnect        (PCI) connection;
       a processor with Random Access Memory          (RAM) and Read Only
25     Memory    (ROM) memory;
       a photonic switch;
       at least one Ethernet PHY interface device;
30
                                  24

      a    first       passive      optical         mux-demux        comprising      fibers        and
      combining          multiple      optical       wavelengths        on the    fibers,         said
      mux-demux being linked to optical receivers;                             and
 5    a   second       passive      optical       mux-demux        comprising     fibers,         said
      second          passive        optical         mux-demux         linked      to      optical
      transmitters             on      different         wavelengths          and       combining
      multiple optical wavelengths on the fibers,
 0    said      fibers       interconnecting           a    plurality       of   mux-demux          to
      implement an optical direct interconnect torus network.
   12.The     Peripheral         Component         Interconnect         Express     (PCIe)        card
      of claim 11, wherein the PCIe card is                           capable of supporting
 5    port      to     Peripheral        Component        Interconnect         (PCI)     traffic,
      hair pinning traffic, and transit with add/drop traffic.
   13.Use     of    a    Peripheral        Component      Interconnect         Express         (PCIe)
      card      of    claim     12   with      a   top  of    a    rack   switch     in    a    torus
 0    structure           suited    to     implement       hybrid      compute     servers         and
      storage server configurations,
      wherein         said    top   of    the     rack   switch       provides    connectivity
      to    the      servers      and     connectivity          to    other   top     of     a    rack
25    switches         in   the    torus     structure       where     the   top   of the         rack
      switches are torus nodes.
                                               25

<removed-apn> <removed-date>
<removed-apn> <removed-date>
               3/16
<removed-date>
              PRIOR ART
<removed-apn>
              FIGURE 3

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
               16/16
<removed-date>
<removed-apn>
              FIGURE 21
              FIGURE 22

