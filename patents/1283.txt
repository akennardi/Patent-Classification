          Abstract
          Automatic process control of additive manufacturing. The system includes an additive
  manufacturing device for making an object (16) and a local network computer controlling the
5 device. At least one camera (10) is provided with a view of a manufacturing volume of the
  device to generate network accessible images of the object (16). The computer is programmed
  to stop the manufacturing process when the object (16) is defective based on the images of the
  object (16).

                                                     1
       AUTOMATIC PROCESS CONTROL OF ADDITIVE MANUFACTURING DEVICE
   The present application is a divisional application from Australian Patent Application No.
   2014306218, the entire disclosure of which is incorporated herein by reference.
 5                                    Background of the Invention
           This invention relates to general additive manufacturing devices such as 3-D printers
   which may utilize a variety of technologies, including extrusion deposition, granular melting
   and sintering, powder bed and binder, and light polymerisation. The system is particularly
   suited for, but not limited to, devices that are automated such that a finished job can be
10 removed from the printing volume and the next job started without any manual human actions.
           Conventional additive manufacturing devices require a toolchain with numerous
   different software applications for various steps in the process. All process feedback such as
   dimensional accuracy and surface finish must be measured and evaluated manually, with no
   systematic way of integrating this feedback to improve system function.
15         Efficiently operating an additive manufacturing device to produce objects that meet
   designer-specified tolerances involves minimizing machine time per job, operator time,
   material consumption and overall machine downtime in order to maximize throughput and
   limit material and personnel cost. An ideal system would operate around the clock and only
   produce objects within the specified tolerances without requiring a human operator.
20         In reality, a variety of issues result in failed jobs, objects that do not meet tolerance
   requirements, and unnecessary machine downtime. Manually controlling these issues,
   especially on low-cost additive manufacturing devices, requires a significant amount of
   operator time to pre-inspect parts, input various machine parameters to meet specified
   tolerance requirements, monitor the job manually, remove the object after completion,
25 measure the object to test adherence to specified tolerance requirements, and iteratively repeat
   the process until the object meets the specified requirements.
           The discussion of the background to the invention included herein including reference
   to documents, acts, materials, devices, articles and the like is included to explain the context of
   the present invention. This is not to be taken as an admission or a suggestion that any of the

                                                    2
   material referred to was published, known or part of the common general knowledge in
   Australia or in any other country as at the priority date of any of the claims.
           Where the terms "comprise", "comprises", "comprised" or "comprising" are used in
   this specification (including the claims) they are to be interpreted as specifying the presence of
 5 the stated features, integers, steps or components, but not precluding the presence of one or
   more other features, integers, steps or components, or group thereof.
                                       Summary of the Invention
           According to an aspect of the present invention, there is provided a system for
   automated process control of an additive manufacturing device comprising an additive
10 manufacturing device for making an object; a local networked computer controlling the
   device; at least one camera with a view of a manufacturing volume of the device to generate
   network accessible images of the object, the images of the object are used to determine its
   quality by comparing the outer surface of the object to a defined model and specification so as
   to devise a computation model that aids in finding the optimum slicing parameters for use by a
15 3-D print driver; wherein the computer is programmed to stop the manufacturing process when
   the object is defective based on the images of the object.
           The disclosed invention minimizes the number of tools an operator must use to control
   an additive manufacturing device by providing a single unified interface for inspecting
   potential jobs, monitoring jobs remotely in real-time, and gathering and evaluating process
20 feedback during and after a job. In addition, the system uses modern computer vision and
   machine learning algorithms to automate the process of identifying and correcting system
   errors and inaccuracies, providing tolerance control without requiring operator input.
           The disclosed system increases the efficiency of operating an additive manufacturing
   device by automatically minimizing factors contributing to cost. Machine learning algorithms
25 correlate the input CAD (Computer Aided Design) file with machine parameters to predict the
   properties of the manufactured object and the time necessary to manufacture it. Computer
   vision algorithms or an integrated 3D scanner evaluate the object after manufacturing to
   ensure tolerance requirements are met and to provide feedback to the machine learning
   algorithms so that the predictions improve over time. Thus the system gradually improves its
30 ability to set machine parameters that maximize likelihood of meeting specified tolerances
   while minimizing manufacturing time. This minimizes both the time per job and the number of

                                                     3
   iterations required to meet specifications. Minimizing iterations limits wasted material
   consumption and increases overall throughput of the device.
           The system can perform various calibration routines automatically using computer
   vision techniques. Temperature calibration for a given material involves printing a test object
 5 at varying temperatures and selecting the object with the best bed adhesion, surface finish, and
   dimensional accuracy. Images of a calibration pattern printed onto the plate provide
   information on bed levelness and nozzle height. Part removal characteristics such as the
   optimal z-height for aligning the blade with the printing surface are optimized using computer
   vision. Part adhesion can be estimated from the amount of current drawn by the motor
10 powering the removal blade.
           The system tracks material consumption and automatically notifies material suppliers
   when to ship new material before scheduling a planned downtime for an operator to replace
   the material cartridge. This prevents the operator from replacing the material cartridge at a
   non-optimal time; too early means usable material may be wasted, and too late means
15 unnecessary machine downtime.
           The system monitors jobs in real-time using computer vision algorithms to detect
   failures at the earliest point possible. Rather than waiting until a job is finished to measure and
   inspect the part, the system can detect that a job is unlikely to meet specifications early in the
   process. The job can be terminated early and either restarted or skipped to avoid wasting
20 additional material and machine time.
           The system provides a single interface for an operator to add jobs to the queue, input
   specifications and tolerances, inspect CAD files, and provide additional manual monitoring if
   desired. The interface includes 3D print preview, which incorporates a CAD file and proposed
   machine parameters to visualize the predicted output of the additive manufacturing device. 3D
25 print preview allows the operator to adjust a multitude of specifications and get predictive
   feedback in realtime of how those adjustments will affect the printed object. 3D print preview
   can also detect and annotate features of a CAD model that are likely to result in errors.
           The invention also includes a novel 3D scanning method that allows non-disruptive
   scanning to be integrated into a 3D printer. An array of individually-controlled lights shining
30 on the object creates shadows that are imaged and measured to verify dimensional accuracy.

                                                     4
           The system according to the invention for automated process control of an additive
   manufacturing device includes an additive manufacturing device such as a 3-D printer for
   making an object and a local networked computer controlling the device. At least one camera
   is provided with a view of a manufacturing volume of the device to generate network
 5 accessible images of the object. The computer is programmed to stop the manufacturing
   process when the object is defective based on the images of the object.
           In a preferred embodiment, the at least one camera has a fixed view of the
   manufacturing volume. Alternatively, the camera may have a robotically controlled view of
   the manufacturing volume. The images may be video streams or static images. A preferred
10 additive manufacturing device is a 3-D printer. In another preferred embodiment, the computer
   further includes a series of server-side applications executing remote algorithms. A web
   browser based control interface may be provided. The algorithms may include machine
   learning algorithms such as Markov, Bayesian inference or artificial neural network
   algorithms.
15         Another preferred embodiment includes 3-D print preview to update object rendering
   in real time. The system may further include an array of lights for creating object shadows for
   reconstructing a profile view from the point of view of each light.
                                     Brief Description of the Drawing
           Figs. la and lb are perspective views of an embodiment of the invention including a
20 camera    viewing    a printing    surface  within  a manufacturing    volume  of an additive
   manufacturing device.
           Figs. 2a and 2b are perspective views of an embodiment of the invention showing layer
   by layer verification along with the comparison of an image with a render.
           Figs. 3a and 3b are side and top views, respectively, of an embodiment of the invention
25 utilizing an array of lights to cast shadows from an object being printed.
           Fig. 4 is an overall system diagram showing system operation.
           Fig. 5 is a flow diagram for the automated process control with layer-by-layer
   verification according to an embodiment of the invention.

                                                    5
                                Description of the Preferred Embodiment
           Since desktop 3D printers are not completely reliable, fairly robust error detection and
   correction is necessary for true automation. This patent application discusses several software
   and hardware techniques to further automate and optimize the 3D printing process.
 5         Automated Process Control involves building a computational model of the 3D
   printing process. The goal is to optimally select printing parameters based on the CAD file and
   dimensional and structural specifications and tolerances. The model is continuously improved
   by automatically and/or manually evaluating resulting outputs to provide feedback to the
   computational model.
10         Input features are gathered from both the CAD file and user-specified parameters. The
   CAD file is analyzed to determine relevant characteristics such as cross-sectional area along
   any closed surface or layer, contact area and perimeter with the printing surface, wall
   thickness, required support material, and angles of supported features. The user may specify
   dimensional tolerances and strength requirements along a multitude of axes and surface finish
15 requirements for designated faces.
           These input features are used to estimate the optimal slicing parameters. Slicing
   parameters include information such as printing surface and nozzle temperature, rotational
   orientation, layer height, maximum speeds and accelerations for each axis, and infill patterns
   and densities. Once the settings are determined, the CAD file is sliced and sent to the printer.
20         During printing, computer vision algorithms monitor for failures. In Fig. la, a camera
   10 is located for viewing a printing surface 12. Comers of the printing surface 12 are
   determined and defects show up as glare regions 14 as shown in Fig. lb.
           The algorithms are tuned to predict the cause of the error. For example, if an error is
   detected very early in the print there is likely an issue with the levelness or coating of the
25 printing surface. If the error is sudden and drastic later on in the print, it is likely that the part
   became detached from the printing surface and the cause of failure is poor bed adhesion and/or
   warping.
           After printing, the system gathers a variety of outputs. Dimensional accuracy and
   surface finish are estimated from a 3D scan of the object provided by the integrated 3D

                                                   6
   scanner. Print surface adhesion can be estimated from the amount of power drawn by the
   motor powering the removal system.
           All of the information gathered can be used as feedback for the computational model.
   Over time, various failure modes will become associated with corresponding slicing
 5 parameters. For example, poor bed adhesion is likely caused by incorrect temperature settings
   or printing orientation. Failure to meet dimensional tolerances is likely caused by incorrect
   acceleration, speed, or layer height. The machine learning algorithm determines the degree of
   correlation between each input and each failure mode.
           The system maintains a print queue for upcoming jobs, and each job includes metadata
10 such as the amount of filament consumed and an estimated print time. Additionally, the
   system tracks the amount and type of filament for each printer.
           Before shipping to the customer, the material supplier can perform various quality
   control and calibration processes on a given material cartridge. For example, the supplier
   measures the precise filament diameter and runs a calibration routine to determine the optimal
15 printing temperatures for both the nozzle and printing surface. The cartridge is tagged with a
   unique ID number, which is entered into a web interface along with the corresponding
   calibration information. By having the material supplier perform these steps, calibration can be
   done on a per-batch basis. This is much more efficient than having the end user calibrate for
   each material cartridge. It minimizes variability from an assumed mean value, resulting in
20 much higher accuracy than assuming that batches will have the same properties.
           The system stores the active cartridge ID for each printer, as well as calibration and
   remaining material information for each cartridge ID. Whenever a user wishes to replace the
   material in the printer, she must first input the new cartridge ID. The system will update the
   corresponding printer's active material and re-slice objects in the queue based on the
25 calibration information associated with that material cartridge ID. After each print job
   (successful or failed) the system updates the amount of material remaining for the relevant
   material id.
           Whenever a new job is added to the printing queue, the system checks if there will be
   enough remaining material for the job to complete. If there is an insufficient amount of
30 material remaining, the system will schedule a material replacement and notify the operator.
   Later, if another job is submitted that requires less material such that it can be completed

                                                       7
   before the material replacement, the system will automatically move the new job ahead in the
   queue.
           The system performs these material checks as each part is added to the queue. This
   means that rather than detecting insufficient material supplies immediately before each print
 5 starts, the system detects such shortages well in advance. This allows for a variety of
   optimizations. Smaller jobs that require less material can be moved up in the queue to avoid
   wasting material at the end of a cartridge. Material replacement can be scheduled far in
   advance, allowing operators to plan their schedules around the replacement.
           The system maintains an average rate of material consumption for each printer. This
10 combined with information about the supplier and shipment lead time allows the system to
   automatically order replacement material and have it arrive just in time. This avoids issues
   associated with getting replacement material at the wrong time: too early and the unused
   material may degrade before being used, too late and there may be unnecessary machine
   down-time while waiting for material to arrive.
15         Note that users may change material even before the current active material is running
   low, for example if they require a different color or material type. The old cartridge can be
   reinstalled at a later time. In fact, an old cartridge could even be installed on a different printer
   connected to the system. This is possible because the system stores remaining material
   information on a per-cartridge basis, not per-printer.
20         One-way mirrors can be used to facilitate a stable lighting environment for computer
   vision algorithms. Allowing light to exit the printing volume but not to enter it means that
   operators will still be able to observe the machine, but that external lighting conditions will not
   affect the accuracy of the computer vision algorithms. Internal lighting conditions are stable
   and can be integrated into the scenes used to generate renders as shown in Fig. 2 that will be
25 compared to images. Controlling for varying external lighting conditions with one-way glass
   will improve the accuracy and consistency of the computer vision algorithms.
           Figs. 2a and 2b illustrate layer-by-layer camera verification. As shown in Fig. 2a a
   partially printed part 16 is compared with a render of the partial part 18. If the partially printed
   part 16 differs beyond a selected threshold from the render 18, the part 16 is defective and the
30 system should shut down so that the partially printed part 16 may be removed and discarded.

                                                     8
           Fully automated 3D printers utilize some form of automated part removal to clear the
   printing surface between prints. A computer vision algorithm is used to verify that the removal
   was successful and that the printing surface is clean enough for the next print to begin. The
   process consists of a calibration routine, techniques to compensate for varying lighting
 5 environments, and a print surface clearness verification routine. See Figs. la and lb.
           Calibration is a manual process involving a human operator. The operator first
   confirms that the print surface is completely clear. The system then moves the surface to the
   home position and captures a reference image. Next an edge detection or other computer
   vision algorithm is used to identify the comers of the printing surface in order to determine
10 which pixels in the reference picture represent the print surface. The human operator verifies
   or corrects this print surface isolation. See Figs. 2a and 2b.
           Next the operator cycles through various lighting conditions the printer may encounter.
   This includes turning on and off lights, opening and closing window shades, and testing during
   both day and night time. For every possible combination of lighting conditions, the human
15 operator signals to the computer system to capture a glare calibration image with the print
   surface in the home position. The webcam and/or external sensors on the printer measure
   ambient lighting conditions associated with each glare calibration image. This concludes the
   calibration process.
           After an attempted removal, the system performs the print surface clearness
20 verification routine. First the printer measures the ambient lighting conditions to determine the
   current most likely lighting conditions. It uses these conditions to select which glare
   calibration image to use for glare reduction. Next the printer moves the print surface into the
   home position and captures a verification image. Glare reduction techniques such as feature
   recognition or thresholding remove as much of the glare as possible from the verification
25 image. Finally, edge detection algorithms are run on the glare-reduced verification image. If
   no edges are detected within the print surface then the printer is notified that the print volume
           is clear so it may begin the next print. If edges are detected, the printer is notified to
   run the removal routine again. If the system detects a failed removal multiple times in a row,
   the operator is notified to inspect the system and manually remove the part.
30         Layer-by-layer verification is used to detect errors during the printing process. If an
   error is detected, the system can cancel and remove the failed print before either retrying the

                                                     9
   job or moving to the next job in the queue. Layer-by-layer verification consists of a calibration
   routine, an augmented slicing routine, and a per-layer verification routine.
           The calibration routine identifies the boundaries of the print surface depending on its
   height (the z-height) and distance from the camera. First a human operator must verify that the
 5 surface is clear. Next the printer moves to a variety of z-heights and captures a calibration
   image that is associated with that z-height. For each calibration image, the corners of the print
   surface are identified either automatically by an edge detection algorithm or manually by a
   human operator.
           Layer-by-layer verification requires a specialized slicing technique. Rather than only
10 generating g-code for every layer, the slicer must also generate a predictive render of what the
   part will look like after each layer is printed. This takes into account not only the shape of the
   partially printed objects, but also other slicing parameters such as layer height and infill
   pattern and density. These renders are adjusted to accommodate material shrinkage based on
   the material being used and the ambient and printing temperatures. This information is
15 combined with the calibration image of the print surface at the appropriate z-height and
   distance from the camera to generate a render of what the print should look like after each
   layer is printed.
           The per-layer verification routine requires a technique to capture images after every
   layer of a 3D print. This could include a custom g-code inserted into the g-code file for the
20 print at the end of each layer. When the firmware executes this custom g-code it signals to the
   webcam to capture a layer verification image. The print can continue uninterrupted while a
   remote or local server processes the layer verification image. Only if the verification process
   detects a variation from the acceptable tolerances will the print be cancelled.
           Once the layer verification image is captured and sent to the remote or local server, the
25 system compares the verification image with the predictive render associated with the current
   z-height from the slicing routine. This verification           involves edge detection, feature
   recognition, and other computer vision techniques to determine how close the current print is
   to the predicted render. If the difference between the image and the render is below a threshold
   value the print may proceed. Otherwise the print job is cancelled and removed, after which the
30 print job can either be attempted again (potentially with adjusted parameters) or skipped so the
   next job on the queue can be printed.

                                                     10
           The verification algorithm compares not only the shape of the part, but also it's position
   relative to the printing surface. This allows the system to detect when parts become either
   partially or completely detached from the printing surface. This is a common failure mode that
   cannot be recovered from without restarting the job, and can potentially cause damage to the
 5 printer if the failed job is not terminated.
           Detecting shadows is very useful for detecting failures, particularly when the material
   is a very similar color to the printing surface or background. The lighting conditions of the
   printer are included in the scene used to generate renders, so shadows are present in both the
   render and the image. Comparing these shadows provides additional data on if the print is
10 succeeding.
           With reference now to Figs. 3a and 3b, lights 20 form an array of lights. The lights 20
   may be light emitting diodes. As shown in Figs. 3a and 3b, the lights cast shadows from the
   feature A being observed.
           In order to improve the ability of the algorithm to predict if a print has failed, users can
15 annotate which frame of a time lapse generated from the layer verification images as the point
   of failure. All previous images can be used as negative training examples (no errors) and all
   subsequent images can be used as positive training examples (error) for an error detecting
   classification algorithm.
           Layer-by-layer verification also enables automatic printer self-preservation. Some
20 failed prints potentially result in the printer damaging itself, for example by crashing the print
   head after the motor skips or by getting stray material stuck in components such as belts,
   pulleys, and bearings.. By detecting failures in-process, the machine is much less likely to
   continue a failed print long enough to cause serious damage.
           To facilitate continuous improvement of the entire 3D printing process, the system uses
25 a combination of computer vision and machine learning techniques to evaluate prints and
   improve parameter optimization for the slicing process. This involves various relevant inputs
   to the slicing software and features of the model being printed, as well as relevant features of
   both successful and unsuccessful prints. The system evaluates the features of the 3D model to
   select the inputs for the slicing software, and then evaluates either 2D images or 3D scans of
30 the printed part to provide feedback. Over time, the system will learn how to best optimize the
   slicer settings in order to produce high-quality prints.

                                                   11
           Features of 3D models may include cross sectional area at various heights, print
   surface contact area, user-annotated force vectors, wall thickness, user-specified print time
   requirements, and other relevant features. Slicer parameters may include layer height, infill
   pattern and density, extrusion width, printing temperatures, and various other parameters of
 5 the slicing software. Feedback from models may include metrics such as dimensional
   accuracy, surface finish, measured print time, and user-supplied strength ratings.
           3D scanning and/or computer vision are used to verify the quality of the part. This
   allows an operator to print a part and know that it will meet specifications. The system uses a
   3D scan and/or computer vision algorithms to compare the outer surface of the completed or
10 partially completed part to the CAD model and specifications. If a job does not meet
   specifications, it can automatically be resubmitted, optionally with altered settings to increase
   the likelihood of compliance with specifications. The operator no longer needs to iteratively
   set parameters, print, and measure the part, since the system performs all steps of this process
   automatically.
15         A variety of machine learning algorithms may be suitable to map these features and
   develop a model for 3D printing slicer parameters. These include hidden markov models,
   bayesian inference, and artificial neural network algorithms.
           Part evaluation can be performed on a layer-by-layer basis. This allows the system to
   verify tolerances on internal features that may not be visible when the print has finished. All of
20 the techniques from this section can be integrated into the layer-by-layer routine to provide in
   process feedback to the machine learning algorithms.
           When new feedback causes drastic changes to the model, old slicing parameters may
   no longer be optimal. The system can detect such drastic changes and re-slice parts waiting in
   a queue to take advantage of the updated model. If the system learns from one or more printers
25 that adjusting a certain parameter greatly improves quality of a related feature, other printers
   can use this information to re-slice queued parts based on the improved model.
           There are many tools available to view and manipulate 3D models, both in the cloud
   through the browser and with locally installed software. However, none of these tools provide
   direct feedback on how various slicing settings will affect the object being printed. Our system
30 includes 3D print preview, which gives users real-time visual feedback of how input settings
   will affect the printed shape.

                                                    12
           3D print preview incorporates relevant context attributes to generate accurate real-time
   renderings of a potential 3D print. For example, the system selects the color of the print
   preview based on the color of filament currently loaded into the user's printer. Additionally,
   the object is rendered within a to-scale representation of the print volume.
 5         3D print preview updates the rendering in real time as users adjust print settings. For
   example, when a user adjusts the speed vs. quality setting the system adjusts the layer height
   used to print the object. The render of the object includes a representation of the layer height,
   which is adjusted in real time as the user adjusts the speed vs. quality slider. Since layer height
   is very small relative to the size of most parts, anti-aliasing techniques are necessary to avoid
10 unwanted artifacts in the render.
           Unlike toolpath visualization tools which require complex pre-processing on a model
   before rendering, 3D print preview generates a computationally-efficient visualization of the
   model. Even the fastest pre-processors (aka slicers) can take 10-20 seconds to generate a
   toolpath from an average sized file. These preprocessors must run again after every parameter
15 update. 3D print preview loads quickly in the user's web browser and responds in real-time to
   sliders that control key printing parameters before slicing the CAD file.
           Conventional 3D scanners generally require rotation of a laser and camera relative to
   the object being scanned. Integrating this rotation with a cartesian 3D printer to make a dual
   3D printer/scanner can be difficult. We propose a system with a small number of standard
20 webeams and an array of lights to generate 3D scans without moving the camera relative to the
   object.
           Figs. 3a and 3b illustrate a suitable array of lights to gather 2-D projections from the
   object being printed.
           The system uses the array of lights to gather 2D projections (shadows) of the object
25 from various angles. The lights must emit a spectral range that is within what the cameras can
   detect, but this range may or may not fall within the visible spectrum. The lights are turned on
   one at a time or in specific groups and an image of the object is gathered with each webcam.
   The shadow of the object can be used to reconstruct a profile view from the point of view of
   each light. Several of these profile views can be used to reconstruct a 3D model of the object.

                                                  13
           The system can be improved by using a printing surface with a precise grid to provide
   reference points for the computer vision algorithm. Additionally, the system can align axes
   with specified dimensional tolerances with one or more lights to guarantee that the relevant
   dimension can be measured. Finally, the system can move the printing surface along the z axis
 5 or move the webcam in order to get necessary views.
           The difference between scanning an arbitrary object and checking if a manufactured
   part meets specifications is subtle but important. Since the system knows what the object
   should look like, it can predict what shadows a particular light combination will create, then
   verify if those shadows appear as expected. This is a much simpler process than attempting to
10 generate an entire 3D model from a small selection of profile views.
           This technique could potentially be optimized by using multi-color lights or lights with
   different wavelengths. If the camera can accurately identify any possible combination of
   lights, a single image can be used for the above process. Camera filters can be used to isolate a
   specific spectrum. Based on the color measured at each pixel of the image, the system would
15 determine which lights are shining on that point unobstructed. This would result again in a
   series of shadows from each light view, which can be used to generate profile views and in
   turn a reconstructed 3D model of the object.
           Fig. 4 shows an overall system diagram. This figure shows both the remote and local
   aspects of the system. Fig. 5 illustrates the automated process control of the invention with
20 layer-by-layer verification.

                                                   14
   What is claimed is:
           1. A system for automated process control of an additive manufacturing device
   comprising:
 5         an additive manufacturing device for making an object;
           a local networked computer controlling the device;
           at least one camera with a view of a manufacturing volume of the device to generate
   network accessible images of the object, the images of the object are used to determine its
   quality by comparing the outer surface of the object to a defined model and specification so as
10 to devise a computation model that aids in finding the optimum slicing parameters for use by a
   3-D print driver;
           wherein the computer is programmed to stop the manufacturing process when the
   object is defective based on the images of the object.
15         2. The system of claim 1 wherein the at least one camera has a fixed view of the
   manufacturing volume.
           3. The system of claim 1 or 2 wherein the at least one camera has a robotically
   controlled view of the manufacturing volume.
20
           4. The system of any one of claims 1 to 3 wherein the images are video streams.
           5. The system of any one of claims 1 to 4 wherein the images are static.
25         6. The system of any one of claims 1 to 5 wherein the additive manufacturing device is
   a 3-D printer.
           7. The system of any one of claims 1 to 6 wherein the computer further includes a
   series of server-side applications executing remote algorithms.
30
           8. The system of any one of claims 1 to 7 further including a web-browser based
   control interface.
           9. The system of claim 7 wherein the algorithm includes machine learning algorithms.

                                                   15
            10. The system of any one of claims 1 to 9 wherein the manufacturing volume includes
   a calibration pattern thereon.
 5          11. The system of claim 9 wherein the machine learning algorithms include markov,
   bayesian inference or artificial neural network algorithms.
            12. The system of any one of claims 1 to 11 further including 3-D print preview to
   update object rendering in real time.
10
            13. The system of any one of claims 1 to 12 further including an array of lights for
   creating object shadows for reconstructing a profile view from the point of view of each light.

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
