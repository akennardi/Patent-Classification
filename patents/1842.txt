ABSTRACT
       This invention shows a type of method to develop intelligent Gomoku
system based on the reinforcement learning. It utilizes critic network to evaluate
board situations, taking advantages of Incentive and Punishment System to
reward or penalize the move that caused win or lose. As a result, the invention
apply to reinforcement learning to put the Incentive and Punishment System into
use of Gomoku, updating and adjusting the value of critic network through
neutral network. And a training system of Gomoku Al is built, cultivating a
Gomoku Al with candidate level by the way that allow two Gomoku Al against
each other.

51e ~  P* areM le I, Help
      Figure 3
             3

DESCRIPTION
Title
                Method of intelligent Gomoku system based on
                           reinforcement learning
Field of the Invention
      This invention applies to the research field of board game type-artificial
intelligence gaming, especially includes a mechanism of intelligent Gomoku
system based on reinforcement learning.
Background of the Invention
      The modern intelligent Gomoku systems are builtby the reinforcement
learning train with supervision, and the knowledge and experience of human
experts' accumulation. In this case, a great number of previous Gomoku game
accumulation is needed as an input, and processing the input data is necessary.
In general, there is a huge connection between the trained Gomoku program and
the number of previous game inputted as well as the amount of expert's
knowledge. The more game record with more experience of human experts, the
better performance of the Trained Gomoku program. Conversely, the intelligent
degree of program would be lower. Nowadays, generally the intelligent level of
Gomoku Al depends on the quantity and quality of the data inputted. However,
the system introduced by this invention would not need to input any human
previous game. The system is capable of improve itself by the way of
reinforcement learning through game against itself. Whereas as the amount of
gaming against itself reach a candidate number, the system would possess great
level of intelligence. In comparison, the way provided by this invention would
                                         1

be more convenient, more cost-saving, and more effective than modern Gomoku
Al program.
SUMMARY OF THE INVENTION
     The state in subjects about Gomoku is the board situation. If the situation is
represented by the occupational condition of each position, then there are 3255
situations in total. With evaluation function to show probability of a situation's
winning, it is obvious impracticable to store evaluation functions of so many
situations. Moreover, due to the convergence condition of reinforcement
learning, it is also impossible to traverse each situation for infinite times. To
compress state space, this invention has artificially extracted several characters
(patterns) to describe situations. There are 20 extracted patterns to be applied to
either defensive or offensive situations. In this way, one situation is described by
20 patterns of the black side and 20 patterns of the white side. More specifically,
with a given board situation, the number of each pattern in horizontal and
diagonal lines is counted to describe this situation. 8 of these patterns are shown
in Fig. 1.
     Besides the above 20 patterns for each side, whose turn to move is also an
important character. There are now 42 state variables, reduced from 225 of them.
However, the state space remains large, which means it is still unfeasible to use
table to store the evaluation. This invention, therefore, use a neural network to
approximate evaluation functions. Because on one hand, the design of reward
function r has been relatively matured through previous researches about chess
games, and can be considered as prior knowledge; on the other hand, Gomoku is
a certain Markov decision process, while state transitions are clear: the situation
transited from a certain situation is immediately apparent, so the state transition
function can also be considered as prior knowledge. With the perfect knowledge
of the two functions, we only need to choose a state where the highest
                                          2

evaluation can be reached in one step. Based on this, this invention puts forward
a method to develop intelligent Gomoku system based on the reinforcement
learning.
DESCRIPTION OF DRAWING
     Figure 1 displays some patterns used in the presented program.
     Figure 2 displays the neural network used to approach V - function of
gomoku.
     Figure 3 displays the intelligent gomoku system training interface based on
piskvorky platform.
DESCRIPTION OF PREFERRED EMBODIMENT
     The implementation processof the present invention can be divided into
four stepsas state space compression, evaluation function determination, training
process design and Gomoku strategy selection.
      1)    state space compression
     Gomoku is played between two players on a 15 * 15 square mesh, each
grid has 3 states, so a total of 3255 situations are formed, which means 225
patterns are used to represent all the situations. From the game situation of
Gomoku, we can see that the positions of the pieces placing are more
concentrated and therefore not all possible situations will appear. In the present
invention, a total of 42 patterns representing the situation are selected. When
calculating the number of patterns, the present invention uses the method of
string matching to set 20 kinds of patterns as constant strings, and encodes all
the situations of placing pieces on the Yang and Yin lines as characters string,
then through string matching to get the number of each pattern.
     2)     evaluation function determination
     The present invention uses a feed-forward three-layered fully connected
                                         3

neural network, containing only one hidden layer shown in Fig 2.
      The input layer contains 274 neurons, the hidden layer contains 100
neurons, and the output layer contains 1 neuron. Hidden layer and output layer
neurons without introducing bias term. The output of the neural network
represents a V-value function, indicating the probability of winning. In order to
improve the learning effect, the present invention encodes the number of each
pattern and then serves as an input to a neural network. Five input neurons are
used to represent the number of patterns, as shown in Table 1. It's important to
note that there is a special pattern - the number of the situation of "five
connected" uses only one input neuron representation. And if this pattern
appears, the corresponding input is 1, otherwise 0. Therefore, the number of
patterns corresponds to 5x19x2+1x2=192 input neurons. Gomoku's value
function is not as smooth as Backgammon, so each pattern is equipped with 2
input neurons as a moving sequence so that the moving sequence corresponds to
2x20x 2 =80 input neurons. Two players share the same neural network to
approximate evaluation function, so we make a rule that when Playerl moves
piece, as long as the number of patterns is bigger than 0, the first input
associated with the sequence of representation is 1 and the second input is 0.
Similarly when player 2 moves piece, as long as the number of patterns is bigger
than 0, the first input associated with that sequence is 0 and the second input is 1.
Two input neurons are used to represent who place piece. If Player1 is the
first-hand, the first input will be 1 and the second 0. On the contrary, the first
input is 0 and the second 1 if player 2 is first-hand. There are total 192 + 80 + 2
=  274 inputs.
                                          4

Table 1 Coding method used in the presented program(n denotes the number of a pattern)
Value of Input1                  Input2     Input3    Input4     Input5
n
0             0                  0          0        0           0
1             1                  0          0        0           0
2             1                  1          0        0           0
3             1                  1          1        0           0
4             1                  1          1         1          0
   >4         1                  1          1         1          (n-4)12
The evaluation function is calculated as follows:
 hi(t) =        j1xj(t)
                ni      w,(),(),                                                       (1)
 gi(t) = 1+exp-     (t) '                                                              (2)
 p(t) = E 1w (2)(t)gi(t)                                                               (3)
 V(s(t))   = 1+exp-P( t)                                                               (4)
Among them,
         w indicates the weight from the jth input node to the ith hiddennode;
      xindicatesthe jth input of the evaluation neural network;
      nindicates totalnumber of input nodes;
      hiindicatesthe ith hidden node input;
      giindicates the ithhidden node output;
      wi indicatesthe weight from the ith hidden node to theoutput node;
      mindicates total number of hidden nodes;
      pindicates the input of theoutput node (before function operation)
      V(s) indicates the output of the evaluation neural network, winning
probability in the state S.
                                                5

     At this point, the evaluation of each situation are stored in the neural
network, and learning evaluation function also need to provide feedback
function. Even before the end of a game, it is hard for an expert to give an
accurate probability of winning for each situation, so an immediate return of 0 is
obtained after each move. When the game is over, if it wins, it can get an
immediate return of 1; if it fails, it can get an immediate return of -1; and if it
draws, it get an immediate return of 0.5. Since reinforcement learning is
unsupervised and there is no correct state-V sample for training, theprediction
error is defined as follows:
     e(t) = a[r(t + 1) + V(s(t + 1)) - V(s(t))]                               (5)
and the objective error to be minimized is
          1                                                                  (6)
E(t) = - e 2(t)
          2
ais the learning rate in reinforcement learning. The weight update rule for the
neural network is based on the gradient anti-pass method, described by:
Aw(t)    =  -1(t)         ,                                                      7)
                  aw(t)
Where I is the learning rate of the evaluation neural network. Byapplying the
error back propagation mechanism, the weights areupdated as follows:
                   (2 )               OE(t)                                     (8)
                 w     t)=l2(t)[-]
                                     awi 2 (t)
                                           6

                  oE(t) de(t) OV(s(t))        dp(t)                             (9)
                  ( e(t) dV(s(t))  dp(t)    9wi(2) (t)
 = 12 (t)ae(t)exp-P(t)V 2 (s(t))g7 (t),                                       (10)
                                        E(t)                                  (1
                      i
                     Wj
                         ()=l1t[ ~
                        Aw~~        i
                                   ~~
                                                                              (12)
         1,(0 E(t) de(t) OV(s(t)) ap(t) Ogift)
                                                        dh(t)
             dett) OV~stt)) Op(t) dgitt)Ohjtt)dw (1)ct)
                                                                              (13)
 =li(t)ae(t)exp-P(C)V2(s(t))w 2(t)g?       (t)exp-'i(t)       .
      3)     training process design
      Next, in process of designing the training of intelligent Gomoku system,
the present invention designs two identical AI that use the same neural network
and perform a large number of games through the platform Piskvorky, as shown
in Fig 3. Because the two AI are two separate executables, a neural network
needs to be shared by one data file (actually a text file), and the weight
parameters of the neural network are stored in a text file. Suppose Playerl is the
first-hand, thenPlayerl reads the weight of the neural network in the text file,
and walk the first move according to the neural network. After that, Player2
reads the weight of the neural network in the text file, and go the second step
according to the neural network, At the end of the two steps (the first transition
ends), Player2 updates weights according to (10) and (13) and saves the updated
weights back to the original text file. Then, Player1 reads the weight of the
neural network in the text file and takes the third step according to the neural
network. At the end of the third step, Player1 updates the weight according to
(10) and (13), and saves the updated weight back to the original text file ..., so
repeated until a game of Gomoku ended. The time required for this version of AI
                                         7

to play 60,000 games is 30 hours. The speed of file operations relative to the
memory operation is quite slow, in order to speed up the update of reading the
weight of the speed, it can use shared memory method to achieve data
interaction between two Al processes.
      4)    Gomoku strategy selection
     On Gomoku's chessboard, each empty position is a workable choice to
place piece, but most of the good choices appears near already occupied
positions, so the action set can be compressed to consider only the positions near
the black and white as actionable ones. When using neural network to select
motion, we traverse each feasible motion and then calculate the value of V
transferred to the scene, the motion which result in the maximum value of V is
the optimal motion. In the learning training stage, we found that if we choose the
optimal prodigal every time, the neural network will converge after a few
hundred games, and the Gomoku game will be exactly the same afterwards. To
avoid this, we used two kinds of action exploration strategies. The first
exploration strategy is that whether Player2 is the first-hand or not, the first
move is random, but if Playerl is the first-hand, then the first move should be in
the tengen, and if the backhand, the first move is random. The second
exploratory strategy employs the c-greedy strategy applied to the strategy after
the first move of both sides. After training for some time, remove the first
exploration strategy, leaving only the second exploration strategy. We use two
methods to speed up the training. The first method is to train 50 times instead of
1 time for each state transition. The second method is to directly connect five
pieces when it is detected that five pieces can be connected.
                                          8

CLAIMS
1. A training system of Gomoku Al, being built to cultivate a Gomoku Al with
   candidate level by the way that allows two Gomoku Al against each other.

<U+271F><U+2720><U+2721><U+261B><U+261E><U+270C><U+270D><U+270E><U+270F><U+270E><U+270C><U+270D><U+270D><U+2711>
<removed-date>
<removed-apn>
                 <U+2701><U+2702><U+2704><U+260E><U+2706><U+271D> <U+271E>

<removed-apn> <removed-date>
           <removed-apn>   <removed-date>
<U+2701><U+2702><U+2704><U+260E><U+2706><U+271D> <U+271E>

