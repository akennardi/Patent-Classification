   ABSTRACT
   COMPUTER-IMPLEMENTED METHOD AND SYSTEM FOR ANALYSING DATA IN A
   DATASET
 5
   A computer-implemented method for analysing data in a dataset, comprising:
   processing the dataset using a regularization model; and applying a regression model to
   the processed dataset to determine, based on importance, a group and a variable within
   the group; wherein the regularization model is arranged to approximate to the
10 combination of Lq and Lp regularizations; wherein 1/2 s q < 1 and 1 s p < 2; and
   wherein p and q are adjustable for evaluating grouping effect and sparsity of the dataset.
   [Figure 1]

    En
    cn   (DIA
        cnf
                    ~c
               "I
~ct
    0 n           0
            cn         0
            (D c0

                                                             1
       COMPUTER-IMPLEMENTED METHOD AND SYSTEM FOR ANALYSING
                                              DATA IN A DATASET
    TECHNICAL FIELD
  5
             The invention relates to a computer-implemented method and system for
    analysing data in a dataset. Particularly, although not exclusively, the invention relates
    to a complex harmonic regularization method arranged for group variable selection.
10  BACKGROUND
             Variable selection has recently attracted much attention in machine learning
    field. The issue of grouping effect and high correlation variables are widely found in
    high-dimensional data analysis. In recent years, regularization methods are commonly
 15 used approaches for variable selection. The limitation of the existing regularization
    methods is that a L2 penalty is usually used to evaluate the grouping effect, and a Lq
    (o<q<1) penalty with a fixed value of q is used to evaluate the variable sparsity
    respectively. The existing methods often require the data to satisfy a fixed probability
    distribution, or require assumptions be made on the probability distribution of the data.
20
    SUMMARY OF THE INVENTION
             In accordance with a first aspect of the invention, there is provided a computer
    implemented method for analysing data in a dataset, comprising: processing the dataset
25  using a regularization model; and applying a regression model to the processed dataset
    to determine, based on importance, a group and a variable within the group; wherein
    the regularization model is arranged to approximate to the combination of Lq and Lp
    regularizations; wherein 1/2 s q < 1 and 1 s p < 2; and wherein p and q are adjustable
    for evaluating grouping effect and sparsity of the dataset. The determination may
30  provide one or more groups, or one or more variables within an identified group, or
    both.
             In one embodiment of the first aspect, the regularization model is expressed as:
                        argrmin
                                 (1 h
                                                Y,,3) + A, E m (0) + A2
                                                                           k
                                                                                  }
35  wherein o < a, b < 1,
                                                     2
                    n(          2    1,2+1-b)          1  -b
                           6(b +1)             b         b_
                         11k                                         k
                       _yi0-                             Zm(ij) + A2Zn(,)
    wherein =                  is a loss function, -,               -1    is a penalty function, and /b and
    A2  are tuning parameters; and wherein a and b are adjustable.
40           In one embodiment of the first aspect, the regression model comprises a logistic
    regression model.
             In one embodiment of the first aspect, the logistic regression model is arranged
    to apply a path seeking method.
45
             In one embodiment of the first aspect, the path seeking method comprises:

                                                                 2
            initialling a path          =     OM );           l
            computing a vector T(v) using the following:
                         aR(B)
                            a(-lh-Ei=   yi Xi3 + logo1 -v-X       )
                                                        - n2)
                                   3
                             (y2           +
                                          )|j        (
                      -1
                           h1+0
                                         21 -   r-|#
                             2(     k 1)(0)      + (1 -    _Y)E- 1 .(0j))
                             V2.(- + 1) 10      J~      1)2
                              +       2(1 - -y) [ij
                   r(0)=   W___0
  5
                 identifying coefficients 1(v) *o with sign opposite to that of the
                 corresponding cj(v);
                 when set S is empty, selecting a coefficient corresponding to the largest
                 component of T(v) in absolute value;
10               when set S is not empty, selecting a coefficient with corresponding largest
                 IT(v) I within a subset;
                 increasing the selected coefficient Pj*(v) in the direction of the sign of the
                 corresponding Tj*(v) to determine a solution for the next point v + Av of the
                 path; and
 15              performing iterations until all components of c(v) are zero.
            In one embodiment of the first aspect, the dataset comprises simulated data and
    real data.
20          In one embodiment of the first aspect, the dataset comprises gene data.
            In one embodiment of the first aspect, the gene data is related to cancer disease.
            In one embodiment of the first aspect, the cancer disease is prostate cancer,
25  lymphoma cancer, or lung cancer.
            In one embodiment of the first aspect, the computer-implemented method
    further comprises outputting the determined group and variable on a display.
30          In accordance with a second aspect of the invention, there is provided a
    computer system for analysing data in a dataset, comprising: a processor arranged to:
    process the dataset using a regularization model; and apply a regression model to the
    processed dataset to determine, based on importance, a group and a variable within the
    group; wherein the regularization model is arranged to approximate to the combination
35  of Lq and Lp regularizations; wherein 1/2 s q < 1 and 1 s p < 2; and wherein p and q are
    adjustable for evaluating grouping effect and sparsity of the dataset.

                                                                         3
             In one embodiment of the second aspect, the regularization model is expressed
    as:
                      -arg min         ( 1-   h
                                                           Y,,-3)2 + A       m(+2)+ k7 n()
                                              i=1                          1           j=1
                  wherein o < a, b < 1,
                                  b+1 2                   1Y)        1-b
  5
    wherein i=
                 Z
                 1 (th
                                    is a loss function, -
                                                                    -AZm 3,)+ A2Zn (03,)
                                                                                -     is a penalty function, and A1 and
    A2 are tuning parameters; and wherein a and b are adjustable.
             In one embodiment of the second aspect, the regression model comprises a
10  logistic regression model.
             In one embodiment of the second aspect, the logistic regression model is
    arranged to apply a path seeking method.
 15          In one embodiment of the second aspect, the processor is arranged to apply the
    path seeking method by:
             initialling a path = 0{()                           = a}i;
             computing a vector T(v) using the following:
                             '0j.
                           ,9(_,     h      IYXifi1            9 (mXio))
                         = ~            yi            +1og
                           3h i=13                     X3
                            h                       exU3
                                   2(y
                                     1    -     +#)
                          26(b+          +            1-
                                                     (1
               j(v)    I          1,4
20           identifying coefficients                  p3(v)    to with sign opposite to that of the corresponding
             when set S is empty, selecting a coefficient corresponding to the largest
             component of T(v) in absolute value;
             when set S is not empty, selecting a coefficient with corresponding largest|jv)
25           within a subset;
             increasing the selected coefficient 1j*(v) in the direction of the sign of the
             corresponding Tj*(v) to determine a solution for the next point v + Av of the path;
             and

                                                         4
            performing iterations until all components of c(v) are zero.
            In one embodiment of the second aspect, the dataset comprises gene data.
  5         In one embodiment of the second aspect, the gene data is related to cancer
    disease.
            In one embodiment of the second aspect, the computer system further comprises
    a display arranged to output the determined group and variable.
10
            In accordance with a third aspect of the invention, there is provided a non
    transitory computer readable medium for storing computer instructions that, when
    executed by one or more processors, causes the one or more processors to perform a
    method for analysing data in a dataset, comprising: processing the dataset using a
 15 regularization model; and applying a regression model to the processed dataset to
    determine, based on importance, a group and a variable within the group; wherein the
    regularization model is arranged to approximate to the combination of Lq and Lp
    regularizations; wherein 1/2 s q < 1 and 1 s p < 2; and wherein p and q are adjustable
    for evaluating grouping effect and sparsity of the dataset.
20
            In one embodiment of the third aspect, the regularization model is expressed as:
                       argrmn
                               ( 1   h
                                            Y,3) + A, E m (0) + A2
                                                                    k
                                                                           }
                wherein o < a, b < 1,
                                                   2
                   M()=                   1     21b 11a
                          6(b +1)                     b_
                        11k                                     k
                     _y0)
                     -(3;                                  + A2Zn/
25  wherein =                 is a loss function, -,           -1  is a penalty function, and Ai and
    A2 are tuning parameters; and wherein a and b are adjustable.
    BRIEF DESCRIPTION OF THE DRAWINGS
30          Embodiments of the invention will now be described, by way of example, with
    reference to the accompanying drawings in which:
          Figure 1 is a flow chart showing a method for identifying important data from a
    dataset in one embodiment of the invention;
35
          Figures 2A to 2F show the plots of a complex harmonic regularization model in
    one embodiment of the invention, in particular: Figure 2A shows a set of curves that
    represent the first part of the complex harmonic regularization at different parameter a
    values; Figure 2B shows a solid curve that represents the first part of the complex
40  harmonic regularization at the parameter a = 0.99 and a dashed curve that represents
    L1/2 regularization; Figure 2C shows a solid curve that represents the first part of the
    complex harmonic regularization at the parameter a = 0.01 and a dashed curve that
    represents Li regularization; Figure 2D shows curves that represent the second part of
    the complex harmonic regularization at different parameter b values; Figure 2E shows a
45  solid curve that represents the second part of the complex harmonic regularization at
    the parameter b = 0.01 and a dashed curve that represents L2 regularization; Figure 2F
    shows a solid curve that represents the second part of the complex harmonic

                                                      5
    regularization at the parameter b = 0.99 and a dashed curve that represents Li
    regularization;
           Figures 3A to 3F show the plots of exact solutions of different penalized methods,
  5 in particular: Figure 3A shows the exact solution of Lasso; Figure 3B shows the exact
    solution of Elastic net; Figure 3C shows the exact solution of SCAD; Figure 3D shows the
    exact solution of SCAD-L2; Figure 3E shows the exact solution of the first part of the
    complex harmonic regularization in one embodiment of the invention; Figure 3F shows
    the exact solution of the second part of complex harmonic regularization in an
10  orthogonal design (the regularization parameters of SCAD and SCAD-L2 in Figures 3C
    and 3D are X,= 1 and X,= 0.5, and a = 3.7);
           Figures 4A to 4E show the plots of the penalty functions (solid curves) and least
    squares functions at two different initial values for Lasso (Figure 4A), Elastic net (Figure
 15 4B), SCAD (Figure 4C), SCAD-L2 (Figure 4D), and complex harmonic penalty functions
    in one embodiment of the invention (Figure 4E); and
             Figure 5 is a functional block diagram of an information handling system
    arranged to implement the method of Figure 1 in one embodiment of the invention.
20
    DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT
             Figure 1 shows a computer-implemented method for analysing data in a dataset
    in one embodiment of the invention. The method 100 begins with step 102, in which the
25  dataset is processed using a regularization model that approximates the combination of
    Lq (1/2sq<1) and Lp (1sp<2) regularizations. After the processing, the method 100 then
    proceeds to step 104, to apply a regression model to the processed dataset to determine
    one or more important groups and one or more variables within the group. In the
    method 100, p and q are adjustable for evaluating grouping effect and sparsity of the
30  dataset. Optionally, the method 100 may further include displaying the result, i.e., the
    determined group(s) and variable(s) on a display. The result may be sorted in order of
    importance and displayed. In the present embodiment, the regression model is a logistic
    regression model that applies a path seeking method. The method 100 can be applied to
    different datasets, including a dataset that includes gene data related to cancer disease.
35
    Naive Harmonic Regularization
             The inventors of the invention have devised, through research, experiments and
    trials, that a united framework of the regularization in machine learning can take the
40  form:
                                      (A) = arg min [R(#) + AP(O)]
                                               P                   (1)
    where R(P) is a loss function, P(P) is a penalty function, and Xis a tuning parameter. In
    this expression, the P(P) = Lq (osqsi) penalty function performs variable selection by
    removing irrelevant variables with very small estimated coefficients. Lq regularization
45  with o<q<1 was studied as it can find solutions sparser than those found with Li
    regularization, and it is easier to solve than Lo regularization.
             Nevertheless, the applications on the Lq penalty function with o<q<1 not often
    attract much attention because the regularization problem changes from a convex Li
50  penalty to a non-convex Lq penalty when o<q<1, and so the corresponding optimization
    problem is difficult to solve. Another problem is that the first derivative of the penalty
    function at origin is infinity, and this renders ordinarily optimization algorithms invalid.

                                                           6
             In one embodiment of the invention, there is provided a harmonic regularization
    method which can at least partly approximate the Lq penalty with 1/2 sq<1. The naive
    harmonic regularization scheme, disclosed in G.-J. Chu, Y. Liang, and J.-X. Wang,
    "Novel harmonic regularizationapproachfor variable selection in coxs proportional
  5 hazards model," Computational and mathematical methods in medicine, vol. 2014,
    2014, can be expressed as:
                                                         k                               a
                    =arg   min{      04n xN)2              a   a      I1)9 (1a)2       1
                                                       0    Z+     )a                      '(2)
    where o<a<1.
10           When the shrinkage parameter a is close to o, NI 1 + -- 4 , the naive
    harmonic regularization approximates the Li regularization. When the parameter a is
    close to 1,    a-(-rl) 1+ (10)2 :      ' the naive harmonic regularization approximates the
    L1/2    regularization. Moreover, comparing with Lq (1/2 q<1) penalties, the naive
    harmonic regularization has the property that its first derivative is finite at origin, which
 15 implies that the corresponding regularization problem can be efficiently solved via direct
    seeking techniques.
    Complex Harmonic Regularization For Group Variable Selection
20           The inventors of the invention have devised, through research, experiments and
    trials, that in many scientific and engineering applications, covariates are naturally
    grouped. When the group structures are available among covariates, there is generally
    an interest to identify both important group(s) and important variables within the
    identified or selected group(s).
25
             In one embodiment of the invention, there is provided a complex harmonic
    regularization (CHR) method for group variable selection, and an efficient group-level
    path seeking algorithm to fit the model and to solve the problem. The method can
    identify relevant variable groups as well as select important variables within the group.
30
             In this embodiment, the complex harmonic regularization (CHR) method can
    approximate the combination of the Lq (1/2sq<1) and Lp (1sp<2) penalties. The Lq
    (1/2 q<1) penalties can be taken as an efficient tool for key variable selection, and Lp
    (1 p'<2) penalties can be taken as a efficient tool for grouping effect correlation. The
35  complex harmonic regularization scheme can be expressed as:
                           /=argrmin~k         (zb  j 4 )2 + A, Ev           A   k(, A
                                                 5=1j=1                         j=1
     where 0 < a, b < 1,
                                    rn(.)=
40           The complex harmonic regularization of the above embodiment is plotted in
    Figures 2A to 2F. When the parameter a is close to o, m(p) z IPI (Figure 2C), the first
    part of the complex harmonic regularization approximates Li regularization. When the
    parameter a is close to 1, m() = p I | (Figure 2B), the first part of the complex harmonic
    regularization approximates L1/2 regularization. When the parameter b is close to o,

                                                     7
    n(3) z Ip12 (Figure 2E), the second part of the complex harmonic regularization
    approximates L2 regularization. When the parameter b is close to 1, n() = | I (Figure
    2F), the second part of the complex harmonic regularization approximates Li
    regularization. Moreover, comparing with Lq (1/2 q<i) penalties, the first derivative of
  5 the complex harmonic regularization is finite at origin. This implies that the
    corresponding regularization problem can be efficiently solved via the direct seeking
    techniques.
             In the regression penalized with the model y = XP + e, the characteristics of
10  coefficient estimators were proposed in T. Hastie,R. Tibshirani,and J. Friedman,"The
    elements of statistical learning. 2001," NY Springer, 2001 And L. Zeng and J. Xie,
    "Group variable selection via scad-12," Statistics, vol. 48, no. 1, pp. 49-66, 2014 as
    follows:
         * Unbiasedness: the resulting estimator should have low bias when the true
 15          parameter is large to avoid model bias;
         * Sparsity: the resulting estimator can automatically set small estimated
             coefficients to zero for variable selection;
         * Continuity: the resulting estimator should be continuous in data to avoid
             instability in model prediction; and
20       *   Grouping effect: the penalty function should encourage highly correlated
             predictors to be in or out of the model at the same time so that the whole group
             will be selected.
             The above properties of several penalized methods can be plotted as shown in
25  Figures 3A to 3F. Figures 3A to 3F show five methods of penalized least squares, Lasso,
    Elastic net, SCAD, SCAD-L2 and complex harmonic regularization with an orthogonal
    design matrix in the regression model. The expressions of the five penalized methods
    indicate that they satisfy properties of sparsity, and continuity. SCAD, SCAD-L2and
    CHR penalties satisfy the property of unbiasedness to easily construct unbiased
30  estimators when the coefficient is large. On the contrary, Lasso and Elastic net
    estimators are biased.
             In genomic analysis, the focus is often on selecting genes in group that function
    together as a common biological pathway. Based on the Elastic net method, the other
35  property for penalty functions is added as coefficient estimators of penalized least
    squares.
             In one embodiment of the invention, the method first assumes that X is
    orthonormal matrix, that is, XTX = I. The method then builds a common estimator for
                                                         d
                                              PA (!) =EP    )
40  the regularization approaches. Let                 i=1    be a penalty function, where X
    denotes control parameters for regularization, either a single parameter X or (Xi, X2)
    when there are two penalty terms.
                     Figures 4A to 4E illustrates the penalized least-squares functions for
45  Lasso, Elastic   net, SCAD, SCAD-L2 and complex harmonic penalty functions. The solid
    curves show the penalty functions and the two dashed curves are the penalized least
    squares functions at two different initial values of the coefficients. As shown in Figures
    4A to 4E, the penalized least-squares functions work well for all five penalty functions.
50  Path Seeking Algorithm for the CHR Penalty

                                                                 8
             The method also includes a generalized path seeking algorithm to solve the
    complex harmonic regularization (CHR) with a logistic regression model. The logistic
    regression model can be defined by Equation (4) below:
                                            eXt f
                    P(yi = 1Xi)= 1 +e                                                          (4)
  5
                       With the loss function:
              fi(0) = arg min -             yt -XaJ + log(] + e-3)                                  (5)
    and in equation (3), let               1+%   2                 the CHR scheme for the logistic regression
    can be re-expressed as:
                                                    k               k
          /3= argmin         N(O)+A(7             m(#g) + (1 - 7)     n(A))                           (6)
10
             As mentioned above, when Lq (o<q<1) regularization is applied, the
    computational challenge is how to solve the optimization of the Lq (o<q<i) penalty
    efficiently. Fortunately, direct path seeking approaches can overcome it. The advantage
 15 of path seeking methods is that it provides a way of solving regularization with non
    convex penalty. Let v measure length along the path and Av > o be a small increment.
    Define
                y(v)
                  =[O           )]
                                                                                                     (7)
                           (-         ,. yh Xio       _<g(1 +e      )
                       =1          ,j    i
                                               X+           +h
                   -jV Opp)                                                                          (8)
                       -9(_y E_  1 m(3 )+ (1 - 7)Z            I n($))
                                                       2
                           2a(a +1)|[3| + (1 -a          )2
                  +             2(1 - -y)|     |
                         2b(b + 1) |#J2 + (1 -       62)2
            ry (v)                                                                                   (9)
20           Here <pj(v) and 4j(v) are the jth component of the gradient of loss function
    equation (5) and the penalty function evaluated at the path point 1(v) respectively. The
    components of the vector j(v) are the component ratios of these two gradients. Then,
    the path seeking algorithm for the CHR regularization in logistic regression is provided
    as follows.

                                                              9
                     Algorithm I The path seeking algorithm for the CHR penalty
                       I: Initialize: V = 0. {Q(0) - 0}k
                      2: repeat
                      3:     Compute {iry (v)}
                      4:     S = {jlTj(v) - MV) < 0}
                      5:     if S   =  empty then
                      6:        j*      arg maxj |rj(v)
                      7:     else
                      8:         j =arg naxjEs Irj((v)
                      9:     end if
                     10:     ,3~ (v. 4~- Lv) ,d*(L) + A           v  - sign (7j - (il))
                     12:     v <-- P + Ar'
                     13 until r(v) = 0
             After initializing the path, the vector c(v) is computed using equations (7)-(9).
    Then those coefficients (v) to with sign opposite to that of the corresponding Tj(v) are
  5 identified. When set S is empty, the coefficient corresponding to the largest component
    of T(v), in absolute value, is selected at Step 6. And when there are one or more elements
    in the set S, the coefficient with corresponding largest ITj(v) I within this subset is instead
    selected. The selected coefficient pj*(v) is then incremented by a small amount in the
    direction of the sign of its correspond Tj*(v) with all other coefficient remaining
10  unchanged, producing the solution for the next path point v + Av. Iterations continue
    until all components of T(v) are zero and the algorithm then reaches a regularized
    solution for the CHR penalty.
    Experiment Results
 15
             To demonstrate the performance of the proposed CHR procedure, data is
    simulated from:
                                         log(          = X0 + o-,      F ~ N(O, 1)
    where X~N(o, 1), e is the independent random noise and u is the control parameter for
20  noise . In each scenario, there were total of 2000 predictors, and the simulated data
    consisted of a training set and test set with 200 samples for both sets in first and second
    scenarios, and 400 samples for both sets in third scenario.
             For Scenario 1, the simulation provided one group of the correlated features with
                                                 3 = (2, -2,2, -2, --- ,2,-2, 0             0).
25  noise parameter       u = 0.3, and                             100                  100      .  A grouped variable
    situation £i=pXXI+(l-p)x3i2=2,3                         - 99,100.        was simulated, where p is the
    correlation coefficient of the grouped variables.
             For Scenario 2, all was similar as the first scenario, except there were one group
30  variables and 100 independent non-zero variables:
                                (2, -   2, -2,     2, -2,0,      .0, 15,-2, 1.7,3, -1,0,           0).
                                             100             100            5x20            1700
             For Scenario 3, the noise parameter a was up to 0.4 and there were two groups
    variables and 100 independent non-zero variables:

                                                                          10
                                    3 = (2, -2,2, -2,-          2, -2,0,...   ,0,1.5, -2,1.7,3, -1,0,        ,0,
                                                      50                   50             Sx10            50
                                          3, 3, -3,0,..          0,3,3, 3-3 -20..           0).
                                               50         50             5Xi0          1650
              Defined two grouped variables:
                                   xi= p xx1 + (1 - p) x x, i                    =2,3,         - ,49,50;
                                   Xi = p x 101+(1 - p) x xi=                          102,103,--- ,149,150.
 5
              In the experiment, p = 0.1, 0.4, 0.7 respectively, and the Lasso and Elastic net
   were conducted by Glmnet available at http://web.stanford.edu/-hastie/glmnet
   matlab/. A 10-fold cross validation (CV) was conducted on training set for tuning
   parameters of all approaches. Note that SCAD-L2 has three parameters a, X1 and X2, and
10 a = 2, 2.25, 2.5, 3, 3.5, 3.7 respectively, X1 and X2 were tuned by 10-fold CV. Each
   scenario was repeated 100 times and the classification accuracy on the test sets was
   computed. Additionally, both sensitivity and specificity for each approach were
   calculated as follows:
                             TruePositive(TP):=             3.*     $   , TrueNegative(TN) := P. *                /3
                           FalsePositive(FP):=             /.    *      ,  FalseNegative(FN):= '3. *0                   ,
                                                                 TP                                   TN
                                     Sensitivity:= TP + FN , Specificity
                                                                                                   TN + FP'
15 where the .* is the element-wise product, and 1.1 calculates the number of non-zero
   elements in a vector, /3 and / are the logical not operators on the true coefficients vector
     and the simulated P.
             As shown in Table I, the proposed CHR approach gave higher or comparable
20 accuracies than other penalties. The CHR penalty also resulted in much higher
   sensitivity in identifying the relevant features. For example, in Scenario 1 with p = o.1,
   CHR method gained Accuracy 99.85% with perfect sensitivity and perfect specificity.
   When the correlation coefficient p increases, the specificities of all methods are
   somewhat decreased, but not greatly as compared to the achieved in sensitivities. This
25 means that high correlations among the variables are the difficult problem in variable
   selection.
                                                                      TABLE I
                                                       RESULTS OF THE SIMULATION*
                                                                                   Scenario
                  p   Approaches        1         2           3          1         2          3         1            2       3
                                              Sensitivity                     Specificity                        Accuracy
                         Lasso       0.948      0.690     0.638       0.995      0.989     0.961    98.32%        85.16%  74.70%
                          L1/2       0.951      0.700     0.655       0.998      0.989     0.975    98.33%        88.01%  77.66%
                 0.1   SCAD-L 2      0.973      0.772     0.740       0.999      0.975     0.944    99.60%        83.70%  80.60%
                      Elastic net    0.954      0.757     0.684       0.994      0.971     0.923    98.80%        82.90%  79.23%
                      L1/2 + L2      0.960      0.802     0.793       0.999      0.985     0.934    99.63%        82.60%  85.70%
                          CHR        0.981      0.859     0.823       0.999      0.997     0.978    99.85%       91.56%   87.47%
                         Lasso       0.724      0.553     0.480       0.985      0.975     0.914    92.50%        77.43%  74.29%
                          L1/2       0.738      0.616     0.540       0.989      0.977     0.916    93.30%        78.62%  75.32%
                 0.4   SCAD-L        0.917      0.770     0.732       0.990      0.979     0.948    96.30%        79.50%  76.14%
                      Elastic net2   0.885      0.720     0.720       0.988      0.960     0.936    92.20%        74.53%  69.28%
                      L 1/ 2 + L2    0.936      0.792     0.738       0.992      0.989     0.944    96.90%        82.68%  77.21%
                          CHR        0.950      0.806     0.766       0.993      0.990     0.953    97.65%       85.99%   79.52%
                         Lasso       0.636      0.476     0.473       0.973      0.952     0.895    91.40%        68.90%  65.26%
                          L1/2       0.652      0.530     0.503       0.978      0.958     0.902    92.00%        73.40%  70.40%
                 0.7   SCAD-L        0.816      0.731     0.630       0.981      0.967     0.924    92.70%        72.90%  67.51%
                      Elastic net2   0.795      0.690     0.611       0.977      0.955     0.912    90.00%        70.40%  64.47%
                      L 1/ 2 + L2    0.891      0.745     0.675       0.982      0.969     0.927    93.53%        75.54%  68.20%
                          CHR        0.936      0.785     0.717       0.985      0.978     0.932    95.46%       80.31%   77.95%
   *the best performances are in bold; CHR: complex harmonic regularization

                                                                11
               Next, experiments were performed to demonstrate the CHR method by analysing
    three common public datasets named Prostate, Lymphoma and Lung cancer. The
    prostate           and        lymphoma             cancer        datasets           can        be         found         at
    http://ico2s.org/datasets/microarray.html,_the Lung cancer dataset can be found at
  5 www.ncbi.nlm.nih.gov/geo, under access number: GSE40419. A brief description of
    these three datasets is given below and summarized in Table II. The Prostate dataset
    contains 12,600 genes for 50 normal tissues and 52 prostate tumor tissues. The
    lymphoma dataset contains 7,129 genes for 58 diffuse large B-cell lymphomas (DLBCL)
    and 19 follicular lymphomas (FL). The first two datasets were pre-processed based on
10  the method in E. Glaab, J. Bacardit,J. M. Garibaldi,and N. Krasnogor, "Using rule
    based machine learning for candidate disease gene prioritization and sample
    classificationof cancer gene expression data," PloS one, vol. 7, no. 7, p. e39932, 2012.
    The lung cancer dataset is a RNA-sequencing (RNA-seq) dataset which contains 22,401
    genes for 87 lung adenocarcinomas and 77 adjacent normal tissues. The lung cancer
 15 dataset was pre-processed based on the method in J.-S. Seo, Y. S. Ju, W.-C. Lee, J.-Y.
    Shin, J. K. Lee, T. Bleazard, J. Lee, Y. J. Jung, J.-O. Kim, J.-Y. Shin et al., "The
    transcriptionallandscape and mutational profile of lung adenocarcinoma,"Genome
    research,2012.
                                                            TABLE II
                                                 THE THREE REAL DATASETS.
                                 Dataset            No. samples       No. genes            Classes
                              Prostate              102 (77 / 25)        12,600        Nornal/Tumor
                            Lymphoma                77 (57/ 20)          7,129           DLBCL /FL
                           Lung cancer             168 (124 / 40)        22,401        Normal/Tumor
20
    FL: follicular lymphomas; the notation ./. represents the number of samples in the training and test sets respectively.
               Each of the three datasets was random partition into training set (70% - 80%)
    and test set (20% - 30%). The training and the test classification accuracies on these 3
25  real datasets were calculated using 6 approaches, whose parameter(s) were tuned by 10
    fold CV. Each approach was repeated 50 times. The averaged classification results were
    summarized in Table III. Generally, the test accuracies of CHR penalty on real datasets
    are higher than 93% with the best performances among those six classifiers.
                                                           TABLE III
                                                 RESULT OF REAL DATASETS*
                     Datasels      Approaches      Training accuracy     Test accuracy      No. selected genes
                                       Lasso             96.09%             92.08%                   12
                                       L1/2              96.00%             92.08%                    7
                     Prostate       SCAD-L               96.09%             91.31%                   21
                                              2
                                    Elastic net          9625%              92.40%                   15
                                   L1/ 2 + L 2           97.50%             93.88%                   11
                                       CHR               98.00%             94.32%                   19
                                       Lasso             96.33%             87.47%                   18
                                       L1/2              95.25%             91.00%                   14
                                    SCAD-L2              96.t0%             91.40%                   24
                    Lymphoma
                                    Elastic net          95.98%             93.67%                   31
                                   L1/ 2 + L 2           96.39%             92.00%                   16
                                       CHR               98.28%             93.83%                   17
                                       Lasso             98.33%             92.51%                   10
                                       L 1/ 2            98.38%             91.50%                    9
                                    SCAD-L 2             97-85%             92.29%                   25
                  Lung cancer
                                    Elastic net          97.18%             91.38%                   24
                                   L1/2 + L 2            98.63%             92.34%                   15
                 L___         _        CHR               99.19%             93.56%                   23
30  *the best performances are in bold; CHR: complex harmonic regularization

                                                        12
              The top 10 selected lung cancer genes by the six regularization methods are
   shown in Table IV. As shown in this table, AGER (advanced glycosylation end product
   receptor) is the first selected by the CHR method and also appears using Elastic net and
 5 L1/2+ -L2 approaches. AGER belongs to the immunoglobulin superfamily (IgSF) of cell
   surface molecules, which has been documented in a large number of primary human
   carcinomas in recent years, especially in lung cancer. B. Barling, H.-S. Hofmann, B.
   Weigle, R.-E. Silber, and A. Simm, 'Down-regulation of the receptorfor advanced
   glycation end-products (rage) supports non-small cell lung carcinoma,"
10 Carcinogenesis,vol. 26, no. 2, pp. 293-301, 2005 verified down-regulation of advanced
   glycation end-products (RAGE, also called AGER) in human lung cancer might
   contribute to a loss of epithelial tissue structure and concomitant oncogenic
   transformation without restriction of cell proliferation in epithelial carcinomas.
   Tenascin-X (TNX) is a member of the tenascin family of large oligomeric glycoproteins
15 of the extracellular matrix. In humans, TNX is encoded by the TNXB gene (see M. K.
   Tee, A. A. Thomson, J. Bristow, and W. L. Miller, 'Sequences promoting the
   transcription of the human xa gene overlapping p450c21a correctly predict the
   presence of a novel, adrenal-specfic,truncatedformoftenascin-x,"Genomics, vol. 28,
   no. 2, pp. 17-178, 1995) that is selected by lasso, SCAD-L2 and CHR in Table IV. K-i.
20 Matsumoto, N. Takayama, J. Ohnishi, E. Ohnishi, Y. Shirayoshi,N. Nakatsuji, and H.
   Ariga, "Tumour invasion and metastasisam promoted in mice deficient in tenascin-x,"
   Genes to cells, vol. 6, no. 12, pp. 1101-1111, 2001 verified that TNX plays a part in
   tumor invasion. Similarly, EIF4E1B is a member of eIF4E family. J. R. Graff, B. W.
   Konicek, J. H. Carter, and E. G. Marusson, Targeting the eukaryotic translation
25 initiation factor 4efor cancer therapy," Cancer research, vol. 68, no. 3 pp. 631-634,
   2008 illustrated the levels of free eIF4E are commonly elevated consequent to increased
   eIF4E expression or release of eIF4E from 4EBPs, a result of signaling through the
   AKT/mTOR and ras signaling pathways, which are frequently activated in a diverse
   range of human cancers.
30
               Increased eIF4E expression has been documented repeatedly in association with
   malignant progression in multiple human cancers, including lung cancer (see A. De
   Benedetti and J. R. Gmff, "if4e expression and its role in malignancies and
   metastases," Oneogene, vol. 23, no. 18, pp. 3189-3199, 2004). A similar case in other
35 results of the real datasets was observed, as shown in Tables V and VI respectively.
                                                    TABLE IV
                                  THE TOP 10 GENES IN THE LUNG CANCER DATASET
                Rank   Lasso          LI        SCAD-L2        Elastic net LiI, + L 2 CHR
                 I     KRTAP4-6       A2M       SCUBEI         ER]         NCFI       AGER
                2      FAM150B        A4GNT     MIR548Y        C]6orfO0    ABCAS      STX l
                3      RADIL          ABCAS     CAVI           ACER        PDLIM2     ABCA8
                4      MIR548Y        AATK      CIS132         ILlF9       MIR548Y    TNXB
                5      PIYH]PL        B9D2      103            CBLN4       MR98       E]F4ElB
                6      LGI3           BDKRB1    TNXB           ERAP4-4     AGER       PHYH]PL
                7      AKAP2          PNLIP     PHYHIPL        SIK2        CIorf54    CDC42EP2
                8      [3             CDON      KCNT2          RADIL       IG]3       CXXC1PI
                9      SCUBEI         CASQI     CCL19          FABPS       OR6C76     ZCCHC18
                 10    TNXr           CD244     ARHGAP44       CFD         CYP17AI    NCF1
   'thel Lest performances   in bold; CHRcomplex harmoniregularization

                                                                  13
                                                             TABLE V
                                           THE TOP 10  GENPS  IN THE PROSTATE DATASET.
                   Rank    Lasso      L 1/              SCAD-L-       Elastic net            L 1 , ±L.)   CHR
                    1      FTDSSI     PTGDS             ATP5I         PTDSSI                 HPN          HPN
                   2       ATP5I      JUNB              TP63          G0S2                   DUSP1        NELL2
                    3      S100A4     RPI 1-124D2.7     PrDSS1        S100A4                 NTRK1        MAF
                   4       AGR2       POLR2M            AGR2          TP63                   GSTA2        JUNB
                   5       XBPL       CYBA              MYOF          XBP                    TP63         KCNA5
                   6       HPN        RRAD              D4S234E       JUNB                   NELL2        ATP51
                   7       PTVDS      NR4A3             SLOOA4        ClQTNF3-AMACR          PTDSSL       TP63
                    8      SDCI       CTSG              PRKCB         FP236383.12            SERINC5      S100A4
                   9       JUNB       PRKCB             CTSG          SERP]NA3               PHLDA2       XBFI
                    10     CFD        ATP5I             MEl           JUND                   ATP5         DUSPI
                                                             TABLE VI
                                         TI-E TOP 10 GENES IN THE LYMPHOMA DATASET.
                     Rank    Lasso             L1 2         SCAD-L     2    Elastic nel   L 1 2 + L2    CHR
                      1      TLN2              TLN2         ESPLI           GANAB         UPK2          GABRA1
                     2       ARID4B            IFNA2        UPK2            NUP93         TLN2          OMD
                     3       BRD2              TGLV4-3      IFF01           MTR4668       CCL21         [EAR
                     4       [FF01             CSTA         PRDX6           UPK2          RRSI          PTPRT
                     5       RPL24             DLGAP5       IFNA1           OMD           IFFOI         CCL21
                     6       PRDX6             CCL2J        PNP             TLN2          CIRBP         FrL
                     7       CCL21             IFNAJ        IGHEPI          CCL2J         PSMA4         SRPR
                     a       RPL I-419C5.2     BRD2         CCL21           CYP4F3        CCRI          IF127
                     9       UPK2              ADIPOQ       KDR             RPL24         RPL24         LDHA
                      10     CIRBP             ESPLI        DTYMK           PRDX6         GANAB         CXCL1C
  5            Except for comparing with the common regularization techniques, the inventors
    have also compared the results obtained using the method of the present embodiment
    with other published results. Table VII illustrates these results. As shown in the Table,
    the accuracy of the CHR method in the embodiment of the invention is highest among
    the various approaches, and the number of selected genes is quite precise.
                                                            TABLE VII
                                                RESULT OF THE LITERATURES*
                         Dataser             Authors            Accuracy (CV)           No. of selected features
                                      T.K. Paul et al[34]            96.60%                        485
                                       Wessels et al.[35]            93.40%                         14
                                         Sheri et al.[36]            94.60%                    unknown
                         Pastate       Lecocke et a].!37             9010%                     unknown
                                      Dagliyan et al.[381            94,80%                    unknown
                                        Glaab et al.[26]             94.00%                         30
                                        Huang et al.15]              97.50%                         11
                                              CHR                    98.00%                         19
                                       Wesselset al. [35|            95.70%
                                          Liu et al.1391             93.50%                          6
                                        Shipp et al.24]              92.20%                         30
                                         Goh et a14401]              91.00%                         10
                                                 el al371            90,20%                    unknown
                        Lymphoma       Lecocke
                                          Hu et al.f41]              87.01%                    unknown
                                      Dagliyan et aL.[38]            92.25%                    unknown
                                        Glaab et al[26]              95.00%                         30
                                        Huang et al.[15]             96.39%                         [6
                                              CHR                    9828%                          17
10
    *the best performances are in bold
    [15] H.-H. Huang, X.-Y. Liu, and Y. Liang, "Featureselection and cancer classification via sparse logistic regression
    with the hybrid 11/2+ 2 regularization,"PloS one, vol. 11, no. 5, p. e0149675, 2016.
    [24] M. A. Shipp, K. N. Ross, P. Tamayo,A. P. Weng, J. L. Kutok, R. C. Aguiar, M. Gaasenbeek,M. Angelo, M. Reich, G.
15  S. Pinkus et al., "Diffuse large b-cell lymphoma outcome prediction by gene-expression profiling and supervised
    machine learning,"Nature medicine, vol. 8, no. 1, pp. 68-74, 2002.

                                                              14
     [26] E. Glaab, J. Bacardit, J. M. Garibaldi,and N Krasnogor, 'Using rule-based machine learningfor candidate
    disease gene prioritizationand sample classification of cancer gene expression data," PloS one, vol. 7, no. 7, p. e39932,
    2012.
     [34] T. K. Paul and H. Iba, "Extractionof informative genes riom microarraydata," in Proceedingsof the 7th annual
  5 conference on Genetic and evolutionary computation.ACM, 2005, pp. 453-460.
    [35] L. F. Wessels, M. J. Reinders, A. A. Hart, C. J. Veenman, H. Dai, Y. D. He, and L. J. van't Veer, "A protocolfor
    building andevaluatingpredictors of disease state based on microarraydata,"Bioinformatics, vol. 21, no. 19, pp. 3755
    3762,2005.
    [36] L. Shen and E. C. Tan, "Dimension reduction-basedpenalized logistic regressionfor cancer classification using
10  microarray data," JEEE/ACM Transactions on ComputationalBiology and Bioinformatics (TCBB), vol. 2, no. 2, pp.
    166-175, 2005.
    [37] M. Lecocke and K. Hess, "An empirical study of univariateand genetic algorithm-basedfeature selection in binary
    classificationwith microarraydata," CancerInformatics, vol. 2, 2006.
    [38] 0. Dagliyan, F. Uney-Yuksektepe, 1. H1. Kavakli, and M. Turkay, "Optimization based tumor classificationfrom
 15 microarraygene expression data,"PloS one, vol. 6, no. 2, p. e14579, 2011.
    [39] J. Liu and H.-B. Zhou, "Tumor classification based on gene microarray data and hybrid learning method," in
    Machine Learning and Cybernetics, 2003 InternationalConference on, vol. 4. IEEE, 2003, pp. 2275-2280.
    [40] L. Goh, Q. Song, and N. Kasabov, "A novel feature selection method to improve classification of gene expression
    data," in Proceedingsof the second conference on Asia-Pacificbioinformatics-Volume 29. Australian ComputerSociety,
20  Inc., 2004, pp.1 6 1-1 6 6 .
    [41] Y. Hu and N. Kasabov, "Ontology-basedframework for personalized diagnosis and prognosis of cancer based on
    gene expression data,"in NeuralInformation Processing.Springer,2008, pp. 846-855.
    Exemplary System
25
               Referring to Figure 5, there is shown a schematic diagram of an exemplary
    information handling system 200 that can be used to perform the method in the above
    embodiment. Preferably, the information handling system 200 may have different
    configurations, and it generally comprises suitable components necessary to receive,
30  store and execute appropriate computer instructions or codes. The main components of
    the information handling system 200 are a processing unit 202 and a memory unit 204.
    The processing unit 202 is a processor such as a CPU, an MCU, Raspberry Pi board, etc.
    The memory unit 204 may include a volatile memory unit (such as RAM), a non-volatile
    unit (such as ROM, EPROM, EEPROM and flash memory) or both. Preferably, the
35  information handling system 200 further includes one or more input devices 206 such
    as a keyboard, a mouse, a stylus, a microphone, a tactile input device (e.g., touch
    sensitive screen) and an image or video input device (e.g., camera). The information
    handling system 200 may further include one or more output devices 208 such as one or
    more displays, speakers, disk drives, and printers. The displays may be a liquid crystal
40  display, a LED/OLED display, or any other suitable display that may or may not be
    touch sensitive. The information handling system 200 may further include one or more
    disk drives 212 which may encompass solid state drives, hard disk drives, optical drives,
    flash drive, and/or magnetic tape drives. A suitable operating system may be installed in
    the information handling system 200, e.g., on the disk drive 212 or in the memory unit
45  204 of the information handling system 200. The memory unit 204 and the disk drive
    212 may be operated by the processing unit 202. The information handling system 200
    also preferably includes a communication module 210 for establishing one or more
    communication links (not shown) with one or more other computing devices such as a
    server, personal computers, terminals, wireless or handheld computing devices. The
50  communication module 210 may be a modem, a Network Interface Card (NIC), an
    integrated network interface, a radio frequency transceiver, an optical port, an infrared
    port, a USB connection, or other interfaces. The communication links may be wired or
    wireless for communicating commands, instructions, information and/or data.
    Preferably, the processing unit 202, the memory unit 204, and optionally the input
55  devices 206, the output devices 208, the communication module 210 and the disk drives
    212 are connected with each other through a bus, a Peripheral Component Interconnect
    (PCI) such as PCI Express, a Universal Serial Bus (USB), and/or an optical bus
    structure. In one embodiment, some of these components may be connected through a
    network such as the Internet or a cloud computing network. A person skilled in the art
6o  would appreciate that the information handling system 200 shown in Figure 5 is merely

                                                  15
    exemplary, and that different computing systems may have different configurations and
    still be applicable in the invention. Also, the information handling system 200 may take
    various forms, including one or more of: a mobile phone, a tablet, a portable computer, a
    desktop computer, etc.
  5
             Although not required, the embodiments described with reference to the Figures
    can be implemented as an application programming interface (API) or as a series of
    libraries for use by a developer or can be included within another software application,
    such as a terminal or personal computer operating system or a portable computing
10  device operating system. Generally, as program modules include routines, programs,
    objects, components and data files assisting in the performance of particular functions,
    the skilled person will understand that the functionality of the software application may
    be distributed across a number of routines, objects or components to achieve the same
    functionality desired herein.
 15
             It will also be appreciated that where the methods and systems of the invention
    are either wholly implemented by computing system or partly implemented by
    computing systems then any appropriate computing system architecture may be
    utilized. This will include stand-alone computers, network computers and dedicated
20  hardware devices. Where the terms "computing system" and "computing device" are
    used, these terms are intended to cover any appropriate arrangement of computer
    hardware capable of implementing the function described.
             The complex harmonic regularization (CHR) method in the above embodiment
25  can approximate the combination of Lq (1/2sq<1) and Lp (1sp<2) regularizations, with
    adjustable p and q to select the groups of the key relevant variables. The method can be
    solved effectively using a direct path seeking approach, which can globally optimize the
    regularization solutions with convex loss function and the non-convex penalty. The
    experimental results in both simulation and real datasets indicate that the CHR method
30  in the above embodiment is generally outperforms existing methods for solving binary
    classification and select several genes in groups.
             It will be appreciated by persons skilled in the art that numerous variations
    and/or modifications may be made to the invention as shown in the specific
35  embodiments without departing from the spirit or scope of the invention as broadly
    described. The present embodiments are, therefore, to be considered in all respects as
    illustrative and not restrictive.
             Any reference to prior art contained herein is not to be taken as an admission
40  that the information is common general knowledge, unless otherwise indicated.
45

                                                            16
    CLAIMS
    1. A computer-implemented method for analysis data in a dataset, comprising:
       processing the dataset using a regularization model; and
  5    applying a regression model to the processed dataset to determine, based on
       importance, a group and a variable within the group;
       wherein the regularization model is arranged to approximate to the combination of
       Lq and Lp regularizations;
       wherein 1/2 s q < 1 and 1 s p < 2; and
10     wherein p and q are adjustable for evaluating grouping effect and sparsity of the
       dataset.
    2. The computer-implemented method of claim                  1, wherein the regularization model is
       expressed as:
        #    arg min             (i> - 41#)2 + A        m(#) + k    n(#,)
 15
       wherein o < a, b < 1,
       wherein         '='              is a loss function,           '=   is a penalty function, and
       and L are tuning parameters; and
20     wherein a and b are adjustable.
    3. The computer-implemented method of claim                  2, wherein the regression model
       comprises a logistic regression model.
25  4. The computer-implemented method of claim 3, wherein the logistic regression
       model is arranged to apply a path seeking method.
    5. The computer-implemented method of claim 4, wherein the path seeking method
       comprises:
30     initialling a path =0 {$(                  = a};
       computing a vector T(v) using the following:
          ()        [          3(4)]
                  -       h  Z>= n Xi3 + log(o
                                      -             -r-X )
                            h               1 0
                      E   (y      ±,ji1            ,ji)
                        h                  ~+ea          O4,

                                                        17
                   a p)
                                          2  2
                     2o(a+ 1) |#0|+ (1 - _ )
                          2(1 - -y)131
                   V26(b +1)1    1+ (1 - V22
         r1 (v) = __
        identifying coefficients P(v) *o with sign opposite to that of the corresponding Tj(v);
        when set S is empty, selecting a coefficient corresponding to the largest component
  5     of c(v) in absolute value;
        when set S is not empty, selecting a coefficient with corresponding largest Ij(v) I
        within a subset;
        increasing the selected coefficient pj*(v) in the direction of the sign of the
        corresponding T1*(v) to determine a solution for the next point v + Av of the path;
10      and
        performing iterations until all components of c(v) are zero.
    6. The computer-implemented method of claim                1, wherein the dataset comprises
        simulated data and real data.
 15
    7. The computer-implemented method of claim                1, wherein the dataset comprises gene
        data.
    8. The computer-implemented method of claim 7, wherein the gene data is related to
20      cancer disease.
    9. The computer-implemented method of claim 8, wherein the cancer disease is
        prostate cancer, lymphoma cancer, or lung cancer.
25  10. The computer-implemented method of claim 1, further comprising:
             outputting the determined group and variable on a display.
    11. A computer system for analysing data in a dataset, comprising:
        a processor arranged to:
30      process the dataset using a regularization model; and
        apply a regression model to the processed dataset to determine, based on
        importance, a group and a variable within the group;
        wherein the regularization model is arranged to approximate to the combination of
        Lq and Lp regularizations;
35      wherein 1/2 s q < 1 and 1 s p < 2; and
        wherein p and q are adjustable for evaluating grouping effect and sparsity of the
        dataset.
    12. The computer system of claim           11, wherein the regularization model is expressed as:
             -arg min    h(   h
                                   -  432+ A, En(01)     + A2 Z k
                                                                   (d}
40                      L                          1          j-1
        wherein o < a, b < 1,

                                                                 18
                                     (-b
                                  hk                                        k
        wherein                         is a loss function,                '=   is a penalty function, and Al
        and A2 are tuning parameters; and
        wherein a and b are adjustable.
  5
    13. The computer system of claim                12,    wherein the regression model comprises a logistic
        regression model.
    14. The computer system of claim 13, wherein the logistic regression model is arranged
10      to apply a path seeking method.
    15. The computer system of claim 14, wherein the processor is arranged to apply the
        path seeking method by:
        initialling a path " = {$o = a}i;
 15     computing a vector T(v) using the following:
                    OR(O)
                     - '       yi=i        +
                     -   ~                 1   giO  Ij~ij]fr,(3I
         rg (V) I 2a(a+)|3 1,94)       ±(1 -    2 2
                                                  )
                   .913i
                              +1
                           26(1            l
                                         +)|
                  V2b(b +1)|        |+    (1 -  2)2
        identifying coefficients j(v) *o with sign opposite to that of the corresponding Tj(v);
        when set S is empty, selecting a coefficient corresponding to the largest component
        of i(v) in absolute value;
20      when set S is not empty, selecting a coefficient with corresponding largest |cj(v)|
        within a subset;
        increasing the selected coefficient 1j*(v) in the direction of the sign of the
        corresponding cj*(v) to determine a solution for the next point v + Av of the path;
        and
25      performing iterations until all components of c(v) are zero.
    16. The computer system of claim 11, wherein the dataset comprises gene data.
    17. The computer system of claim 16, wherein the gene data is related to cancer disease.
30

                                                   19
    18. The computer system of claim 11, further comprising:
            a display arranged to output the determined group and variable.
    19. A non-transitory computer readable medium for storing computer instructions that,
  5     when executed by a processor, causes the processor to perform a method for
        analysing data in a dataset, comprising:
        processing the dataset using a regularization model; and
        applying a regression model to the processed dataset to determine, based on
        importance, a group and a variable within the group;
10      wherein the regularization model is arranged to approximate to the combination of
        Lq and Lp regularizations;
        wherein 1/2 s q < 1 and 1 s p < 2; and
        wherein p and q are adjustable for evaluating grouping effect and sparsity of the
        dataset.
 15
    20. The non-transitory computer readable medium of claim     19, wherein the
        regularization model is expressed as:
                 m  (m h         4  ) + A,              kr
        wherein o < a, b <  1,
20
            ~
           20      b+)          b}        -bk              k
        wherein =>             is a loss function,    -    -    is a penalty function, and Al
        and A2 are tuning parameters; and
        wherein a and b are adjustable.
25

                                   1/5
               processing the dataset using a regularization
              model that approximates the combination of Lq
                 <U+2721><U+2720><U+271F> <U+271E><U+2701><U+271D><U+2706><U+260E><U+2704><U+2702><U+2701>
                               Lp  <U+2715><U+2720><U+2714><U+2711><U+2713><U+271F><U+2712><U+2711><U+261E><U+271F><U+270F><U+270E><U+270D><U+270C><U+261E> <U+271E><U+2704><U+271D><U+261B><U+260E><U+2701>
               applying a regression model to the processed
               dataset to determine one or more important
               groups and one or more variables within the
                                   group
                                 Figure 1
<removed-date>    <removed-apn>

                                       2/5
    2                        1.5                      2
   1.5                                               1.5
                              1
    1                                                 1
                             0.5
   0.5                                               0.5
    0                         0                       0
     -2        0         2     -2       0        2     -2       0
              (a)                       (b)
                                     Figure                    (c)
          Figure 2A                 Figure  2B              Figure 
    4                         4                       3
    3                         3
                                                      2
    2                         2
                                                      1
    1                         1
    0                         0                       0
     -2        0         2     -2       0        2     -2       0
              (d)                      (e)                     (f)
          Figure 2D                 Figure 2E               Figure
<removed-date>         <removed-apn>

                                       3/5
       2                      2                      2
       1                      1                      1
       0                      0                      0
       -1                     -1                     -1
       -2                     -2                     -2
         -2       0       2     -2      0        2     -2       0
                 (a)                    (b)
                                        Figure                 (c)
              Figure 3A              Figure 3B              Figure 
       2                      2                      2
       1                      1                      1
       0                      0                      0
       -1                     -1                     -1
       -2                     -2                     -2
         -2       0       2     -2      0        2     -2       0
                 (d)                    (e)                    (f)
              Figure 3D              Figure 3E              Figure
<removed-date>      <removed-apn>

                                          4/5
    3                              3                              3
    2                              2                              2
    1                              1                              1
    0                              0                              0
     -5        0           5        -5         0          5        -1
              (a)                             (b)                            (
          Figure 4A                      Figure 4B                      Figu
                    3                               5
                                                    4
                    2
                                                    3
                                                    2
                    1
                                                    1
                    0                               0
                     -5        0          5          -5       0          5
                             (d)                             (e)
                          Figure 4D                       Figure 4E
<removed-date>     <removed-apn>

                                    5/5
                                Processor                 202
                                   Memory                 20
              206                         Communication
                    Input device                          210
                                             module
              208                                         212
                    Output device           Disk drives
                                Figure 5
<removed-date>    <removed-apn>

