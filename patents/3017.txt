   ABSTRACT
   A  genetic   feature   identifying        system     and   a  search  method
   for   identifying     features        of    genetic      information.    The
 5 method    comprises    the       step    of    processing       the  genetic
   information    using    a   combined      global       search   process  and
   local   search   process,      wherein     the   global      search  process
   includes   a  population-based         optimization       process   arranged
   to   determine    a  global        optima     of   a    search    population
10 associated    with  the    genetic      information,       and   wherein the
   local     search    process          includes        a    machine-learning
   optimization     process      arranged       to   further      optimize  the
   search    population      so     as    to    determine       at   least  one
   signature feature associated with the genetic information.

                                  C),
                           0  C)J
     C0)               E,     o       C
                   00
                   U)     C)C
 0                            U
'4-j
         0a)                  aZ
             0     0                  c
               C)l
                     CZ0
                   a)5~

                                                          1
      A GENETIC FEATURE IDENTIFYING SYSTEM AND A                                     SEARCH METHOD
               FOR IDENTIFYING FEATURES OF GENETIC INFORMATION
   TECHNICAL FIELD
 5
              The      present       invention           relates        to   a    genetic       feature
   identifying              system       and      a    search        method       for    identifying
   features              of     genetic           information,              and        particularly,
   although            not    exclusively,            to     a   feature       selection        process
10 combined             with     a     machine-learning                  optimization           process
   using memetic framework.
   BACKGROUND
15            Genetic         information            processing           may      be    useful         for
   identifying               or    predicting              diseases         of     patients         using
   statistical             analysis.         In    general,          the   genetic       information
   may         be      embedded         with        key        features          representing            or
   associating              with      an   existence             or    a   probability           of     the
20 existence of certain diseases or health-related issues.
              Explosive            growth            of        data        urgently           requires
   development             of   new technologies                 and    automation        tools       that
   can       intelligently            help      translating            large      amounts       of   data
25 into useful information and knowledge.                                   Sometimes it is             not
   that all the features in the data are essential. Therefore
   it      is     necessary         to    select          only     a    small      portion       of     the
   relevant            features        from       the      original        large       data    set      for
   further processing or analysis.
30
   SUMMARY OF THE INVENTION

                                                2
           In   accordance         with     a   first    aspect    of     the     present
   invention,         there        is     provided      a    search       method        for
   identifying        features         of   genetic    information,          comprising
   the     step   of   processing          the    genetic    information         using     a
 5 combined      global       search     process    and    local   search       process,
   wherein      the   global       search     process    includes     a    population
   based     optimization         process     arranged     to  determine        a   global
   optima     of   a  search population            associated with         the     genetic
   information, and wherein the                  local search process includes
10 a     machine-learning             optimization        process       arranged         to
   further      optimize       the    search population        so as    to determine
   at least one signature               feature associated with the genetic
   information.
15         In   an   embodiment          of    the   first    aspect,       the     global
   search      process       includes       a   biological      evolution         process
   involving       at   least       one    genetic    operator     applied         to   the
   search population.
20         In   an   embodiment          of    the   first    aspect,       the     global
   search process includes                an   optimization of      a plurality of
   control parameters for a regularization process.
           In     an     embodiment           of    the     first      aspect,          the
25 regularization           process         includes      a   hybrid        L112      +   L2
   regularization         (HLR) process.
           In   an   embodiment          of    the   first    aspect,       the     method
   further      comprises        the    step    of  encoding     and     representing
30 the genetic information in an intron part and an exon part.
           In an embodiment of the first aspect, the intron part
   is    associated       with      penalized      control    parameters          for   the

                                            3
   regularization       process     and     the     exon      part    is     associated
   with     coefficients          used        in         the      machine-learning
   optimization process.
 5       In    an   embodiment      of    the      first      aspect,       the   global
   search     process      includes       a    wrapper          feature       selection
   process    arrange     to   induce      the     search      population        and  to
   perform a heuristic searching process to globally optimize
   the plurality of control parameters for the regularization
10 process.
         In    an    embodiment      of    the      first      aspect,       the   local
   search    process     includes        an   embedded          feature       selection
   process     arranged     to   optimize         the      search     population      by
15 selecting       the   signature        feature          and    to    construct       a
   learning      model     for    the     machine-learning               optimization
   process
         In   an    embodiment     of    the    first       aspect,     the    learning
20 model    is     constructed      based       on     an     efficient        gradient
   regularization process.
         In   an    embodiment     of    the    first       aspect,     the    combined
   global search process and local search process is based on
25 a memetic      framework     arrange      to    facilitate        an    integration
   of  the    determination       of    the    signature         features       and  the
   machine-learning optimization process.
         In   accordance      with    a    second       aspect      of    the   present
30 invention, there is provided a genetic feature identifying
   system    for    identifying      features         of    genetic      information,
   comprising       a  global     search      module         and   a   local      search
   module arranged to process the genetic information using a

                                                4
   global      search         process       and      a     local      search       process
   respectively, wherein the global                     search process includes a
   population-based              optimization             process        arranged            to
   determine        a     global       optima        of     a     search      population
 5 associated      with       the  genetic       information,         and   wherein         the
   local      search          process         includes         a     machine-learning
   optimization         process       arranged         to    further      optimize          the
   search     population          so     as     to     determine        at     least        one
   signature feature associated with the genetic information.
10
         In    an    embodiment         of    the    second      aspect,      the     global
   search     process        includes       a   biological         evolution       process
   involving      at    least     one     genetic       operator       applied      to      the
   search population.
15
         In    an    embodiment         of    the    second      aspect,      the     global
   search    process      module     is    arranged       to   optimize     a   plurality
   of control parameters for a regularization process.
20       In      an     embodiment           of     the       second      aspect,           the
   regularization           process        includes         a    hybrid       L112      +     L2
   regularization         (HLR) process.
         In    an    embodiment        of    the    second      aspect,     the     genetic
25 feature      identifying        system        further       comprises       a    genetic
   information        encoder     arranged        to    encode     and   represent          the
   genetic information in an intron part and an exon part.
         In    an    embodiment         of    the    second      aspect,      the     intron
30 part   is   associated         with    penalized        control      parameters          for
   the regularization process and the exon part                           is associated
   with      coefficients             used         in       the      machine-learning
   optimization process.

                                             5
         In  an   embodiment         of    the   second    aspect,     the   global
   search   module     is    arranged       to   perform    a   wrapper    feature
   selection    process       so   as   to    induce   the   search     population
 5 and  to perform       a heuristic         searching    process     to globally
   optimize    the    plurality          of    control    parameters      for   the
   regularization process.
         In   an  embodiment          of    the   second    aspect,     the   local
10 search module      is    arranged       to perform     an   embedded    feature
   selection process         so as     to optimize      the search population
   by  selecting     the      signature        feature    and   to   construct     a
   learning     model      for      the     machine-learning         optimization
   process
15
         In   an  embodiment          of    the   second    aspect,     the   local
   search   module        is     further       arranged     to   construct      the
   learning      model         based        on    an     efficient        gradient
   regularization process.
20
         In  an embodiment          of the     second aspect,      a combination
   of the global search module and the local search module is
   arrange   to facilitate          an integration       of the determination
   of   the    signature          features       and    the    machine-learning
25 optimization process based on a memetic framework.
   BRIEF DESCRIPTION OF THE DRAWINGS
         Embodiments        of    the    present      invention     will    now  be
30 described,     by     way     of    example,      with    reference     to   the
   accompanying drawings in which:

                                                 6
            Figure    1    a   schematic       diagram      of   a  computing        server
   for     operation      as     a  genetic     feature      identifying       system in
   accordance with one embodiment of the present invention;
 5          Figure    2   is     a schematic       diagram      of an     embodiment       of
   the genetic        feature        identifying       system in       accordance       with
   one embodiment of the present invention; and
            Figure      3     is    an    illustration         of    a     procedure       of
10 crossover operation.
   DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT
            The    inventors          have,     through       their      own    research,
15 trials      and   experiments,          devised      that various         methods      may
   be     used   for    feature        selection,      such     as  filter,      wrapper,
   embedded        and       combined        methods.       For     filter       methods,
   different        feature        selection       measures      may    be    applied      to
   rank      individual         features,       such     as    but    not    limited       to
20 information        theory;        consistency        measures;       dependency        (or
   correlation) measures; distance measures; rough set theory
   and fuzzy set theory.
            The      filter            methods        examine          each        feature
25 independently,            and    ignore     the    individual        performance        of
   the     feature     in     relation      to   the    group,     of which      it    is    a
   part, despite the fact that features in a group may have a
   combined effect in a machine learning task.
30          For wrapper         methods,      machine     learning      algorithms        may
   be     used   to   evaluate         the   performance        of  selected       feature
   subsets, these may include support vector machines                               (SVMs);
   Knearest        neighbor           (KNN);      artificial        neural       networks

                                                      7
    (ANNs);        decision      tree        (DT) ;      Naive      Bayes     (NB) ;      multiple
   linear         regression        for       classification;             extreme         learning
   machines         (ELMs);     and      linear         discriminant        analysis          (LDA).
   The     results       of   the      wrapper          methods       may   be     superior        to
 5 those of the filter methods, but the computational cost of
   the wrapper methods may be higher.
            The    embedded      methods          integrate         feature     selection        and
   learning         procedure         into      a     single       process.      For      example,
10 regularization           methods        may be         an   embedded     technique          which
   performs         both    learning          model        construction        and      automatic
   feature selection simultaneously.
            Preferably, the            applications of regularization method
15 for feature selection may process high dimensional feature
   selection         problems         such        as     gene      expression         microarray
   data,        Lasso      (Li),      smoothly           clipped       absolute         deviation
    (SCAD),         minimax         concave             penalty         (MCP),        and       L1/2
   regularization may also be used.
20
            In    gene   expression           analysis,         if   genes    share       the   same
   biological          pathway,         they       may      be     highly    correlated          and
   grouped.         Therefore,         some     methods         may    be  applied         to   deal
   with     issues      of   high relevance               and grouping        features,         such
25 as group Lasso, Elastic net,                        SCAD-L2,       and hybrid L1/2 + L2
   regularization           (HLR).
            Given     that     each       feature         evaluation       measure         has   its
   own advantages and disadvantages, preferably, the combined
30 method         means      that       the       evaluation           procedure          includes
   different         types      of      feature          selection       measures          such    as
   filter and wrapper.

                                                8
         Without     wishing         to be     bound by        theory,       Evolutionary
   Computations        (EC)      methods       may   be     combined        with     feature
   selection     methods         because       of   their       global       optimization
   capabilities.        Based       on   the    relevant       evaluation        criteria,
 5 the  EC process        of     feature       selection        may    also    be    divided
   into    four    categories,              similar       to     the      categorization
   mentioned above.
         For    example,           these      methods        may       include       genetic
10 algorithm     (GA),      genetic        programming         (GP),     particle        swarm
   optimization          (PSO),        ant       colony       optimization              (ACO),
   differential       evolution          (DE);    evolutionary           strategy        (ES);
   estimated      distribution               algorithm          (EDA)       and      memetic
   algorithm (MA).
15
         In  combined methods,               memetic-based          feature      selection
   methods    may      combine         wrapper      and       filter       methods,        and
   provide   an    opportunity           for    population-based             optimization
   with   local    search.         For    example,       GAs     may    be    applied      for
20 wrapper   feature       selection         and Markov blanket              approach may
   be  used   as   a    local       search     for   filter       feature       selection.
   However,    such two-stage             approaches        may have       the potential
   limitation     that      filter       evaluation        measures        may   eliminate
   potentially          useful          features          regardless           of        their
25 performance      in    the      wrapper      approaches.          In    addition,       the
   wrapper     approaches            may      involve        a     large       number        of
   assessments,       and     each     assessment        may    take     a   considerable
   amount   of   time,       especially         when    the     numbers       of   features
   and   instances       are       large.     The    second       limitation         of    the
30 combined     feature          selection        methods        is     that     they      are
   primarily    concerned           with    the   relatively          small numbers          of
   features and instances.

                                                9
         Feature       interaction           (or    grouping       effect)     presents
   another   difficulty          in    feature      selection.       On   one   hand,   a
   feature, which is weakly relevant to the target, could end
   up  significantly         improving         the    accuracy      of   the   learning
 5 model when used together with some complementary features.
   On  the  other       hand,     an    individually        relevant       feature    can
   become  redundant        when used         together      with     other    features.
   Feature   interaction          occurs      frequently       in   many    areas.    The
   third   limitation           of      some      combined       feature      selection
10 methods   is    that    filter        measures,      which     evaluate     features
   individually,        do not      work well,        and   a subset       of  relevant
   or grouping features requires to be evaluated as a whole.
         With    reference          to    Figure     1,   an    embodiment       of   the
15 present    invention         is     illustrated.          This      embodiment      is
   arranged    to provide          a genetic        feature     identifying       system
   for  identifying        features        of genetic       information,        and   the
   system comprises a global search module and a local search
   module arranged to process the genetic information using a
20 global    search        process          and     a    local       search     process
   respectively, wherein the global search process includes a
   population-based             optimization            process         arranged       to
   determine       a     global        optima       of    a     search      population
   associated with         the genetic           information,        and wherein      the
25 local    search         process           includes        a      machine-learning
   optimization        process        arranged       to   further       optimize      the
   search   population            so     as    to     determine       at    least     one
   signature feature associated with the genetic information.
30       Preferably,        the      genetic       feature      identifying       system
   may be used to solve the abovementioned limitations of the
   combined feature         selection        approaches,        and may be      used to
   perform    a     combined         wrapper-embedded            feature      selection

                                                    10
   approach          (WEFSA).        For     example,              the       system        may      use    a
   memetic       framework          to    combine           genetic            algorithm           (global
   search)        and       embedded        regularization                    approaches            (local
   search),          so      as    to    determine               the         signature           features
 5 associated         with       the   genetic          information,               such       as   DNA   or
   genes      being         analysed.         Therefore,                the       combined          global
   search      process          and    local      search           process          is    based       on   a
   memetic      framework          arrange       to     facilitate              an   integration         of
   the     determination             of    the         signature              features           and    the
10 machine-learning optimization process.
           In   this       embodiment,        the       global          search module             and the
   local search module are implemented by or for operation on
   a    computer        having       an    appropriate                user        interface.            The
15 computer may be implemented by any computing architecture,
   including          stand-alone           PC,          client/server                 architecture,
   "dumb"       terminal/mainframe                   architecture,                   or     any      other
   appropriate             architecture.                   The         computing            device       is
   appropriately programmed to implement the invention.
20
           Referring          to   Figure      1,     there         is     a    shown     a    schematic
   diagram      of    a     computer     or     a    computing              server       100    which    in
   this      embodiment            comprises            a      server            100     arranged        to
   operate,        at     least     in   part        if     not        entirely,          the     genetic
25 feature         identifying            system            in        accordance              with      one
   embodiment          of      the    invention.            The         server        100      comprises
   suitable          components          necessary              to        receive,          store       and
   execute appropriate computer                        instructions.                 The components
   may    include       a    processing        unit       102,        read-only memory                (ROM)
30 104,     random         access      memory           (RAM)        106,        and     input/output
   devices such as disk drives 108,                            input devices 110 such as
   an Ethernet port, a USB port, etc.                                   Display 112            such as a
   liquid     crystal           display,      a    light         emitting           display        or   any

                                                      11
   other      suitable        display        and     communications                 links       114.      The
   server       100     includes        instructions             that        may      be    included       in
   ROM 104,        RAM 106 or disk drives                    108 and may be executed by
   the       processing          unit       102.           There           may       be     provided         a
 5 plurality          of   communication             links       114       which        may     variously
   connect to one or more computing devices such as a server,
   personal           computers,            terminals,              wireless              or      handheld
   computing           devices.             At     least        one        of      a     plurality         of
   communications              link       may      be      connected               to      an     external
10 computing          network       through        a    telephone            line       or    other      type
   of communications               link.
             The server may include storage devices such as a disk
   drive       108     which      may      encompass          solid         state         drives,        hard
15 disk drives,           optical         drives      or magnetic               tape drives.              The
   server       100     may    use     a    single       disk       drive         or    multiple         disk
   drives.         The server 100 may also have a suitable operating
   system       116    which      resides         on    the    disk drive               or   in    the    ROM
   of the server 100.
20
             The    system has          a   database         120      residing            on   a   disk    or
   other      storage       device        which      is    arranged            to     store       at   least
   one     record      122.     The database             120    is      in    communication              with
   the     server      100    with      an interface,              which        is     implemented         by
25 computer            software            residing             on          the          server          100.
   Alternatively, the database 120 may also be implemented as
   a    stand-alone          database          system       in      communication                with     the
   server        100     via    an     external          computing             network,          or    other
   types of communication links.
30
             With      reference          to      Figure         2,        there         is     shown      an
   embodiment          of   the     genetic        feature         identifying              system       200.
   In    this     embodiment,           the     server       100      is     used       as   part      of  an

                                          12
   genetic    feature    identifying       system     200   as   a  server   or  a
   processor arranged to analyze the genetic information such
   as DNA or genes of a biological specimen or sample. In one
   example,    the   system 200 may         select     the  relevant     features
 5 of bio-mark genes, which may be useful for predicting the
   patients' class or particular diseases.
         Preferably,      the    global      search     process     includes    an
   optimization     of   a plurality        of   control     parameters     for  a
10 regularization       process.      Regularization          methods     may   be
   considered as an important            embedded technique          and perform
   both    model    learning       and     automatic       feature      selection
   simultaneously.        Focusing       on    high      dimensional      feature
   selection     problems,     such     as   relevant      gene    selection    in
15 microarray     data.    Example     regularization        methods     includes
   Lasso, SCAD, MCP and Ll/2. Since Lasso is a convex penalty
   function,     the gradient-based         coordinate      descent     algorithm
   is suitable and may be used for the global optimization of
   Lasso.
20
         In   response    to  the problem        of highly       correlated   and
   grouped    features,     for   example,      Elastic     net,   SCAD-L2,    and
   hybrid    Ll/2   +L2   regularization,         a   more    complex    harmonic
   regularization      approach      (CHR)    may    be   used    for   uncertain
25 probabilities distribution of data. Alternatively, a self
   paced    curriculum     learning      (SPLC)     regularization       approach
   may  be    used,   which     significantly         improves     the   learning
   efficiency      when     the     number      of      instance      is   large.
   Regularization approaches are one-stage feature evaluation
30 measures, which are suitable for complex feature selection
   problems     with    high     interaction         and    large     scales    of
   features and instances.

                                          13
         In   regularization          methods,       the    control       parameter
   between     loss    function        and    penalty        function       may   be
   important    for their performance            in   feature     selection.     The
   feasible value of the control parameter is generally tuned
 5 by  the   grid    search      method    with      k-fold     cross     violation
   approach.       Preferably,         some      efficient        regularization
   methods    may    include       non-convex       and    multimodal       penalty
   functions.    These    regularization         methods may       search across
   multiple parameters, which are suitable to be optimized by
10 EC approaches, for example, GA can deal with both unimodal
   and multimodal search space well, and the population-based
   search    can    find      the    global     optima      of    these     control
   parameters efficiently.
15       In    accordance         with    an      embodiment        the     present
   invention,     there    is   provided      a   feature      selection     method
   performed by a global search module 202 and a local search
   module    204     arranged        to    process        the    input      genetic
   information 208 using a global search process and a local
20 search   process     respectively,         using      a   combined      wrapper
   embedded     feature      selection      approach        (WEFSA)      which   may
   improve   learning performance           and accelerate         the search     to
   identify    the    relevant       feature     subsets.       Preferably,      the
   genetic   feature    identifying        system      200   may   fine-tune     the
25 population     of   GA     solutions     by     selecting      the     signature
   feature 210, and constructs the learning model based on an
   efficient gradient regularization process.
         Preferably,     the global       search module         202   is   arranged
30 to perform     a wrapper        feature    selection       process     so  as  to
   induce   the    search     population     and     to   perform    a    heuristic
   searching    process      to   globally      optimize      the   plurality     of
   control parameters for the regularization process.

                                                       14
             The   wrapper        methods            induce          the        population              of     GA
   solutions,         using      heuristic             search        strategies                to   globally
   optimize         the       control          parameters                for          the        non-convex
 5 regularization. Base on a memetic framework, the method in
   accordance         with      the    present           invention             integrates             feature
   selection        and      learning          model        construction                 into      a    single
   process       under       the    global          optimization                of     the       non-convex
   regularization.
10
            Any   efficient          regularization                  approach              can     serve       as
   the       embedded         method          in       WEFSA,          and          preferably,               the
   regularization              process           includes             a        hybrid           L112      +    L2
   regularization              (HLR)        process             arranged             to        optimize          a
15 plurality         of      control          parameters               for       a       regularization
   process.
            As   discussed        above,         regularization                  may       be    considered
   an    important        embedded          feature          selection            approach           in     some
20 example embodiments. Suppose X denotes the nxp data matrix
   whose        rows        are     X=(n     1 Xi-.
                                                 2      ,Xi),1<i<n           ,      Y       denotes           the
   corresponding dependent variable                             (Yl v2    ,-     n.
             For  any     control       parameter             A(A>O),          the     common        form      of
25 regularization is:
                                   L(A,    )=arg mn {R(13) +AP(O)}                   (1)
   where       OER' are       the   estimated              coefficients,                  R(O)   is    a    loss
   function        and      P   represents             the      regularization                   term.        The
   most       commonly        used      regularization                  method            is     the      least
30 absolute shrinkage and selection operator                                         (Lasso, also the
                                                   Ip~
   Li    penalty),         i.e.,     P(3) =4         .    It     is     performing               continuous
   shrinkage and gene selection at the same time.

                                          15
         Some   other     Li-norm     type    regularization                methods     may
   also be used.       For example,       the   smoothly-clipped-absolute
   deviation     (SCAD)    penalty,     which     is     symmetric,            nonconvex,
 5 and   can  produce      spare     solutions         at    the       origin      in    the
   parameter       space.      The    adaptive          Lasso          penalizes         the
   different    coefficients       with the dynamic weights                     in  the Li
   penalty.   The    minimax      concave     penalty         (MCP)       provides       the
   convexity    of the penalized          loss in       sparse         regions      to   the
10 greatest    extent,       given    certain       thresholds             for     feature
   selection      and    unbiasedness.         However,            for       large-scale
   feature   selection problem,          such as genomic                data     analysis,
   the   results     of   the    Li   type    regularization                 may   not    be
   sparse enough for real application.
15
         Genetic information such as a gene microarray or RNA
   seq data sets may have many thousands of genes, and it may
   be   desirable      to     select    fewer      but       informative            genes.
                                                                    P
                                                            P(   =     lj
   Although the LO regularization, where                           3=1     ,   yields the
20 sparsest    solution      theoretically,          it    has      to     solve     an   NP
   hard combinatory       optimization problem.               In     order to obtain
   a   more   concise        solution      and     improve            the      predictive
   accuracy    of    the   machine      learning         model,          the     inventors
   studied the Lp-norm         (0 < p < 1),      especially p              =   1/10, 1/2,
25 2/3, or 9/10.
         Preferably,      a Ll/2     regularization            can be         taken    as  a
   representative       of    the   Lp    (0 <      p    <     1)      penalties,        and
   analyzed       its      analytically          expressive                thresholding
30 representation. Based on this thresholding representation,
   solving   the    L1/2    regularization        may      be    much        easier    than
   solving the LO regularization.              Moreover,           the Ll/2 penalty

                                                            16
   is       unbiasedness                   and        has         oracle         properties.             These
   advantages              make      L1/2       penalty          an    effective           tool     for   high
   dimensional feature selection problems.
 5          However,            like most             regularization                methods,        the   L1/2
   penalty           ignores           the       correlation              between          features,       and
   therefore            cannot         analyze         data        with     dependent            structures.
   If     there        is    a   set       of     features          of    which        the      correlations
   are relatively high,                        the Ll/2 method tends to select only
10 one feature to represent the corresponding group. In order
   to solve the problem of highly relevant                                            features,       Elastic
   net penalty may be applied,                               which is         a linear combination
   of     Li      and       L2     (the        ridge        technique)            penalties.           Such  a
   method          emphasizes               the      grouping           effect,           where      strongly
15 correlated              features           tend      to    enter        or    leave         the   learning
   model together.
            Alternatively,                   Elastic        SCAD       (SCAD-L2),            a combination
   of     SCAD       and     L2     penalties            for      feature        interaction           may  be
20 used.
            The hybrid Ll/2+L2                       regularization                (HLR)       approach may
   fit      the       logistic           regression             models         for       gene     selection,
   where       the        regularization                is    a    linear        combination           of  the
25 Ll/2      and        L2    penalties.              For     any       fixed        control       parameter
    AA 2 (AlA->o),      the     hybrid          Ll/2       +    L2     regularization               (HLR) is
   defined as follows:
                                       L(Al, A2.3) = arg min {R()   + 1|l01/2 + A2 I1|2}   (2)
   where     1011/2 = Y     j3 1/2,1112 = E =, I  I1.
30
            The HLR estimator .5 is the minimizer of Eq. (3):

                                                       17
                                                    =arg min{IL (A. n,    )}(3)
                                   where A =A + A2, anda               .
             A    strictly        convex          penalty           function              may      provide        a
   sufficient          condition           for     the     grouping               effect        of      features
 5 and the L2           penalty       ensures         a   strict            convexity.             Therefore,
   the     L2     penalty       induces        the    grouping              effect         simultaneously
   in     the     HLR    approach.           Experimental                 results           on     artificial
   and      real     gene      expression           data      obtained                in    some        examples
   demonstrated that the HLR method may be used.
10
             However,         some      efficient           regularization                     methods          are
   nonconvex           and      need       to     tune       across              multiple            penalized
   parameters,            which       are        generally               adjusted            by      the      grid
   search method with k-                    fold cross violation approach. It is
15 believed          that      the    population             based            search          in    EC      is   an
   efficient          approach         to      globally          optimize                these       penalized
   parameters.
             Preferably,           the       global        search               process          includes         a
20 biological            evolution            process           involving                 at      least         one
   genetic         operator         applied          to     the          search           population.            In
   Evolutionary             Computations             (EC),       an        initial          population           of
   candidate          solutions          is     randomly          generated                in     the       search
   space         and    iteratively             updated         by         artificial              crossover,
25 mutation            and        selection              operators.                      After            several
   generations,             the     population            can        gradually                develop         high
   quality solutions to the optimization problems.
             In    some      preferably            embodiments,                    local       search          (LS)
30 technologies              may     be       combined           into            the       random           search
   process         of    EC      to    improve          the       optimization                   efficiency.
   These         hybrid        algorithms             may       be          referred            as       memetic

                                            18
   algorithms     (MA) . MAs     for feature         selection,     which combine
   wrapper and filter feature evaluation measures, provide an
   opportunity      for   population-based           optimization      with   local
   search.
 5
         In one example embodiment, the filter feature ranking
   method in    MA to balance        the local         and global    searches   for
   the  purpose      of   improving         the    optimization       quality   and
   efficiency.      Then,     the    Markov        blanket     approach    may   be
10 integrated into MA to simultaneously identify all and part
   of the relevant features.
         Alternatively, in another two-stage feature selection
   process,     a    Relief-F       algorithm         may    be    used   to   rank
15 individual features and then the top-ranked features were
   used   as  input     to   the    memetic       wrapper     feature    selection
   process.
         Heuristic      mixtures      may      be   introduced      which  combine
20 the filter ranking scores to guide the search processes of
   GA and    PSO   for   wrapper      feature       selection.      Moreover,   MAs
   for feature selection have already been used to solve some
   real   application       problems,         such     as,   optimal    controller
   design,     motif-finding          in      DNA,      mircoRNA     and   protein
25 sequences.
         As is shown above, in memetic-based feature selection
   approaches,     the EC      stage    is     included     for wrapper    feature
   selection,     and   the    filter-based         LS   algorithm may help      to
30 reach a local optimal solution. Some "wrapper+filter" two
   stage    memetic      approaches          do    not     guarantee     that   the
   selected    features      in    the    filter      stage    are   also  optimal
   candidates for the EC stage, since the evaluation criteria

                                               19
   of each stage may be totally                  different.    In   some examples,
   the  filter       stage    in    MA may       eliminate    potentially     useful
   features       with    no     regard       to    their   performance      in   the
   wrapper process.
 5
         Some      example     combined        feature    selection    methods   may
   have   limitations        of     inconsistency        in   feature    evaluation
   measures,        feature       interactions          and    large     scales    of
   features      and    instants,       it    may be    more   preferable     that  a
10 combined        wrapper-embedded             feature      selection      approach
   (WEFSA) using memetic framework with GA and hybrid L1/2 +
   L2 regularization          (HLR) is provided.
         Preferably,         the     genetic       feature    identifying     system
15 further      comprises         a    genetic       information      encoder     206
   arranged      to encode       and    represent      the   genetic    information
   in  an     intron     part     and      an   exon   part.   For    example,    the
   genetic      information        may      be   a   chromosome     representation
   including       intron      (the    penalized       control    parameters)     and
20 exon    (the     coefficients         of    the   features    in   the   learning
   model) for memetic optimization procedure.
         In    the     first     step      of   WEFSA,    the  GA   population     is
   randomly       initialized         with      each    chromosome      encoded    by
25 intron and exon parts. Subsequently, as the intron part is
   associated        with    penalized          control     parameters      for   the
   regularization           process,            the     hybrid      L1/2      +    L2
   regularization approach               (local search) is performed on the
   exon parts under         the fixed intron parts,             to reach a local
30 optimal     solution or to improve               the fitness of individuals
   in the search population.

                                                 20
            The    global        search      process          includes        a    biological
   evolution process             involving at least one genetic                      operator
   applied to         the    search      population.          Genetic       operators         such
   as     crossovers       and mutations            are    performed         on   the    intron
 5 parts       of    the    chromosomes,            and      the     selection       operator
   generates the next population. This process repeats itself
   till the stopping conditions are satisfied.
            In    an   example         embodiment         of     the     wrapper-embedded
10 feature       selection        process,        a   representation            for   the      two
   penalized         control        parameters          A a,    and     the     coefficients
   (!302,--3p)    of the candidate feature                  subset can be encoded by
   the      genetic      information          encoder         206     as    a    chromosome:
   intron      + exon=(Aa      3 1,~32r")      .    The   length       of   the chromosome
15 is      denoted      as     p+2,     where       p    is     the     total      number       of
   features.        The length of the chromosome                     is   denoted as p+2,
   where p is the total number of features. The chromosome is
   a     real    value      string       and     its     intron       part      is   globally
   optimized by GA operators.
20
            Although       the     search      space       of     the     intron     part       is
   nonconvex         and     multimodal,           GA     has     the      global      optimal
   ability      because      the dimension of the intron is                        quite low.
   On     the other hand,           the exon part may be                 optimized by the
25 regularization           approach       for       learning        model      construction
   and feature         selection        synchronously.            In    the    exon part,         a
   nonzero value of /3 implies that the corresponding                                  feature
   has been selected.              In   contrast,        the candidate           feature       has
   been      rejected       if     its    corresponding             coefficients          13t   is
30 equal      to    zero.     The     maximum       allowable         number      of   nonzero
   0'i in   the    exon    of    each     chromosome          is   denoted        as  T.      When
   prior      knowledge        about     the     optimal       number      of    features       is

                                                  21
   available,          T  may    be    limited          to    no      more        than       the     pre
   defined value; otherwise T is equal to p.
          The objective function may be defined by:
 5
          Fitness(chromosome) = Accuracy of theclassfication model with(A,a   ,1 ,,,) 2            (4)
   where      nonzero         fl   denotes           the       corresponding                  selected
   features         subset       encoded          in       the       exon          part        of      the
10 chromosome.            The      objective              function             evaluates               the
   significance          of   the   given       feature        subset.           In      this paper,
   the fitness of the objective                      function is             specified as the
   classification            accuracy         of    the       logistic             regularization
   model    with       the chromosome                       2      sop,     using the            hybrid
15 L1/2 + L2 penalties method. Note that when two chromosomes
   are   found       to   have    similar         fitness,          i.e.,        the        difference
   between       their       fitness       is     less       than        a    small          value      of
   e (e = 10-5      in   some examples),              then the           one with a             smaller
   number      of     selected      features          is     given        higher           chances      of
20 surviving to the next generation.
          The       hybrid       Ll/2+L2           penalties              method            with       the
   coordinate         descent      algorithm          may     be     applied            as    memes     or
   local     search       approach.        The      coordinate             descent           algorithm
25 may   be     used      as   it    is    an     efficient            method           for     solving
   regularization           problems,          because         its       computational                time
   increases         linearly       with       the       dimension            of        the     feature
   selection          problems.        Preferably,              WEFSA         is         capable        of
   constructing the learning model and selecting the relevant
30 features.
          The     hybrid      Ll/2+L2        regularization                (HLR)         in   logistic
   model may be formed as:

                                                       22
                                        #=argmin                               AP(3)]
                                                                            (#)+                      (5)
   where     A= A +A2. and      (3)i is            a         loss                function             in      logistic
   regression:
                              P(0) = arg min                      Xi# + log(1 + exp( ex(X13) )    (6)
                                           'I       n-
 5 Here,   (YmY2.)TY         denotes              the decision vector                                  of a binary
   value with 0 or 1 in logistic model.
         P(O3is the HLR penalty function is defined as:
                                                       p                           P
                                        P3(O)=a               I      + (I-a)          |0|2     (7
                                                     j=1                          j=1
                              whe   a=        A,   and 0 < a <1.
10 By  using       the   approach              of        the               original            coordinate-wise
   update:
                                                            Half(wj.Aa)                         (8)
                                                             1+A(1-n)
                            where I     j     p and
                                                                    .,i.i-   ij))               (9)
                                                         i=1
         As   the partial          residual                  for            fitting                        is   defined
15 as:
                                           =     L     tXik                              (10)
                                                kfj
   Additionally,         Half( -         is        the           L1/2             thresholding                operator
   coordinate-wise update form for the HLR approach:
                                                                2
                           Hrlf(w3,A)= {ow(1+cos( I-tx (4T)) ifrwlI>(A)
                                              0                                     otherwise
                                                                                                 (11)
                            where   , (w) = arccos(A (             )i),     7r=3.14.
20 Therefore,        the  Eq.        (5)          can           be           linearized                by     one-term
   Taylor series expansion:
                           /9 arg nin[-     Z(Zj       - XU#)'Wj (Zj - Xito) + AP()] (12)
   where   Zi     is  the  estimated                 response                      and      Wi    is      the weight
   for Zi, which can be defined as follows.

                                                       23
                                            Z           +         -f(X   3)               (13)
                                                           f(X)( - f(Xd3))
                                           iW"= (Xla)( - f d))                            (14)
                           where f(Xp3) is evaluated value under the current parameters:
                                                 f(x;p>) 1+exp(Xip)
                                                               exp(Xf3)                   (15)
         Thus,   Eq. (13)    and             (14)         may be           further             redefined    for
   fitting current las:
                                                Zr'sik
                                                E                                 (16)
                                        wy=         1 x:(Z     - ZU)              (17)
 5                                              i=1
         The procedure of the coordinate descent algorithm for
   the HLR penalized logistic model is described as follows.
                               Algorithm I The coordinate descent algorithm for the HLR
                               penalized logistic model
                                 1: Initialize all f,(m) s-O(j = 1,2, - p),
                                    set mn<- andAct are set by GA:
                                 2: if 3(m) dosenot converge then
                                 3: repeat
                                4:        Calculate Z(m) andW(m) and approximate the loss
                                          function Eq. (12) basedon thecurrent 3(m);
                                 5:       for j = 1 to p do
                                 6:          Compute Z$'(m) + E' rapd(m) and
                                              w(m) <- Z2=" W,(m)g(Z,(m) - 5"m)
                                 7:          Update 3,(77)  eH       "_
                                 8:       end for
                                 9:       m <- m + 1, 4(m + 1) <- (m);
                                10i until there areno mom features to beremoved:
                                tt: end if
                               12:return theoptimal featuresubset;
10
         In    the    evolution                process               of        WEFSA,            standard    GA
   operators     such    as    fitness                   proportionate                      selection,      one
   point    crossover     and         uniform                mutation                  operators        can  be
   applied.     Moreover,        if           prior            knowledge                 on       the  optimal
15 number of features        is        available,                   the number of nonzero of
   A  in  each exon part           of the chromosome                               may be          constrained
   to a maximum of T in the evolution process.
         In   an   example      evolution                    process              such         as   crossover,
20 two parents      (pa,  ma)        may be               randomly               select          from current
   population      for   later           breeding.                  Then,              the      operation    of

                                                     24
   crossover        may   be    used      to     produce          offsprings          that     inherit
   characteristics             from      both         parents.            A    single        crossover
   point       on   the    intron        of      both       pa      and     ma    chromosomes           is
   generated between the penalized control parameters A and a
 5 then      these   two penalized              control         parameters          on     both    sides
   of    that     point    are     swapped          in     the      intron      of   the      parent's
   chromosomes         to   create        the      intron         part      of   the    offsprings'
   chromosomes         cl   and    c2.     The      exon       P   of these        two offsprings
   chromosomes           are     evaluated              by       the       local       optimization
10 strategies.
             For   mutation       process,            the       mutation        operator         allows
   diversity        of populations              and     larger         exploration          of   search
   space.       During     this       stage,         one       of     the    penalized         control
15 parameters         X,     a    is     randomly              chosen,         with      a    mutation
   probability          pm     (e.g.       pm      =     0.1)        to     mutate       a    selected
   chromosome.          The     fitness           and       P     of      the     new      chromosome
   generated by          the mutation             operation            are   also     evaluated by
   the local optimization strategies.
20
             The   roulette-wheel             selection           may be       used     to    generate
   the       next     generation            from         the        parent         and       offspring
   populations.           The      selection              probability             probc        of      the
   chromosome         c   is     directly           proportional               to    its      fitness,
25 i.e.,
                                      prob, -     ~aeu    ()(8
                                                    E Am)+Y fCoffsprI-)
   At    the genetic        selection           stage,         the      candidate       chromosomes
   with higher accuracy will be less                               likely to be eliminated
   and still have the chance to be possible.
30
             The   inventors        also       evaluated            the     performance         of     the
   WEFSA       approach      in     a    simulation              study       of    the      system      in

                                                                         25
   accordance               with the embodiments                                   of the present                      invention.
   In     the experiments,                            six approaches                       were        compared:              GA,      GP,
   MA,         Elastic                  net,             SCAD-L2,                and            the        hybrid             L1/2+L2
   regularization (HLR) respectively. Data from a true model:
                                                           log(    ')=X'O+0E,E~N(0,1)
 5                                                              1~Y
   where X-N(0,                     1),        E is         the         independent                 random noise                 and u
   is the control parameter for noise was simulated.
              Three scenarios are presented here. In every example,
10 the       dimension                     of       features                is        6000.          The          notation              '/'
   represents               the number of observations                                            in      the training and
   test sets respectively, e.g. 100/100. Here are the details
   of the three scenarios.
15            In        Scenario                   1,       the          dataset               consists              of       200/200
   observations,                        the         noise             control              parameter               a=       0.2        and
                    100                  1900                100               1900          100       1900
              a grouped feature situation was simulated
                j = p x   x1 +    (1 -p)    x -,.j  = 2,3,-.. ,100;
                   = P XX 2 0 0 1 + (1 -   p) x .Ej,j = 2002,2003,..- 2100;
               X1 = p X X 400 1 + (1 - p) x xj,j = 4002,4003,..- .4100.
20 where          p     is        the          correlation                   coefficient                    of     the        grouped
   variables.                 In         this          example,               there            are        three        groups             of
   correlated                 features.                   An        ideal           sparse            regression               method
   would          select               only           the        300         true          features               and       set        the
   coefficients of the 5700 irrelevant features to zero.
25
              Scenario 2 is defined similarly to Scenario 1, except
   the      case when                   there           are      other          independent                   factors,            which
   also contributes to the decision variable y
    3 (1, -1,1,-1...    1-1,1.5-2,1.7.3,-1,0....         0,2,-2.2,-2,---.2-2,1.5-2,1.7.3.-1,0,...        02,2,2..  ,1-5-2.1.7.3,-10..--     0).
                 100                  Sxa0          1800             100               5x~20        1800      100        5x20         1800
30 In      this         example,                   there          are        three             groups          of      correlated
   features                 (similar                    to        Scenario                    1)       and         300          single

                                                                         26
   independent                     features.                An example                sparse             regression method
   would             select                 the            600         true           features                  and           set           the
   coefficients of the 5400 irrelevant features to zero.
 5           In         Scenario               3,         the        true      features                 were         added            up       to
   1000         of         the         total             features,               a=         0.1,           and       the          dataset
   consists of 500/100 observations, and
   3= (,1-1.1. -i.... .1.- 1, 1.5,-21.7,3,-1,. ...     0,2-2.2,-2 ..  .2-21.5-2, 1,7, 3, -1,.0   .   2.2,-   .2,1.5-2, 1.7, 3-1,,1..      1.0   .0
                10                   5X20         1800           100             5x20          1800      100         5x20            400     1400
                                                        p x x1 + (1- p) x x j = 2,3,-- ,100:
                                               x 1 = p x x2001 + (1 - p) xxy,j = 2002.2003,- --,2100;
                                               Xy = p x x4001 + (1 - p) x x,j = 4002,4003... 4100:
                                               Xy = 0.1 X X 420 1 + 0.9 x x,.j = 4202,4203.- - - 4600.
10 In     this            example,                 there           are       three             groups            of        correlated
   features                (similar to Scenario                               1),        400 correlated                         features
   (the         corrected                   parameter                  is     0.1)              and          300        independent
   features. An example sparse regression method would select
   only the                1000           true         features             and       set          the       coefficients                      of
15 the 5000 irrelevant features to zero.
             In         one          example,                 the       correlation                     coefficient                      p     of
   features               may         be      set          to      0.1,       0.4,            0.7       respectively.                       The
   learning model                          in      GA,        MA,       Elastic              net,          SCAD-L2,              HLR        and
20 WEFSA is               the logistic classification                                           approach.                 In      GP,       the
   multitree classifier is used. For each iteration of GA and
   MA, the number of selected features based on the filter of
   information                      gain            is        set         to       2000.              The          configuration
   parameters used by EC algorithms in these seven approaches
25 are listed                   in     Table            I,    which shows the parameters                                          set for
   the EC process.
                                                                      TABLE I
                               PARAMETERS SET FOR THE EC ALGORITHMS IN THE SEVEN APPROACHES
                                                               Parameter               Value
                                                           Population size (P)           200
                                                       Crossover probability (pc)       0.85
                                                       Mutation probability (pm)          0.1
                                                          Stopping criterion (G)       2000

                                                       27
             In      the       regularization                  process          of         these       seven
   approaches,            the    control       parameters                of  Elastic         net,      SCAD
   L2,      and     HLR      approaches        are       tuned          by   the      10-fold        cross
 5 validation            (CV)    approach         in     the      training          set.     Note      that,
   the      Elastic        net    and    HLR      methods            are    tuned         by  the      10-CV
   approach          on     the    two-dimensional                  parameter            surfaces.         The
   SCAD-L2          is    tuned      by    the        10-CV         approach          on     the     three
   dimensional                parameter               surfaces.                Then,           different
10 classifiers             are    built     by       these          seven       feature         selection
   approaches.            Finally, the           obtained            classifiers are               applied
   to the test set              for classification and prediction.
             The     simulations         may       be     repeated          100      times       for     each
15 method and compute the mean classification accuracy on the
   test        sets.        To    evaluate           the        quality         of       the     selected
   features            for      these      approaches,                   the       sensitivity             and
   specificity              of    the     feature             selection            performance             are
   defined as           follows:
                                                 TruePositive(TP):= 3..
                                                TrueNegative(TN) :=
                                                FalsePositive(FP) :
                                                FalseNegative(FN) :   B   3
                                                        TP                  TN
                                        Senstity     TP+FN ,Specificity:= TN + FP'
20
   where         the      .*     is   the       element-wise                 product,           and       I -o
   calculates           the     number     of      non-zero             elements          in   a   vector,
   (3and 0        are      the     logical           "not"          operators            on    the       true
   coefficients vector                P  and the simulated                    d.
25
             Table          II      shows           the           feature            selection             and
   classification              performances              of      different           methods        in     the
   different          parameter       settings with Scenarios                            1-3,    in    which

                                                                 28
   the   results        with          best          performances                        are      denoted          in  bold
   texts.
                                                                   TABLE II
                                                         RESULTS OF THE SIMULATION
                                                                              Scenario
                    p     Methods        1       2          3        1        2         3                        3
                                             Sensitivity                 Specificity                Accuracy
                            GA        0.902    0.584      0.527/   0.991   0.956     0.862  94.32%   80.48%   77.54%
                            GP        0.915    0.748      0.726    0.997   0.987     0.9(7  95.50%   88.47%   79.66%
                            MA        0.908    0.679      0.652    0,993   0.971     0.893  94.73%   82.91%   80.65%
                   0.1   Elastic net  0.910    0.726      0.724    0.994   0.975     0.904  94.53%   83.03%   79.78%
                         SCAD-L 2     0.916    0.795      0.758    0.997   0.982     0.912  94.49%   82.12%   80.41%
                            HLR       0.919    0.863      0.791    0.998   0.987 0.918      95.81%   90.15%   85.76%
                          WEFSA       0.935    0.906      0.823    0.998   0.989     0.926  97.08%   91.23%   87.81%
                            GA        0.724    0.531      0.457    0.985   0.923 0.813      89.71%   76.49%   70.37%
                            GP        0.798    0.712      0.674    0.992   0.957 0.866      93.64%   82.87%   77.82%
                            MA        0.741    0.635      0.572    0.987   0.929     0.848  89.84%   80.04%   75.63%
                   0.4   Elastic net  0.805    0.712      0.623    0.991   0.940 0.863      92.06%   82.19%   75.19%
                         SCAD-L 2     0.837    0.741      0.698    0.992   0.949     0.894  92.44%   82.84%   76.51%
                            HLR       0.862    0.820      0.725    0.994   0.960 0.903      93.89%   83.45%   79.22%
                          WEFSA       0.904    0.852      0.782    0.995   0.972     0.912  95.31%   85.79%   80.06%
                            GA        0.563    0.467      0.417    0.961   0.891     0.775  75.08%   69.04%   62.65%
                            GP        0.620    0.665      0.633    0.984   0.928 0.832      90.15%   73.94%   70.24%
                            MA        0.596    0.579      0.536    0.971   0.897     0.794  89.66%   70.96%   66.30%
                   0.7   Elastic net  0.675    0.637      0.561    0.977   0.905 0.816      88.17%   71.85%   65.26%
                         SCAD-L 2     0.691    0.694      0.583    0.986   0.929 0.822      89.79%   74.27%   68.83%
                            HLR       0.763    0.729      0.671    0.988   0.937 0.837      90.04%   77.18%   73.75%
                          WEFSA       0.820    0.754      0.724    0.991   0.943     0.851  92.34%   80.65%   76.94%
 5        It is found that with the decrease of the correlation
   coefficient         p,      the models'                    performances                   can be better.             In
   Table      II,     the        WEFSA           approach                  always            selects          the     most
   correct      relevant             features                  in      different                data       environment
   with      Scenarios               1-3.             The            highest               sensitivities               and
10 specificities of feature selection obtained by WEFSA means
   that WEFSA selects most relevant features and deletes most
   irrelevant features respectively. Thus, the classification
   accuracy       obtained by                 the WEFSA                   approach              also       outperforms
   other EC and regularization methods.
15
          In    another          experiment                   performed                 by the inventors,               to
   further      evaluate             the       effectiveness                          of    the      WEFSA         method,
   five     example          gene           expression                    microarray                datasets          were
   used,     including             AML,        DLBCL,               Prostate,               Lymphoma           and    Lung
20 cancer.      The     AML        dataset               has         116       patients,             which         contain
   6283     genes.          The          DLBCL                contains                  about         240       samples'
   information,          each         sample              includes                the      expression             data  of
   8810    genes.      The         Prostate                dataset              contains           the       expression

                                                        29
   profiles       of   12,600           genes        for        50      normal             tissues     and  52
   prostate      tumour tissues.                 The Lymphoma                     dataset        contains   77
   microarray         gene        expression                  profiles                  of     the   2    most
   prevalent        adult        lymphoid              malignancies:                       58    samples    of
 5 diffuse large B-cell lymphomas and 19 follicular lymphomas
   (FL).      The   original            data       contains               7,129           gene    expression
   values.      The Lung cancer dataset                           contains               164    samples with
   87    lung    adenocarcinomas                  and        77     adjacent                normal    tissues
   with     22401     microarray            gene         expression                   profiles.      A   brief
10 summary of these datasets is provided in Table III below.
                                                    TABLE III
                          THE DETAILED INFORMATION OF IVE REAL GENEEXPRESSION     DATASETS
                                             USEDIN THE EXPERIMENTS
                                | aaset     o. samples|   . genes          lasses
                                 AML          6283         116     High risk / Low risk
                                DLBCL         7399        240      High risk / Low risk
                               Lymphoma       7129         77         DLBCL I FL
                                Proslate     12600         102       Normal / Tumor
                              Lung cancer    22401         164       Normal / Tumor
           In   order    to accurately                   assess the performance                         of the
15 seven      different        feature             selection                approaches,            the    real
   datasets are randomly divided into two sets:                                               two thirds of
   the samples are put in the training set used for the model
   estimation,        and the           remaining              one third of                  data   are used
   to    test     the    estimation               performance.                     For       regularization
20 approaches,        the penalized                parameters                  are tuned by the 10
   fold      cross      validation.                 For         each             real         dataset,     the
   procedures        using      different             methods              are         repeated     over   100
   times respectively.

                                                          30
                                                           TABLE IV
                                               RESULTS OF EMPIRICAL DATASETS
                                    Mlettleds     lrItang Accuracy   'RAt accuracy  No. lecte  Renem
                                      UA               9193%            793W              5
                                       GP              9732%            92,E2%            21
                                      MA                9635%           91.13%            25
                         AML       Elastic nt           9667%           9204%             28
                                   SCAD-t 2            96,62%           9294%             23
                                        L              97.46%           93.78%            22
                                    WEFSA              97.84%           94.32%             19
                                      GA               9L97%            88.58%            24
                                       GP              95.34%           9122%              14
                                      MA               93.40%           9%34%              [8
                       DLBCL       Elastic nrf         94.62%           92.54%            21
                                   SCAD-L2             95.39%           9203%              16
                                      HLR              97.21%           9315%              17
                                    WESA               97.28%           9173%              13
                                      UA               943%         -r6                   63
                                       GP              96.14%           93.37%            38
                                      MA               96.08%           92.64%            54
                      Lymphona     Elastic net          9593%           9217%             41
                                   SCAD-L 2            96.42%           91L65%            28
                                      HLR              9K18%            93,26%            29
                                    WEFSA              9.51%            94.03%            27
                                      GA               95.79%           90.34%            42
                                       GP               97.26%          93.81%            27
                                      MA               95317%           90.83%            34
                       Prostate    Elastic net         9652%            92.51%            31
                                   SCAD-L2             95.82%           92.89%            26
                                      HLR              97T15%           92.63%            23
                                    WEFSA              9132%            9417%             22
                                         A              9L-14%          90.3
                                       GP              9&2%             9211%             45
                                      MA               97.42%           91.59%            49
                      Lung cancr   Elastic net          96.94%          90.5%             34
                                   SCAD-L 2            97.63%           92.27%            36
                                      HLR              9&16%            92.48%            34
                                    WEFSA              9&93%            9141%             33
        Table     IV   describes                 the         averaged            training            accuracies
   (10- CV) and test accuracies obtained by different feature
 5 selection    approaches                regularization                       models           in    the   five
   datasets,   in    which the results with best performances                                                are
   denoted in bold texts.
         It   is     obvious             that            the       performance                of     the   WEFSA
10 approach   is     better           than            the       other          six      approaches.          The
   relevant     gene            selection                    performances                    of       different
   approaches     in    the       five           real          datasets            are       also      shown  in
   Table IV. The number of genes selected by the WEFSA model
   is  the   smallest             compared                   to       the         other         six      feature
15 selection    approaches.                  In       regularization                    approaches          with
   grouping effect,            such as Elastic                         net,        SCAD-        L2,    and HLR,
   the performance of HLR is better than that of Elastic net
   and SCAD-L2 in gene selection.

                                                     31
         On the contrary, in EC approaches, such as GA, GP and
   MA,  the performance of GP is better than that of GA and MA
   in   gene      selection           and         classification.                  Comparing        the
   performances        of    the      seven          feature           selection         algorithms,
 5 Table     IV      proves          that           WEFSA            approach          has      better
   performances          in     both           gene          selection            and      predictive
   classification.
         For biological analysis of the results, 10 top-ranked
10 selected    genes        obtained           by     the       different          methods      in  the
   AML dataset are shown in Table V below.
                                                  TABLE V
                                    THE 10 TOP GENES IN THE AML DATASET.
                Rank    GA       GP          MA      Elastic net SCAD-L2     HLR    WEFSA
                  S    VEG3F  SNRPN        MEISI       GSTMI      DNMTl     MLR       FLT3
                 2    PRDX2     FHIT     ALOX12        SFRP2      CDH13    CCDC69   CDKN2B
                 3     TP73    PNLIP     CDKN2A        GRAF        SFRP5    JUNB     INK4B
                 4    SFRP5    INK4B       CDH[3       GSTMI      ABCA8    GLIPR I   GSTM I
                 5     FHT    A4GALT       SNRPN       PTPN6       RARA     VTPN6    SFRPI
                 6    GLIPRI   SFRPI      GSTMI         FHIT      GSTMI     RUNX3    NPM1
                 7     GRK5   PRDX2         GRAF       JUNB       WNT5A    GSTMI     SFRP2
                 8    TNXB      SLN        SFRP5       SFRP5       PNLIP    MEISI    SFRP5
                 9     GRAF    PTPN6      PDLIM2      ALOX12      DAPKI    ALOX12    CEBPA
                 10    PTRF   DAPK          PfRF      CDKN2A ,1ROXA9        SFRP5   GLIPRI
15       Compared        with      the        other         feature        selection         methods,
   the   WEFSA      approach         selects            some        unique        genes,      such   as
   SFRP1 and SFRP2,           which         are members               of   the Sfrp         family,   a
   kind   of    signal       transduction                   proteins.          The      Sfrp     family
   proteins     play       a  key        role        in       transmitting            the     TGF-beta
20 signals    from      the    cell-surface                   receptor        to    cell      nucleus,
   mutation or deletion of AML disease, which has been proved
   to lead to pancreatic cancer. It is believed that the Sfrp
   family may be strongly associated with AML diseases.
25       In   the     other      genes          selected           by    the      WEFSA     approach,
   the gene FLT3 can            stimulate the motility of AML diseases.
   The expression          of FLT3 has              been found to be                   up   regulated

                                           32
   in  some    different      kinds      of    AML    diseases.      The    protein
   encoded by the gene NPMl is said to be very similar to the
   tumour     suppressor      of    drosophila,          which     is    a   highly
   relevant gene to AML diseases.
 5
         Moreover,      some     relevant        genes     selected      by    other
   regularization      models using Elastic net,               SCAD-L2,     and HLR
   approaches are also found by the WEFSA, for example, SFRP5
   and   GSTM1.     They   are      significantly          associated       to   AML
10 diseases, which has been discussed in.
         Advantageously,       the WEFSA approach            not only can       find
   the  relevant      genes    that     are     selected     by   other     feature
   selection     methods,    but    also     can   find    some    unique    genes,
15 which     are    not    selected         by     other     models       but    are
   significantly      associated        to    diseases.      Hence,     the    WEFSA
   approach    may   identify      the    relevant      genes    accurately      and
   efficiently.
20       It   will   also   be    appreciated       that    where     the   methods
   and  systems     of  the    present       invention      are   either     wholly
   implemented     by computing        system or       partly    implemented      by
   computing     systems   then      any    appropriate       computing      system
   architecture       may    be     utilised.             This    will      include
25 standalone      computers,       network       computers       and    dedicated
   hardware devices.        Where the terms           "computing system" and
   "computing     device"   are used,         these    terms    are   intended    to
   cover    any   appropriate       arrangement         of   computer      hardware
   capable of implementing the function described.
30
         It will be appreciated by persons skilled in the art
   that numerous variations           and/or modifications            may be made
   to  the    invention    as     shown     in    the    specific      embodiments

                                  33
  without   departing    from   the   spirit    or   scope    of  the
  invention   as  broadly  described.    The present     embodiments
  are,   therefore,   to  be   considered    in   all   respects   as
  illustrative and not restrictive.
5
        Any reference to prior art contained herein is not to
  be  taken  as  an  admission  that  the  information     is  common
  general knowledge, unless otherwise indicated.

                                         34
   CLAIMS:
   1.   A  search   method    for    identifying          features       of    genetic
        information,     comprising         the     step     of    processing        the
 5      genetic   information       using        a   combined       global      search
        process   and   local   search process,             wherein      the    global
        search      process         includes            a       population-based
        optimization     process      arranged        to    determine       a   global
        optima   of   a   search     population          associated        with      the
10      genetic    information,         and    wherein        the     local     search
        process     includes      a     machine-learning               optimization
        process    arranged      to      further       optimize         the     search
        population    so  as   to determine           at least       one   signature
        feature associated with the genetic information.
15
   2.   The search method in accordance with Claim 1, wherein
        the   global     search     process          includes        a   biological
        evolution    process      involving          at     least      one     genetic
        operator applied to the search population.
20
   3.   The search method in accordance with Claim 1, wherein
        the global search process includes an optimization of
        a    plurality       of        control          parameters            for       a
        regularization process.
25
   4.   The search method in accordance with Claim 3, wherein
        the regularization process includes                    a hybrid      L112  +   L2
        regularization     (HLR) process.
30 5.   The search method in accordance with Claim 4,                          further
        comprising    the  step     of encoding          and    representing         the
        genetic   information        in    an     intron      part     and    an  exon
        part.

                                      35
   6.  The search method in accordance with Claim 5, wherein
       the intron part      is   associated with penalized          control
       parameters     for  the     regularization       process    and  the
 5     exon part is associated with coefficients used in the
       machine-learning optimization process.
   7.  The search method in accordance with Claim 3, wherein
       the global    search process        includes    a wrapper    feature
10     selection     process      arrange      to    induce   the    search
       population     and    to    perform      a   heuristic    searching
       process to globally optimize the plurality of control
       parameters for the regularization process.
15 8.  The search method in accordance with Claim 1, wherein
       the local search process includes an embedded feature
       selection   process      arranged      to   optimize    the   search
       population    by selecting       the   signature    feature   and to
       construct   a   learning     model    for   the   machine-learning
20     optimization process
   9.  The search method in accordance with Claim 8, wherein
       the   learning     model      is    constructed      based    on  an
       efficient gradient regularization process.
25
   10. The search method in accordance with Claim 1, wherein
       the  combined    global     search    process    and  local   search
       process  is    based    on   a memetic      framework   arrange   to
       facilitate an integration of the determination of the
30     signature       features        and       the     machine-learning
       optimization process.

                                        36
   11. A genetic    feature      identifying       system for identifying
       features   of genetic        information,        comprising    a global
       search module       and   a   local    search     module   arranged    to
       process the genetic information using a global search
 5     process   and      a   local      search     process    respectively,
       wherein     the      global       search      process      includes      a
       population-based         optimization         process     arranged     to
       determine     a    global      optima    of    a   search    population
       associated with the genetic              information,      and wherein
10     the local    search process         includes      a machine-learning
       optimization process arranged to further optimize the
       search   population        so   as   to   determine     at    least   one
       signature       feature        associated        with    the    genetic
       information.
15
   12. The genetic      feature     identifying       system in     accordance
       with   Claim     11,    wherein      the    global    search    process
       includes a biological evolution process                   involving at
       least   one    genetic       operator     applied     to    the   search
20     population.
   13. The genetic      feature     identifying       system in     accordance
       with   Claim     11,    wherein      the    global    search    process
       module is arranged to optimize a plurality of control
25     parameters for a regularization process.
   14. The genetic      feature     identifying       system in     accordance
       with   Claim     13,    wherein     the    regularization       process
       includes    a     hybrid      L112  +   L2    regularization        (HLR)
30     process.
   15. The genetic      feature     identifying       system in     accordance
       with    Claim       14,     further       comprising        a   genetic

                                               37
       information          encoder      arranged        to  encode     and     represent
       the genetic information in an intron part and an exon
       part.
 5 16. The     genetic      feature       identifying        system     in    accordance
       with      Claim    15,     wherein       the    intron   part     is   associated
       with        penalized            control           parameters           for       the
       regularization               process          and    the      exon       part      is
       associated          with      coefficients          used    in     the    machine
10     learning optimization process.
   17. The     genetic      feature       identifying        system     in    accordance
       with      Claim     13,     wherein        the    global   search       module     is
       arranged         to     perform        a     wrapper      feature        selection
15     process       so as      to   induce       the   search    population         and to
       perform        a   heuristic          searching       process        to    globally
       optimize        the plurality           of     control   parameters         for   the
       regularization process.
20 18. The     genetic      feature       identifying        system     in    accordance
       with      Claim      11,    wherein         the   local    search       module     is
       arranged         to    perform        an     embedded     feature        selection
       process       so   as     to    optimize         the  search     population        by
       selecting         the     signature         feature    and     to    construct        a
25     learning        model      for   the     machine-learning            optimization
       process
   19. The     genetic      feature       identifying        system     in    accordance
       with      Claim      18,    wherein         the   local    search       module     is
30     further        arranged         to     construct       the     learning         model
       based on an efficient gradient regularization process.

                                  38
  20. The genetic    feature  identifying    system in   accordance
      with   Claim  11,   wherein   a combination   of  the  global
      search module and the local       search module    is arrange
      to facilitate     an integration    of the determination    of
5     the    signature    features    and   the   machine-learning
      optimization process based on a memetic framework.

        <U+2736> <U+2733><U+25CF><U+274B>
                            <U+2735><U+2735>
              <U+2735><U+2737>
         <U+273B>         <U+2735>
         <U+273D><U+2735>            <U+273B><U+2735>        <U+2737>
         <U+2739><U+2735>            <U+2737><U+2735>
<removed-date>   <removed-apn>

 <U+2737> <U+2733><U+25CF><U+274B>
                      <U+272E><U+2735> <U+2737><U+272D> r<U+2709>t<U+275B> <U+2762> r<U+2709>t<U+275B><U+2665><U+2763><U+2710>s <U+275E> t<U+275D> <U+2767> <U+2759>
                  <U+272E><U+2735><U+2735><U+2737><U+272D> <U+2660> ts<U+2461>s <U+2763><U+2665><U+2710><U+2461><U+2762><U+2710>t<U+2665> <U+275E><U+2710> r<U+2709>t<U+275B> <U+2762> <U+275D><U+2710>t <U+2665> <U+25CF>
                     <U+272E><U+2739><U+2735><U+2737><U+272D>                        <U+272E><U+2737><U+2735><U+2737><U+272D>
                 <U+2767><U+2709><U+275E><U+2666><U+2660> ss <U+275D><U+2666>r<U+2663>                <U+2767><U+2709><U+275E><U+2666><U+2660> ss <U+275D><U+2666>r<U+2663>
                  <U+2701><U+275D>r<U+275B> s <U+2767><U+275B><U+275D><U+2666><U+25B2>                <U+2701><U+275D>r<U+275B> s <U+2767><U+275B><U+275C><U+2666><U+2767><U+25CF>
                              <U+272E><U+273B><U+2735><U+2737><U+272D> r <U+275E><U+2666><U+275D><U+2665><U+274A>
                                <U+2665><U+2666><U+2710>t<U+275B><U+2660>r<U+2666><U+2762><U+2665><U+2710>
                                 <U+275D><U+2710>t <U+2665> <U+25CF>
                      <U+272E><U+273D><U+2735><U+2737><U+272D> <U+2665><U+2666><U+2710>t<U+275B><U+2660>r<U+2666><U+2762><U+2665><U+2710> <U+275D><U+2710>t <U+2665> <U+2763> t<U+2709><U+2663><U+2665>
<removed-date>   <removed-apn>

<removed-apn>   <removed-date>
         <U+2738> <U+2733><U+25CF><U+274B>

