Abstract
Systems and methods that utilize optical sensors and non-optical sensors to determine the
position and/or orientation of objects. A navigation system (20) includes an optical sensor
(40) for receiving optical signals from markers (50) and a non-optical sensor (60), such as a
gyroscope, for generating non-optical data. A navigation computer (26) determines positions
and/or orientations of objects based on optical and non-optical data.

WO 2014/052428                          PCT/US2013/061642
                             /9
                     k'0
                                  I     I
                                  IAI
                                I
               400
                         .40
                   .0               :iI

WO 2014/052428                                                                PCT/US2013/061642
        NAVIGATION SYSTEM INCLUDING OPTICAL AND NON-OPTICAL
                                             SENSORS
                                  RELATED APPLICATIONS
   [0001]          This application claims priority to and the benefit of U.S. Provisional
   Patent Application No. 61/705,804, filed on September 26, 2012 and U.S. Non
   Provisional Application No. 14/035207, filed on September 24, 2013 the entire contents
   of which are hereby incorporated by reference.
                                  FIELD OF THE INVENTION
   [0002]          The present invention relates generally to a navigation system that tracks
   objects in space by determining changes in the position and/or orientation of such objects
   over time. More specifically, the present invention relates to navigation systems that
  utilize optical sensors and non-optical sensors to determine the position and/or
   orientation of objects.
                             BACKGROUND OF THE INVENTION
   [0003]          Navigation systems assist users in precisely locating objects.           For
   instance, navigation systems are used in industrial, aerospace, defense, and medical
   applications.   In the medical field, navigation systems assist surgeons in precisely
  placing surgical instruments relative to a patient's anatomy.
   [0004]          Surgeries in which navigation systems are used include neurosurgery and
   orthopedic surgery. Often the instrument and the anatomy are tracked together with their
  relative movement shown on a display.             The navigation system may display the
   instrument moving in conjunction with a preoperative image or an intraoperative image
   of the anatomy. Preoperative images are typically prepared by MRI or CT scans, while
   intraoperative images may be prepared using a fluoroscope, low level x-ray or any
   similar device.    Alternatively, some systems are image-less in which the patient's
   anatomy is "painted" by a navigation probe and mathematically fitted to an anatomical
   model for display.
   [0005]             Navigation systems may employ light signals, sound waves, magnetic
   fields, RF signals, etc. in order to track the position and/or orientation of the instrument
   and anatomy. Optical navigation systems are widely used due to the accuracy of such
                                                  1

WO 2014/052428                                                               PCT/US2013/061642
   systems.
   [0006]             Prior art optical navigation systems typically include one or more
   camera units that house one or more optical sensors (such as charge coupled devices or
   CCDs). The optical sensors detect light emitted from trackers attached to the instrument
   and the anatomy. Each tracker has a plurality of optical emitters such as light emitting
   diodes (LEDs) that periodically transmit light to the sensors to determine the position of
   the LEDs.
   [0007]             The positions of the LEDs on the instrument tracker correlate to the
   coordinates of a working end of the instrument relative to a camera coordinate system.
   The positions of the LEDs on the anatomy tracker(s) correlate to the coordinates of a
   target area of the anatomy in three-dimensional space relative to the camera coordinate
   system.    Thus, the position and/or orientation of the working end of the instrument
  relative to the target area of the anatomy can be tracked and displayed.
   [0008]             Navigation systems can be used in a closed loop manner to control
   movement of surgical instruments. In these navigation systems both the instrument and
   the anatomy being treated are outfitted with trackers such that the navigation system can
   track their position and orientation. Information from the navigation system is then fed
   to a control system to control or guide movement of the instrument. In some cases, the
   instrument is held by a robot and the information is sent from the navigation system to a
   control system of the robot.
   [0009]             In order for the control system to quickly account for relative motion
  between the instrument and the anatomy being treated, the accuracy and speed of the
   navigation system must meet the desired tolerances of the procedure.          For instance,
   tolerances associated with cementless knee implants may be very small to ensure
   adequate fit and function of the implant. Accordingly, the accuracy and speed of the
   navigation system may need to be greater than in more rough cutting procedures.
   [0010]             One of the limitations on accuracy and speed of optical navigation
   systems is that the system relies on the line-of-sight between the LEDs and the optical
   sensors of the camera unit.      When the line-of-sight is broken, the system may not
   accurately determine the position and/or orientation of the instrument and anatomy being
   tracked. As a result, surgeries can encounter many starts and stops. For instance, during
   control of robotically assisted cutting, when the line-of-sight is broken, the cutting tool
   must be disabled until the line-of-sight is regained. This can cause significant delays and
                                                 2

WO 2014/052428                                                                 PCT/US2013/061642
   added cost to the procedure.
   [0011]              Another limitation on accuracy occurs when using active LEDs on the
   trackers. In such systems, the LEDs are often fired in sequence. In this case only the
  position of the actively fired LED is measured and known by the system, while the
  positions of the remaining, unmeasured LEDs are unknown.                In these systems, the
  positions of the remaining, unmeasured LEDs are approximated. Approximations are
  usually based on linear velocity data extrapolated from the last known measured
  positions of the currently unmeasured LEDs. However, because the LEDs are fired in
   sequence, there can be a considerable lag between measurements of any one LED. This
   lag is increased with each additional tracker used in the system. Furthermore, this
   approximation does not take into account rotations of the trackers, resulting in further
  possible errors in position data for the trackers.
   [0012]              As a result, there is a need in the art for an optical navigation system
   that utilizes additional non-optically based data to improve tracking and provide a level
   of accuracy and speed with which to determine position and/or orientations of objects for
  precise surgical procedures such as robotically assisted surgical cutting.
                               SUMMARY OF THE INVENTION
   [0013]           The present invention relates to systems and methods that utilize optical
   sensors and non-optical sensors to determine the position and/or orientation of objects.
   [0014]           In one version of the invention a navigation system is provided for
   tracking an object.     The navigation system includes an optical sensor that receives
   optical signals from one or more markers on a tracker. The tracker also includes a non
   optical sensor that generates non-optical signals.      A computing system determines a
  position of one of the markers at a first time based on a first optical signal.           The
   computing system also determines a position of one ore more of the other markers at the
   first time based on the first optical signal and non-optical signals from the non-optical
   sensor. The determined positions are then correlated to the object to track a position of
   the object.
   [0015]           In another version of the invention a navigation system is provided for
   tracking an object.     The navigation system includes an optical sensor that receives
   optical signals sequentially from three markers on a tracker. The tracker also includes a
   non-optical sensor that generates non-optical signals. A computing system determines a
                                                  3

WO 2014/052428                                                                PCT/US2013/061642
  position of a first of the markers at a first time based on a first optical signal from the
   first marker. The computing system also determines a position of a second and third of
   the markers at the first time based on the first optical signal and non-optical signals from
   the non-optical sensor. The determined positions are then correlated to the object to
   track a position of the object.
   [0016]          In yet another version of the invention, a robotic surgical cutting system
   is provided. The system includes a robotic manipulator and a cutting tool. A robotic
   control system controls or constrains movement of the cutting tool in at least 5 degrees
   of freedom. A navigation system communicates with the robotic control system. The
   navigation system includes at least one optical sensor and a tracker mounted to the
  robotic manipulator. A tracker is also provided for mounting to a patient's anatomy.
   This anatomy tracker includes three markers and a non-optical sensor. The optical
   sensor receives optical signals from the markers and the non-optical sensor generates
   non-optical signals. The navigation system communicates position data indicative of a
  position of the anatomy to the robotic control system to control cutting of the anatomy
   such that the cutting occurs within a predefined boundary.
   [0017]          In another version of the invention a navigation system is provided that
   includes a localizer having at least one optical sensor. A tracker communicates with the
   optical sensor.    The tracker includes three markers and a non-optical sensor.            A
   computing system determines a position of each of the three markers in a localizer
   coordinate system based on optical signals and non-optical signals.         The computing
   system performs a matching algorithm to match the determined positions of one or more
   of the markers in the localizer coordinate system with positions of the one or more of
   said markers in a model of the tracker established relative to a tracker coordinate system
   to obtain a transformation matrix to transform the tracker coordinate system to the
   localizer coordinate system.
   [0018]          In another version of the invention, a system is provided for tracking an
   object. The system comprises at least two optical sensors and a tracker for mounting to
   the object. The tracker has three markers and a non-optical sensor. The at least two
   optical sensors receives optical signals from the markers at an optical-sensing frequency
   of at least 100 Hz. The non-optical sensor generates non-optical signals at a non-optical
   sensing frequency of at least 100 Hz.
   [0019]          A method for tracking an object is also provided. The method includes
                                                 4

WO 2014/052428                                                                PCT/US2013/061642
   operating an optical sensor to receive optical signals sequentially from markers and
   operating a non-optical sensor to generate non-optical signals. A position of a first of the
   markers at a first time is determined based on a first optical signal from the first marker.
   A position of a second and third of the markers at the first time is determined based on
   the first optical signal and non-optical signals from the non-optical sensor.           The
   determined positions of the first, second, and third markers are correlated to the object to
   track a position of the object during the surgical procedure.
   [0020]           Another method for tracking an object during a surgical procedure is
  provided. In this method three markers are positioned in the field of view of the optical
   sensor so that the optical sensor receives optical signals sequentially from the markers.
   A computing system is then operated to determine a position of a first of the markers at a
   first time based on a first optical signal from the first marker and determine a position of
   a second and third of the markers at the first time based on the first optical signal and
   non-optical signals from a non-optical sensor. The positions are then correlated to the
   object to track a position of the object during the surgical procedure.
                          BRIEF DESCRIPTION OF THE DRAWINGS
   [0021]              Advantages of the present invention will be readily appreciated as the
   same becomes better understood by reference to the following detailed description when
   considered in connection with the accompanying drawings wherein:
   [0022]              Figure 1 is a perspective view of a navigation system of the present
   invention being used in conjunction with a robotic manipulator;
   [0023]              Figure 2 is a schematic view of the navigation system;
   [0024]              Figure 3 is schematic view of coordinate systems used with the
   navigation system;
   [0025]              Figure 4 is a flow diagram of steps carried out by a localization engine
   of the navigation system;
   [0026]              Figure 4A is a schematic illustration of matching measured LEDs with
   a tracker model to obtain a transformation matrix;
   [0027]              Figure 5 is a flow diagram of steps carried out by the localization
   engine in a first alternative embodiment;
   [0028]              Figure 5A is an illustration of a tracker model including real and
   virtual LEDs;
                                                  5

WO 2014/052428                                                               PCT/US2013/061642
   [0029]             Figure 6 is a flow diagram of steps carried out by the localization
   engine in a second alternative embodiment; and
   [0030]             Figure 7 is a flow diagram of steps carried out by the localization
   engine when one or more LEDs are blocked from measurement.
                                  DETAILED DESCRIPTION
   I.      OVERVIEW
   [0031]             Referring to Figure 1 a surgical navigation system 20 is illustrated.
   System 20 is shown in a surgical setting such as an operating room of a medical facility.
   The navigation system 20 is set up to track movement of various objects in the operating
  room.     Such objects include, for example, a surgical instrument 22, a femur F of a
  patient, and a tibia T of the patient. The navigation system 20 tracks these objects for
  purposes of displaying their relative positions and orientations to the surgeon and, in
   some cases, for purposes of controlling or constraining movement of the surgical
   instrument 22 relative to a predefined path or anatomical boundary.
   [0032]             The surgical navigation system 20 includes a computer cart assembly
   24 that houses a navigation computer 26.         A navigation interface is in operative
   communication with the navigation computer 26. The navigation interface includes a
   display 28 adapted to be situated outside of the sterile field and a display 29 adapted to
  be situated inside the sterile field. The displays 28, 29 are adjustably mounted to the
   computer cart assembly 24. Input devices 30, 32 such as a mouse and keyboard can be
  used to input information into the navigation computer 26 or otherwise select/control
   certain aspects of the navigation computer 26. Other input devices are contemplated
   including a touch screen (not shown) on displays 28, 29 or voice-activation.
   [0033]             A localizer 34 communicates with the navigation computer 26. In the
   embodiment shown, the localizer 34 is an optical localizer and includes a camera unit 36.
   The camera unit 36 has an outer casing 38 that houses one or more optical sensors 40. In
   some embodiments at least two optical sensors 40 are employed, preferably three. The
   optical sensors 40 may be three separate high resolution charge-coupled devices (CCD).
   In one embodiment three, one-dimensional CCDs are employed.                  It should be
   appreciated that in other embodiments, separate camera units, each with a separate CCD,
   or two or more CCDs, could also be arranged around the operating room. The CCDs
                                                6

WO 2014/052428                                                             PCT/US2013/061642
   detect infrared (IR) signals.
   [0034]             Camera unit 36 is mounted on an adjustable arm to position the optical
   sensors 40 above the zone in which the procedure is to take place to provide the camera
  unit 36 with a field of view of the below discussed trackers that, ideally, is free from
   obstructions.
   [0035]             The camera unit 36 includes a camera controller 42 in communication
   with the optical sensors 40 to receive signals from the optical sensors 40. The camera
   controller 42 communicates with the navigation computer 26 through either a wired or
   wireless connection (not shown). One such connection may be an IEEE 1394 interface,
   which is a serial bus interface standard for high-speed communications and
   isochronous real-time data transfer. Connection could also use a company specific
  protocol. In other embodiments, the optical sensors 40 communicate directly with the
   navigation computer 26.
   [0036]             Position and orientation signals and/or data are transmitted to the
   navigation computer 26 for purposes of tracking the objects.          The computer cart
   assembly 24, display 28, and camera unit 36 may be like those described in U.S. Patent
   No. 7,725,162 to Malackowski, et al. issued on May 25, 2010, entitled "Surgery
   System", hereby incorporated by reference.
   [0037]             The navigation computer 26 can be a personal computer or laptop
   computer. Navigation computer 26 has the display 28, central processing unit (CPU)
   and/or other processors, memory (not shown), and storage (not shown).                The
   navigation computer 26 is loaded with software as described below. The software
   converts the signals received from the camera unit 36 into data representative of the
  position and orientation of the objects being tracked.
   [0038]             Navigation system 20 includes a plurality of tracking devices 44, 46,
   48, also referred to herein as trackers. In the illustrated embodiment, one tracker 44 is
   firmly affixed to the femur F of the patient and another tracker 46 is firmly affixed to
   the tibia T of the patient. Trackers 44, 46 are firmly affixed to sections of bone.
   Trackers 44, 46 may be attached to the femur F in the manner shown in U.S. Patent
   No. 7,725,162, hereby incorporated by reference.            In further embodiments, an
   additional tracker (not shown) is attached to the patella to track a position and
   orientation of the patella.    In further embodiments, the trackers 44, 46 could be
   mounted to other tissue types or parts of the anatomy.
                                                7

WO 2014/052428                                                            PCT/US2013/061642
   [0039]            An instrument tracker 48 is firmly attached to the surgical
   instrument 22.     The instrument tracker 48 may be integrated into the surgical
   instrument 22 during manufacture or may be separately mounted to the surgical
   instrument 22 in preparation for the surgical procedures.      The working end of the
   surgical instrument 22, which is being tracked, may be a rotating bur, electrical
   ablation device, or the like. In the embodiment shown, the surgical instrument 22 is
   an end effector of a surgical manipulator.     Such an arrangement is shown in U.S.
   Provisional Patent Application No. 61/679,258, entitled, "Surgical Manipulator
   Capable of Controlling a Surgical Instrument in either a Semi-Autonomous Mode or a
   Manual, Boundary Constrained Mode", the disclosure of which is hereby incorporated
  by reference, and also in U.S.         Patent Application No. 13/958,834,        entitled,
   "Navigation System for use with a Surgical Manipulator Operable in Manual or Semi
   Autonomous Mode", the disclosure of which is hereby incorporated by reference.
   [0040]            The trackers 44, 46, 48 can be battery powered with an internal
  battery or may have leads to receive power through the navigation computer 26,
   which, like the camera unit 36, preferably receives external power.
   [0041]            In other embodiments, the surgical instrument 22 may be manually
  positioned by only the hand of the user, without the aid of any cutting guide, jib, or
   other constraining mechanism such as a manipulator or robot.           Such a surgical
   instrument is described in U.S. Provisional Patent Application No. 61/662,070, entitled,
   "Surgical Instrument Including Housing, a Cutting Accessory that Extends from the
   Housing and Actuators that Establish the Position of the Cutting Accessory Relative to
   the Housing", hereby incorporated by reference, and also in U.S. Patent Application No.
   13/600,888, entitled "Surgical Instrument Including Housing, a Cutting Accessory that
   Extends from the Housing and Actuators that Establish the Position of the Cutting
   Accessory Relative to the Housing", hereby incorporated by reference.
   [0042]            The optical sensors 40 of the localizer 34 receive light signals from
   the trackers 44, 46, 48. In the illustrated embodiment, the trackers 44, 46, 48 are
   active trackers. In this embodiment, each tracker 44, 46, 48 has at least three active
   markers 50 for transmitting light signals to the optical sensors 40. The active markers
   50 can be light emitting diodes or LEDs 50. The optical sensors 40 preferably have
   sampling rates of 100 Hz or more, more preferably 300 Hz or more, and most preferably
   500 Hz or more. In some embodiments, the optical sensors 40 have sampling rates of
                                               8

WO 2014/052428                                                                PCT/US2013/061642
   1000 Hz. The sampling rate is the rate at which the optical sensors 40 receive light
   signals from sequentially fired LEDs 50. In some embodiments, the light signals from
   the LEDs 50 are fired at different rates for each tracker 44, 46, 48.
   [0043]             Referring to Figure 2, each of the LEDs 50 are connected to a
   tracker controller 62 located in a housing (not shown) of the associated tracker 44, 46,
   48 that transmits/receives     data to/from the navigation computer 26.            In one
   embodiment, the tracker controllers 62 transmit data on the order of several
   Megabytes/second through wired connections with the navigation computer 26. In
   other embodiments, a wireless connection may be used. In these embodiments, the
   navigation computer 26 has a transceiver (not shown) to receive the data from the
   tracker controller 62.
   [0044]             In other embodiments, the trackers 44, 46, 48 may have passive
   markers (not shown), such as reflectors that reflect light emitted from the camera unit
   36. The reflected light is then received by the optical sensors 40. Active and passive
   arrangements are well known in the art.
   [0045]             Each of the trackers 44, 46, 48 also includes a 3-dimensional
   gyroscope sensor 60 that measures angular velocities of the trackers 44, 46, 48. As is
   well known to those skilled in the art, the gyroscope sensors 60 output readings
   indicative of the angular velocities relative to x-, y-, and z- axes of a gyroscope
   coordinate system. These readings are multiplied by a conversion constant defined by
   the manufacturer to obtain measurements in degrees/second with respect to each of
   the x-, y-, and z- axes of the gyroscope coordinate system. These measurements can
   then be converted to an angular velocity vector Vf defined in radians/second.
   [0046]             The angular velocities measured by the gyroscope sensors 60
  provide additional non-optically based kinematic data for the navigation system 20
   with which to track the trackers 44, 46, 48.          The gyroscope sensors 60 may be
   oriented along the axis of each coordinate system of the trackers 44, 46, 48. In other
   embodiments, each gyroscope coordinate system is transformed to its tracker
   coordinate system such that the gyroscope data reflects the angular velocities with
  respect to the x-, y-, and z- axes of the coordinate systems of the trackers 44, 46, 48.
   [0047]             Each of the gyroscope sensors 60 communicate with the tracker
   controller   62   located   within    the   housing    of   the   associated tracker  that
   transmits/receives data to/from the navigation computer 26. The navigation computer
                                                 9

WO 2014/052428                                                             PCT/US2013/061642
   26 has one or more transceivers (not shown) to receive the data from the gyroscope
   sensors 60. The data can be received either through a wired or wireless connection.
   [0048]             The gyroscope sensors 60 preferably have sampling rates of 100 Hz or
   more, more preferably 300 Hz or more, and most preferably 500 Hz or more. In some
   embodiments, the gyroscope sensors 60 have sampling rates of 1000 Hz. The sampling
  rate of the gyroscope sensors 60 is the rate at which signals are sent out from the
   gyroscope sensors 60 to be converted into angular velocity data.
   [0049]             The sampling rates of the gyroscope sensors 60 and the optical sensors
   40 are established or timed so that for each optical measurement of position there is a
   corresponding non-optical measurement of angular velocity.
   [0050]             Each of the trackers 44, 46, 48 also includes a 3-axis accelerometer
   70 that measures acceleration along each of x-, y-, and z- axes of an accelerometer
   coordinate system. The accelerometers 70 provide additional non-optically based
   data for the navigation system 20 with which to track the trackers 44, 46, 48.
   [0051]             Each of the accelerometers 70 communicate with the tracker
   controller 62 located in the housing of the associated tracker that transmits/receives
   data to/from the navigation computer 26. One or more of the transceivers (not shown)
   of the navigation computer receives the data from the accelerometers 70.
   [0052]             The accelerometers 70 may be oriented along the axis of each
   coordinate system of the trackers 44, 46, 48.              In other embodiments, each
   accelerometer coordinate system is transformed to its tracker coordinate system such
   that the accelerometer data reflects the accelerations with respect to the x-, y-, and z
   axes of the coordinate systems of the trackers 44, 46, 48.
   [0053]             The navigation computer 26 includes a navigation processor 52.
   The camera unit 36 receives optical signals from the LEDs 50 of the trackers 44, 46,
   48 and outputs to the processor 52 signals relating to the position of the LEDs 50 of
   the trackers 44, 46, 48 relative to the localizer 34. The gyroscope sensors 60 transmit
   non-optical signals to the processor 52 relating to the 3-dimensional angular velocities
   measured by the gyroscope sensors 60. Based on the received optical and non-optical
   signals, navigation processor 52 generates data indicating the relative positions and
   orientations of the trackers 44, 46, 48 relative to the localizer 34.
   [0054]             It should be understood that the navigation processor 52 could
   include one or more processors to control operation of the navigation computer 26.
                                                10

WO 2014/052428                                                               PCT/US2013/061642
   The processors can be any type of microprocessor or multi-processor system. The
   term processor is not intended to limit the scope of the invention to a single processor.
   [0055]             Prior to the start of the surgical procedure, additional data are loaded
   into the navigation processor 52. Based on the position and orientation of the trackers
   44, 46, 48 and the previously loaded data, navigation processor 52 determines the
  position of the working end of the surgical instrument 22 and the orientation of the
   surgical instrument 22 relative to the tissue against which the working end is to be
   applied.   In some embodiments, navigation processor 52 forwards these data to a
   manipulator controller 54. The manipulator controller 54 can then use the data to
   control a robotic manipulator 56 as described in U.S. Provisional Patent Application
   No. 61/679,258, entitled, "Surgical Manipulator Capable of Controlling a Surgical
   Instrument in either a Semi-Autonomous Mode or a Manual, Boundary Constrained
   Mode, the disclosure of which is hereby incorporated by reference, and also in U.S.
   Patent Application No. 13/958,834, entitled, "Navigation System for use with a
   Surgical Manipulator      Operable     in Manual or Semi-Autonomous Mode",               the
   disclosure of which is hereby incorporated by reference.
   [0056]             The navigation processor 52 also generates image signals that
   indicate the relative position of the surgical instrument working end to the surgical
   site. These image signals are applied to the displays 28, 29. Displays 28, 29, based
   on these signals, generate images that allow the surgeon and staff to view the relative
  position of the surgical instrument working end to the surgical site. The displays, 28,
   29, as discussed above, may include a touch screen or other input/output device that
   allows entry of commands.
   II.     COORDINATE SYSTEMS AND TRANSFORMATION
   [0057]             Referring to Figure 3, tracking of objects is generally conducted
   with reference to a localizer coordinate system LCLZ.            The localizer coordinate
   system has an origin and an orientation (a set of x-, y-, and z-axes).          During the
  procedure one goal is to keep the localizer coordinate system LCLZ stationary. As
   will be described further below, an accelerometer mounted to the camera unit 36 may
  be used to track sudden or unexpected movement of the localizer coordinate system
   LCLZ, as may occur when the camera unit 36 is inadvertently bumped by surgical
  personnel.
                                                 11

WO 2014/052428                                                              PCT/US2013/061642
   [0058]             Each tracker 44, 46, 48 and object being tracked also has its own
   coordinate system separate from localizer coordinate system LCLZ. Components of
   the navigation system 20 that have their own coordinate systems are the bone trackers
   44, 46 and the instrument tracker 48. These coordinate systems are represented as,
  respectively, bone tracker coordinate systems BTRK1, BTRK2, and instrument
   tracker coordinate system TLTR.
   [0059]             Navigation system 20 monitors the positions of the femur F and
   tibia T of the patient by monitoring the position of bone trackers 44, 46 firmly
   attached to bone. Femur coordinate system is FBONE and tibia coordinate system is
   TBONE, which are the coordinate systems of the bones to which the bone trackers 44,
   46 are firmly attached.
   [0060]             Prior to the start of the procedure, pre-operative images of the femur
   F and tibia T are generated (or of other tissues in other embodiments). These images
   may be based on MRI scans, radiological scans or computed tomography (CT) scans
   of the patient's anatomy. These images are mapped to the femur coordinate system
   FBONE and tibia coordinate system TBONE using well known methods in the art. In
   one embodiment, a pointer instrument P, such as disclosed in U.S Patent No.
   7,725,162 to Malackowski, et al., hereby incorporated by reference, having its own
   tracker PT (see Figure 2), may be used to map the femur coordinate system FBONE
   and tibia coordinate system TBONE to the pre-operative images. These images are
   fixed in the femur coordinate system FBONE and tibia coordinate system TBONE.
   [0061]             During the initial phase of the procedure, the bone trackers 44, 46
   are firmly affixed to the bones of the patient. The pose (position and orientation) of
   coordinate systems FBONE and TBONE are mapped to coordinate systems BTRK1
   and BTRK2, respectively. Given the fixed relationship between the bones and their
  bone trackers 44, 46, the pose of coordinate systems FBONE and TBONE remain
   fixed relative to coordinate systems BTRK1 and BTRK2, respectively, throughout the
  procedure.      The pose-describing data are stored in memory integral with both
   manipulator controller 54 and navigation processor 52.
   [0062]             The working end of the surgical instrument 22 (also referred to as
   energy applicator distal end) has its own coordinate system EAPP. The origin of the
   coordinate system EAPP may represent a centroid of a surgical cutting bur, for
   example. The pose of coordinate system EAPP is fixed to the pose of instrument
                                                 12

WO 2014/052428                                                              PCT/US2013/061642
   tracker coordinate system TLTR before the procedure begins. Accordingly, the poses
   of these coordinate systems EAPP, TLTR relative to each other are determined. The
  pose-describing data are stored in memory integral with both manipulator controller
   54 and navigation processor 52.
   III.    SOFTWARE
   [0063]             Referring to Figure 2, a localization engine 100 is a software
   module that can be considered part of the navigation system 20. Components of the
   localization engine 100 run on navigation processor 52.         In some versions of the
   invention, the localization engine 100 may run on the manipulator controller 54.
   [0064]             Localization engine 100 receives as inputs the optically-based
   signals from the camera controller 42 and the non-optically based signals from the
   tracker controller 62. Based on these signals, localization engine 100 determines the
  pose (position and orientation) of the bone tracker coordinate systems BTRKI and
   BTRK2 in the localizer coordinate system LCLZ. Based on the same signals received
   for the instrument tracker 48, the localization engine 100 determines the pose of the
   instrument tracker coordinate system TLTR in the localizer coordinate system LCLZ.
   [0065]             The localization engine 100 forwards the signals representative of
   the poses of trackers 44, 46, 48 to a coordinate transformer 102.              Coordinate
   transformer 102 is a navigation system software module that runs on navigation
  processor 52.      Coordinate transformer 102 references the data that defines the
  relationship between the pre-operative images of the patient and the patient trackers
   44, 46.   Coordinate transformer 102 also stores the data indicating the pose of the
   working end of the surgical instrument relative to the instrument tracker 48.
   [0066]             During the procedure, the coordinate transformer 102 receives the
   data indicating the relative poses of the trackers 44, 46, 48 to the localizer 34. Based
   on these data and the previously loaded data, the coordinate transformer 102 generates
   data indicating the relative position and orientation of both the coordinate system
   EAPP, and the bone coordinate systems, FBONE and TBONE to the localizer
   coordinate system LCLZ.
   [0067]             As a result, coordinate transformer 102 generates data indicating the
  position and orientation of the working end of the surgical instrument 22 relative to
   the tissue (e.g., bone) against which the instrument working end is applied. Image
                                               13

WO 2014/052428                                                               PCT/US2013/061642
   signals representative of these data are forwarded to displays 28, 29 enabling the
   surgeon and staff to view this information. In certain embodiments, other signals
  representative of these data can be forwarded to the manipulator controller 54 to
   control the manipulator 56 and corresponding movement of the surgical instrument
   22.
   [0068]              Steps for determining the pose of each of the tracker coordinate
   systems BTRK1, BTRK2, TLTR in the localizer coordinate system LCLZ are the
   same, so only one will be described in detail. The steps shown in Figure 4 are based
   on only one tracker being active, tracker 44. In the following description, the LEDs
   of tracker 44 shall be represented by numerals 50a, 50b, 50c which identify first 50a,
   second 50b, and third 50c LEDs.
   [0069]              The steps set forth in Figure 4 illustrate the use of optically-based
   sensor data and non-optically based sensor data to determine the positions of the
   LEDs 50a, 50b, 50c of tracker 44. From these positions, the navigation processor 52
   can determine the position and orientation of the tracker 44, and thus, the position and
   orientation of the femur F to which it is attached. Optically-based sensor data derived
   from the signals received by the optical sensors 40 provide line-of-sight based data
   that relies on the line-of-sight between the LEDs 50a, 50b, 50c and the optical sensors
   40. However, the gyroscope sensor 60, which provides non-optically based signals
   for generating non-optically based sensor data do not rely on line-of-sight and thus
   can be integrated into the navigation system 20 to better approximate positions of the
   LEDs 50a, 50b, 50c when two of the LEDs 50a, 50b, 50c are not being measured
   (since only one LED measured at a time), or when one or more of the LEDs 50a, 50b,
   50c are not visible to the optical sensors 40 during a procedure.
   [0070]              In a first initialization step 200, the system 20 measures the position
   of the LEDs 50a, 50b, 50c for the tracker 44 in the localizer coordinate system LCLZ
   to establish initial position data. These measurements are taken by sequentially firing
   the LEDs 50a, 50b, 50c, which transmits light signals to the optical sensors 40. Once
   the light signals are received by the optical sensors 40, corresponding signals are
   generated by the optical sensors 40 and transmitted to the camera controller 42. The
   frequency between firings of the LEDs 50a, 50b, 50c is 100 Hz or greater, preferably
   300 Hz or greater, and more preferably 500 Hz or greater.              In some cases, the
   frequency between firings is 1000 Hz or 1 millisecond between firings.
                                                   14

WO 2014/052428                                                             PCT/US2013/061642
   [0071]              In some embodiments, only one LED can be read by the optical
   sensors 40 at a time. The camera controller 42, through one or more infrared or RF
   transceivers (on camera unit 36 and tracker 44) may control the firing of the LEDs
   50a, 50b, 50c, as described in U.S. Patent No. 7,725,162 to Malackowski, et al., hereby
   incorporated by reference.      Alternatively, the tracker 44 may be activated locally
   (such as by a switch on tracker 44) which then fires its LEDs 50a, 50b, 50c
   sequentially once activated, without instruction from the camera controller 42.
   [0072]              Based on the inputs from the optical sensors 40, the camera
   controller 42 generates raw position signals that are then sent to the localization
   engine 100 to determine the position of each of the corresponding three LEDs 50a,
   50b, 50c in the localizer coordinate system LCLZ.
   [0073]              During the initialization step 200, in order to establish the initial
  position data, movement of the tracker 44 must be less than a predetermined
   threshold.     A value of the predetermined threshold is stored in the navigation
   computer 26. The initial position data established in step 200 essentially provides a
   static snapshot of position of the three LEDs 50a, 50b, 50c at an initial time tO, from
   which to base the remaining steps of the process. During initialization, velocities of
   the LEDs 50a, 50b, 50c are calculated by the localization engine 100 between cycles
   (i.e., each set of three LED measurements) and once the velocities are low enough,
   i.e., less than the predetermined threshold showing little movement occurred, then the
   initial position data or static snapshot is established.     In some embodiments, the
  predetermined threshold (also referred to as the static velocity limit) is 200 mm/s or
   less, preferably 100 mm/s or less, and more preferably 10 mm/s or less along any axis.
  When the predetermined threshold is 100 mm/s, then the calculated velocities must be
   less than 100 mm/s to establish the static snapshot.
   [0074]              Referring to FIGS. 4 and 4A, once the static snapshot is taken, the
  positions of the measured LEDs 50a, 50b, 50c are compared to a model of the tracker
   44 in step 202. The model is data stored in the navigation computer 26. The model
   data indicates the positions of the LEDs on the tracker 44 in the tracker coordinate
   system BTRK1. The system 20 has stored the number and position of the LEDs 50 of
   each tracker 44, 46, 48 in each tracker's coordinate system. For trackers 44, 46, 48
   the origin of their coordinate systems is set at the centroid of all LED positions of the
   tracker 44.
                                                15

WO 2014/052428                                                                PCT/US2013/061642
   [0075]             The localization engine 100 utilizes a rigid body matching algorithm
   or point matching algorithm to match the measured LEDs 50a, 50b, 50c in the
   localizer coordinate system LCLZ to the LEDs in the stored model. Once the best-fit
   is determined, the localization engine 100 evaluates the deviation of the fit to
   determine if the measured LEDs 50a, 50b, 50c fit within a stored predefined tolerance
   of the model. The tolerance may be based on a distance between the corresponding
   LEDs such that if the fit results in too great of a distance, the initialization step has to
  be repeated. In some embodiments, the positions of the LEDs must not deviate from
   the model by more than 2.0 mm, preferably not more than 0.5 mm, and more
  preferably not more than 0.1 mm.
   [0076]             If the fit is within the predefined tolerance, a transformation matrix
   is generated to transform any other unmeasured LEDs in the model from the bone
   tracker coordinate system BTRK1 into the localizer coordinate system LCLZ in step
   204. This step is utilized if more than three LEDs are used or if virtual LEDs are used
   as explained further below. In some embodiments, trackers 44, 46, 48 may have four
   or more LEDs.       Once all positions in the localizer coordinate system LCLZ are
   established, an LED cloud is created. The LED cloud is an arrangement of all LEDs
   50a, 50b, 50c on the tracker 44 in the localizer coordinate system LCLZ based on the
   x-, y-, and z- axis positions of all the LEDs 50a, 50b, 50c in the localizer coordinate
   system LCLZ.
   [0077]             Once the LED cloud is initially established, the navigation system
   20 can proceed with tracking the tracker 44 during a surgical procedure.                 As
  previously discussed, this includes firing the next LED in the sequence.                 For
   illustration, LED 50a is now fired. Thus, LED 50a transmits light signals to the
   optical sensors 40. Once the light signals are received by the optical sensors 40,
   corresponding signals are generated by the optical sensors 40 and transmitted to the
   camera controller 42.
   [0078]             Based on the inputs from the optical sensors 40, the camera
   controller 42 generates a raw position signal that is then sent to the localization engine
   100 to determine at time ti the new position of LED 50a relative to the x-, y-, and z
   axes of the localizer coordinate system LCLZ. This is shown in step 206 as a new
   LED measurement.
   [0079]             It should be appreciated that the designation of time such as tO,
                                                16

WO 2014/052428                                                                 PCT/US2013/061642
   tI ...tn is used for illustrative purposes to indicate different times or different ranges of
   time or time periods and does not limit this invention to specific or definitive times.
   [0080]               With the new position of LED 50a determined, a linear velocity
   vector of LED 50a can be calculated by the localization engine 100 in step 208.
   [0081]               The tracker 44 is treated as a rigid body. Accordingly, the linear
   velocity vector of LED 50a is a vector quantity, equal to the time rate of change of its
   linear position.      The velocity, even the acceleration of each LED, in localizer
   coordinate system LCLZ, can be calculated from the previously and currently
   measured positions and time of that LED in localizer coordinate system LCLZ. The
  previously and currently measured positions and time of a LED define the position
  history of that LED. The velocity calculation of LED 50a can take the simplest form
   of:
                                       - (LED5Oa)       -
                                                        tn - tp
  Where xp = (x, y, z)p and is the previously measured position of LED 50a at time ty;
   and in = (x, y, z)n and is the currently measured position of LED 50a at time tn. One
   can also obtain the velocity and/or acceleration of each LED by data fitting the LED
  position history of that LED as is well known to those skilled in the art.
   [0082]               At time tl, in step 210, the gyroscope sensor 60 is also measuring an
   angular velocity of the tracker 44.         Gyroscope sensor 60 transmits signals to the
   tracker controller 62 related to this angular velocity.
   [0083]               The tracker controller 62 then transmits a corresponding signal to
   the localization engine 100 so that the localization engine 100 can calculate an
   angular velocity vector -a)from these signals. In step 210, the gyroscope coordinate
   system is also transformed to the bone tracker coordinate system BTRK1 so that the
   angular velocity vector -a)calculated by the localization engine 100 is expressed in
   the bone tracker coordinate system BTRK1.
   [0084]               In step 212, a relative velocity vector WR is calculated for the origin
   of the bone tracker coordinate system BTRK1 with respect to position vector
    - (LED50a to ORIGIN).            This position vector - (LED50a to ORIGIN) is also
   stored in memory in the navigation computer 26 for access by the localization engine
                                                  17

WO 2014/052428                                                               PCT/US2013/061642
   100 for the following calculation. This calculation determines the relative velocity of the
   origin -vR(ORIGIN) of the bone tracker coordinate system BTRK1 by calculating the
   cross product of the angular velocity vector -0jderived from the gyroscope signal and the
  position vector from LED 50a to the origin.
                             VR (ORIGIN) = - x 35(LED50a to ORIGIN)
   [0085]              The localization engine 100 then calculates relative velocity vectors
  WR for the remaining, unmeasured LEDs 50b, 50c (unmeasured because these LEDs
  have not been fired and thus their positions are not being measured). These velocity
   vectors can be calculated with respect to the origin of bone tracker coordinate system
   BTRK1.
   [0086]             The calculation performed by the localization           engine 100 to
   determine the relative velocity vector WR for each unmeasured LED 50b, 50c at time
   tI is based on the cross product of the angular velocity vector -0j at time tI and the
  position vectors     - (ORIGIN to LED50b) and - (ORIGIN to LED50c), which are
   taken from the origin of bone tracker coordinate system BTRK1 to each of the
  unmeasured LEDs 50b, 50c.          These position vectors x (ORIGIN to LED50b) and
    - (ORIGIN to LED50c) are stored in memory in the navigation computer 26 for
   access by the localization engine 100 for the following calculations:
                       WR (LED50b) = 0i x         (ORIGIN to LED50b)
                        -vR(LED50c) = 0i x        (ORIGIN to LED50c)
   [0087]             Also in step 212, these relative velocities, which are calculated in
   the bone tracker coordinate system BTRK1, are transferred into the localizer
   coordinate system LCLZ using the transformation matrix determined in step 202. The
  relative velocities in the localizer coordinate system LCLZ are used in calculations in
   step 214.
   [0088]             In step 214 the velocity vector - of the origin of the bone tracker
   coordinate system BTRK1 in the localizer coordinate system LCLZ at time tI is first
   calculated by the localization engine 100 based on the measured velocity vector
    - (LED50a) of LED 50a at time tI. The velocity vector F (ORIGIN) is calculated
                                               18

WO 2014/052428                                                              PCT/US2013/061642
  by adding the velocity vector v (LED50a) of LED 50a at time tI and the relative
   velocity vector vR (ORIGIN) of the origin at time tI expressed relative to the position
   vector of LED 50a to the origin. Thus, the velocity vector of the origin at time tI is
   calculated as follows:
                        - (ORIGIN) = - (LED50a) + WR (ORIGIN)
   [0089]             Velocity vectors    of the remaining, unmeasured LEDs in the
   localizer coordinate system LCLZ at time t1 can now be calculated by the localization
   engine 100 based on the velocity vector - (ORIGIN) of the origin of the bone tracker
   coordinate system BTRK1 in the localizer coordinate system LCLZ at time tl and
   their respective relative velocity vectors at time tl expressed relative to their position
   vectors with the origin of the bone tracker coordinate system BTRK1. These velocity
   vectors at time tl are calculated as follows:
                        - (LED50b) =        (ORIGIN) + vR (LED50b)
                        - (LED50c) = - (ORIGIN) + -vR(LED50c)
   [0090]             In step 216, the localization engine 100 calculates the movements,
   i.e., the change in position Ax (in Cartesian coordinates), of each of the unmeasured
   LEDs 50b, 50c from time tO to time tl based on the calculated velocity vectors of
   LEDs 50b, 50c and the change in time. In some embodiments the change in time At
   for each LED measurement is two milliseconds or less, and in some embodiments one
   millisecond or less.
                             Ax (LED50b) = i (LED50b) x At
                             Ax (LED50c) = i(LED50c) x At
   [0091]             These calculated changes in position (x, y, z) can then be added to
   the previously determined positions of each of LEDs 50b, 50c in the localizer
   coordinate system LCLZ. Thus, in step 218, changes in position can be added to the
  previous positions of the LEDs 50b, 50c at time tO, which were determined during the
   static snapshot. This is expressed as follows:
                                               19

WO 2014/052428                                                              PCT/US2013/061642
                        x (LED50b)t 1 = x (LED50b)to + Ax (LED50b)
   [0092]              In step 220, these calculated positions for each of LEDs 50b, 50c at
   time tl are combined with the determined position of LED 50a at time tl. The newly
   determined positions of LEDs 50a, 50b, 50c are then matched to the model of tracker
   44 to obtain a best fit using the point matching algorithm or rigid body matching
   algorithm. The result of this best fit calculation, if within the defined tolerance of the
   system 20, is that a new transformation matrix is created by the navigation processor
   52 to link the bone tracker coordinate system BTRK1 to the localizer coordinate
   system LCLZ.
   [0093]              With the new transformation matrix the newly calculated positions
   of the unmeasured LEDs 50b, 50c are adjusted to the model in step 222 to provide
   adjusted positions. The measured position of LED 50a can also be adjusted due to the
   matching algorithm such that it is also recalculated. These adjustments are considered
   an update to the LED cloud. In some embodiments, the measured position of LED
   50a is fixed to the model's position of LED 50a during the matching step.
   [0094]              With the best fit transformation complete, the measured (and
  possibly adjusted) position of LED 50a and the calculated (and adjusted) positions of
   LEDs 50b, 50c in the localizer coordinate system LCLZ enable the coordinate
   transformer 102 to determine a new position and orientation of the femur F based on
   the previously described relationships between the femur coordinate system FBONE,
   the bone tracker coordinate system BTRK1, and the localizer coordinate system
   LCLZ.
   [0095]              Steps 206 through 222 are then repeated at a next time t2 and start
   with the measurement in the localizer coordinate system LCLZ of LED 50b, with
   LEDs 50a, 50c being the unmeasured LEDs. As a result of this loop at each time tl,
   t2... tn positions of each LED 50a, 50b, 50c are either measured (one LED being fired
   at each time) or calculated with the calculated positions being very accurately
   approximated based on measurements by the optical sensor 40 and the gyroscope
   sensor 60. This loop of steps 206 through 222 to determine the new positions of the
   LEDs 50 can be carried out by the localization engine 100 at a frequency of at least
   100 Hz, more preferably at least 300 Hz, and most preferably at least 500 Hz.
                                                20

WO 2014/052428                                                             PCT/US2013/061642
   [0096]             Referring to Figure 5, the LED cloud may also include virtual
   LEDs, which are predetermined points identified on the model, but that do not
   actually correspond to physical LEDs on the tracker 44. The positions of these points
   may also be calculated at times tl, t2...tn. These virtual LEDs can be calculated in the
   same fashion as the unmeasured LEDs with reference to FIG. 4. The only difference
   is that the virtual LEDs are never fired or included in the sequence of optical
   measurements, since they do not correspond to any light source, but are merely virtual
   in nature.   Steps 300-322 show the steps used for tracking the trackers 44, 46, 48
  using real and virtual LEDS. Steps 300-322 generally correspond to steps 200-222
   except for the addition of the virtual LEDs, which are treated like unmeasured LEDs
  using the same equations described above.
   [0097]             One purpose of using virtual LEDs in addition to the LEDs 50a,
   50b, 50c, for example, is to reduce the effect of errors in the velocity calculations
   described above.      These errors may have little consequence on the calculated
  positions of the LEDs 50a, 50b, 50c, but can be amplified the further away from the
   LEDs 50a, 50b, 50c a point of interest is located. For instance, when tracking the
   femur F with tracker 44, the LEDs 50a, 50b, 50c incorporated in the tracker 44 may
   experience slight errors in their calculated positions of about 0.2 millimeters.
   However, consider the surface of the femur F that may be located over 10 centimeters
   away from the LEDs 50a, 50b, 50c. The slight error of 0.2 millimeters at the LEDs
   50a, 50b, 50c can result in 0.4 to 2 millimeters of error on the surface of the femur F.
   The further away the femur F is located from the LEDs 50a, 50b, 50c the more the
   error increases.   The use of virtual LEDs in the steps of Figure 5 can reduce the
  potential amplification of such errors as described below.
   [0098]               Referring to Figure 5A, one virtual LED 50d can be positioned on
   the surface of the femur F. Other virtual LEDs 50e, 50f, 50g, 50h, 50i, 50j, 50k can
  be positioned at random locations in the bone tracker coordinate system BTRK1 such
   as along each of the x-, y-, and z- axes, and on both sides of the origin along these
   axes to yield 6 virtual LEDs. These virtual LEDs are included as part of the model of
   the tracker 44 shown in Figure 5A and used in steps 302 and 320.                In some
   embodiments, only the virtual LEDs 50e-50k are used. In other embodiments, virtual
   LEDs may be positioned at locations along each of the x-, y-, and z- axes, but at
   different distances from the origin of the bone tracker coordinate system BTRK1. In
                                               21

WO 2014/052428                                                              PCT/US2013/061642
   still further embodiments, some or all of the virtual LEDs may be located off of the
   axes x-, y-, and z-.
   [0099]             Now in the model are real LEDs 50a, 50b, 50c and virtual LEDs
   50d-50k. At each time tI, t2...tn this extended model is matched in step 320 with the
   measured/calculated positions of real LEDs 50a, 50b, 50c and with the calculated
  positions of virtual LEDs 50d-50k to obtain the transformation matrix that links the
  bone tracker coordinate system BTRK1 with the localizer coordinate system LCLZ.
   Now, with the virtual LEDs 50d-50k included in the model, which are located at
  positions outlying the real LEDs 50a, 50b, 50c, the error in the rotation matrix can be
  reduced. In essence, the rigid body matching algorithm or point matching algorithm
  has additional points used for matching and some of these additional points are
   located radially outwardly from the points defining the real LEDs 50a, 50b, 50c, thus
  rotationally stabilizing the match.
   [00100]            In another variation of the process of Figure 5, the locations of the
   virtual LEDs 50e-50k can be changed dynamically              during use depending on
   movement of the tracker 44. The calculated positions of the unmeasured real LEDs
   50b, 50c and the virtual LEDs 50e-50k at time tI are more accurate the slower the
   tracker 44 moves. Thus, the locations of virtual LEDs 50e-50k along the x-, y-, and z
   axes relative to the origin of the bone tracker coordinate system BTRK1 can be
   adjusted based on speed of the tracker 44. Thus, if the locations of the virtual LEDs
   50e-50k are denoted (s,0,0), (-s,0,0), (0,s,0), (0,-s,0), (0,0,s), (0,0,-s), respectively,
   then s would increase when the tracker 44 moves slowly and s would decrease to a
   smaller value when the tracker 44 moves faster.          This could be handled by an
   empirical formula for s or s can be adjusted based on an estimate in the error in
   velocity and calculated positions.
   [00101]            Determining new positions of the LEDs 50 (real and virtual) can be
   carried out at a frequency of at least 100 Hz, more preferably at least 300 Hz, and
   most preferably at least 500 Hz.
   [00102]            Data from the accelerometers 70 can be used in situations where
   optical measurement of an LED 50 is impeded due to interference with the line-of
   sight. When an LED to be measured is blocked, the localization engine 100 assumes
   a constant velocity of the origin to estimate positions. However, the constant velocity
   assumption in this situation may be inaccurate            and result in errors.       The
                                              22

WO 2014/052428                                                            PCT/US2013/061642
   accelerometers 70 essentially monitor if the constant velocity assumption in the time
  period is accurate. The steps shown in Figures 6 and 7 illustrate how this assumption
   is checked.
   [00103]            Continuing to use tracker 44 as an example, steps 400-422 of Figure
   6 generally correspond to steps 300-322 from Figure 5. However, in step 424, the
   system 20 determines whether, in the last cycle of measurements, less than 3 LEDs
   were measured - meaning that one or more of the LEDs in the cycle could not be
   measured. This could be caused by line-of-sight issues, etc. A cycle for tracker 44 is
   the last three attempted measurements. If during the last three measurements, each of
   the LEDs 50a, 50b, 50c were visible and could be measured, then the system 20
  proceeds to step 408 and continues as previously described with respect to Figure 5.
   [00104]            If the system 20 determines that one or more of the LEDs 50a, 50b,
   50c could not be measured during the cycle, i.e., were blocked from measurement,
   then the algorithm still moves to step 408, but if the new LED to be measured in step
   406 was the one that could not be measured, the system makes some velocity
   assumptions as described below.
   [00105]            When a LED, such as LED 50a, is not seen by the optical sensor 40
   at its measurement time tn in step 406, the previously calculated velocity vector
   v (ORIGIN) of the origin of the tracker 44 in the localizer coordinate system LCLZ
   at the previous time t(n-1) is assumed to remain constant. Accordingly, velocity
   vectors of LEDs 50a, 50b, 50c in the localizer coordinate system LCLZ can be
   calculated based on the previously calculated velocity vector F (ORIGIN) in the
   localizer coordinate system LCLZ and the relative velocity vectors of LEDs 50a, 50b,
   50c, which are derived from a newly measured angular velocity vector from the
   gyroscope 60.      The equations described in steps 316-322 can then be used to
   determine new positions of the LEDs 50a, 50b, 50c.
   [00106]            To start, when LED 50a is to be measured at step 406, but is
   obstructed, the velocity vector of the origin is assumed to be the same as the previous
   calculation. Accordingly, the velocity of the new LED is not calculated at step 408:
                             - (ORIGIN) = previous calculation
   [00107]            Step 410 proceeds the same as step 310.
                                              23

WO 2014/052428                                                             PCT/US2013/061642
   [00108]            The relative velocity vectors VR of LEDs 50a, 50b, 50c calculated
   in step 412 are then based on the previous velocity vector V (ORIGIN) and the newly
   measured angular velocity vector from the gyroscope 60 in the bone tracker
   coordinate system BTRK1:
                  -vR(LED50a) = -W(current) x - (ORIGIN to LED50a)
                  -vR(LED50b) = -W(current) x - (ORIGIN to LED50b)
                 WR (LED50c) = V (current) x - (ORIGIN to LED50c)
   [00109]            In step 414, velocity vectors in the localizer coordinate system
   LCLZ can the be calculated using the origin velocity vector V (ORIGIN) and the
  relative velocity vectors vR of LEDs 50a, 50b, 50c:
                        - (LED50a) =-      (ORIGIN) + vR (LED50a)
                        - (LED50b) = - (ORIGIN) + WR (LED50b)
                        - (LED50c) = - (ORIGIN) + WR (LED50c)
   [00110]            Steps 416 through 422 proceed the same as steps 316-322.
   [00111]            If the system 20 determines at step 424 that one or more of the
   LEDs 50a, 50b, 50c could not be measured during the cycle, i.e., were blocked from
   measurement, another algorithm is carried out simultaneously at steps 500-506 shown
   in Figure 7 until a complete cycle of measurements is made where all of the LEDs
   50a, 50b, 50c in the cycle were visible to the optical sensor 40. Thus, the system 20 is
   considered to be in a "blocked" condition until the complete cycle with all visible
   measurements is made.
   [00112]             Steps 500-506 are carried out continuously while the system 20 is
   in the blocked condition.
   [00113]            In step 500 the navigation processor 52 starts a clock that tracks
  how long the system 20 is in the blocked condition.            The time in the blocked
   condition is referred to below as t (blocked).
   [00114]            In step 502, the accelerometer 70 measures accelerations along the
   x-, y-, and z- axes of the bone tracker coordinate system BTRK1 to track errors in the
   constant velocity assumption.     Accelerometer readings, like gyroscope readings are
                                              24

WO 2014/052428                                                                PCT/US2013/061642
   transformed from the accelerometer coordinate system to the bone tracker coordinate
   system BTRKI.
   [00115]            If the   accelerometer 70        detects  acceleration(s) that exceed
  predefined acceleration tolerance(s), the navigation computer 26 will put the system
   20 into an error condition. The acceleration tolerances could be defined differently
   along each x-, y-, and z- axis, or could be the same along each axis. If a measured
   acceleration exceeds a tolerance then the constant velocity assumption is unreliable
   and cannot be used for that particular application of surgical navigation. Different
   tolerances may be employed for different applications. For instance, during robotic
   cutting, the tolerance may be very low, but for visual navigation only, i.e., not
   feedback for cutting control loop, the tolerance may be set higher.
   [00116]            In step 504, velocity errors associated with the positions of the
   LEDs 50 relative to the optical sensor 40 are taken into account and monitored during
   the blocked condition. For each of the LEDs 50, the velocity error Verror multiplied
  by the time in the blocked condition       tblocked condition must be less than a position
   error tolerance y and thus must satisfy the following equation to prevent the system
   20 from being put into an error condition:
                                Error X blocked condition      < Y
   [00117]            In this equation, the velocity error error is calculated for each of
   the LEDs 50a, 50b, 50c as follows:
                                          Xerror(t) + Xerror(t-1)
                               Error -
                                                      At
   [00118]             Position errors Xerror(t) and xerror(t-1) are predefined position
   errors in the system 20 that are based on location relative to the optical sensors 40 at
   times t and t-1. In essence, the further away the LEDs 50a, 50b, 50c are located from
   the optical sensors 40, the higher the potential position errors. These positions errors
   are derived either experimentally or theoretically and placed in a look-up table or
   formula so that at each position of the LEDs 50a, 50b, 50c in Cartesian coordinates (x,
   y, z) an associated position error is provided.
                                                25

WO 2014/052428                                                               PCT/US2013/061642
   [00119]            In step 504, the localization engine 100 accesses this look-up table
   or calculates this formula to determine the position errors for each of LEDs 50a, 50b,
   50c at the current time t and at the previous time t-1. The position errors are thus
  based on the positions in Cartesian coordinates in the localizer coordinate system
   LCLZ calculated by the system 20 in step 422 for the current time t and at the
  previous time t-1. The time variable At represents the time it takes for subsequent
  position calculations, so the difference between t and t-1, which for illustrative
  purposes may be 1 millisecond.
   [00120]            The position error tolerance y is predefined in the navigation
   computer 26 for access by the localization engine 100. The position error tolerance y
   could be expressed in millimeters.        The position error tolerance y can range from
   0.001 to 1 millimeters and in some embodiments is specifically set at 0.5 millimeters.
   Thus, if the position error tolerance y is set to 0.5 millimeters, the following equation
   must be satisfied:
                        Error   X  tblocked condition < O.S millimeters
   [00121]            As can be seen, the longer the system 20 is in the blocked condition,
   the larger the effect that the time variable has in this equation and thus the smaller the
   velocity errors that will be tolerated.         In some embodiments, this equation is
   calculated by the localization engine 100 in step 504 separately for each of the LEDs
   50a, 50b, 50c. In other embodiments, because of how closely arranged the LEDs 50a,
   50b, 50c are on the tracker 44, the velocity error of only one of the LEDs 50a, 50b,
   50c is used in this calculation to determine compliance.
   [00122]            In step 506, when the error(s) exceeds the position error tolerance y,
   the system 20 is placed in an error condition. In such a condition, for example, any
   control or movement of cutting or ablation tools is ceased and the tools are shut down.
   IV.      OTHER EMBODIMENTS
   [00123]            In one embodiment, when each of the trackers 44, 46, 48 are being
   actively tracked, the firing of the LEDs occurs such that one LED from tracker 44 is
   fired, then one LED from tracker 46, then one LED from tracker 48, then a second
   LED from tracker 44, then a second LED from tracker 46, and so on until all LEDs
                                                26

WO 2014/052428                                                               PCT/US2013/061642
  have been fired and then the sequence repeats. This order of firing may occur through
   instruction signals sent from the transceivers (not shown) on the camera unit 36 to
   transceivers (not shown) on the trackers 44, 46, 48.
   [00124]            The navigation system 20 can be used in a closed loop manner to
   control surgical procedures carried out by surgical cutting instruments.          Both the
   instrument 22 and the anatomy being cut are outfitted with trackers 50 such that the
   navigation system 20 can track the position and orientation of the instrument 22 and the
   anatomy being cut, such as bone.
   [00125]            In one embodiment, the navigation system is part of a robotic
   surgical system for treating tissue. In some versions, the robotic surgical system is a
  robotic surgical cutting system for cutting away material from a patient's anatomy, such
   as bone or soft tissue. The cutting system could be used to prepare bone for surgical
   implants such as hip and knee implants, including unicompartmental, bicompartmental,
   or total knee implants.     Some of these types of implants are shown in U.S. Patent
   Application No. 13/530,927, entitled, "Prosthetic Implant and Method of Implantation",
   the disclosure of which is hereby incorporated by reference.
   [00126]            The robotic surgical cutting system includes a manipulator (see, for
   instance, Figure 1). The manipulator has a plurality of arms and a cutting tool carried by
   at least one of said plurality of arms. A robotic control system controls or constrains
   movement of the cutting tool in at least 5 degrees of freedom. An example of such a
   manipulator and control system are shown in U.S. Provisional Patent Application No.
   61/679,258, entitled, "Surgical Manipulator Capable of Controlling a Surgical
   Instrument in either a Semi-Autonomous Mode or a Manual, Boundary Constrained
   Mode", hereby incorporated by reference, and also in U.S. Patent Application No.
   13/958,834, entitled, "Navigation System for use with a Surgical Manipulator
   Operable in Manual or Semi-Autonomous Mode", the disclosure of which is hereby
   incorporated by reference.
   [00127]            In this embodiment, the navigation system 20 communicates with the
  robotic control system (which can include the manipulator controller 54).               The
   navigation system 20 communicates position and/or orientation data to said robotic
   control system. The position and/or orientation data is indicative of a position and/or
   orientation of instrument 22 relative to the anatomy. This communication provides
   closed loop control to control cutting of the anatomy such that the cutting occurs within a
                                                27

WO 2014/052428                                                              PCT/US2013/061642
  predefined boundary.
   [00128]              In this embodiment, manipulator movement may coincide with LED
   measurements such that for each LED measurement taken, there is a corresponding
   movement of the instrument 22 by the manipulator 56. However, this may not always
  be the case. For instance, there may be such a lag between the last LED measurement
   and movement by the manipulator 56 that the position and/or orientation data sent from
   the navigation computer 26 to the manipulator 56 for purposes of control loop movement
  becomes unreliable. In such a case, the navigation computer 26 can be configured to
   also transmit to the manipulator controller 54 kinematic data.       Such kinematic data
   includes the previously determined linear and angular velocities for the trackers 44, 46,
   48. Since the velocities are already known, positions can calculated based on the lag of
   time. The manipulator controller 54 could then calculate, for purposes of controlling
   movement of the manipulator 56, the positions and orientations of the trackers 44, 46, 48
   and thus, the relative positions and orientations of the instrument 22 (or instrument tip)
   to the femur F and/or tibia T.
   [00129]             In this embodiment, the instrument 22 is held by the manipulator
   shown in Figure 1 or other robot that provides some form of mechanical constraint to
   movement.      This constraint limits the movement of the instrument 22 to within a
  predefined boundary.        If the instrument 22 strays beyond the predefined boundary, a
   control is sent to the instrument 22 to stop cutting.
   [00130]             When tracking both the instrument 22 and the anatomy being cut in
  real time in these systems, the need to rigidly fix anatomy in position can be eliminated.
   Since both the instrument 22 and anatomy are tracked, control of the instrument 22 can
  be adjusted based on relative position and/or orientation of the instrument 22 to the
   anatomy. Also, representations of the instrument 22 and anatomy on the display can
   move relative to one another - to emulate their real world motion.
   [00131]             In one embodiment, each of the femur F and tibia T has a target
   volume of material that is to be removed by the working end of the surgical instrument
   22. The target volumes are defined by one or more boundaries. The boundaries define
   the surfaces of the bone that should remain after the procedure. In some embodiments,
   system 20 tracks and controls the surgical instrument 22 to ensure that working end, e.g.,
  bur, only removes the target volume of material and does not extend beyond the
  boundary, as disclosed in Provisional Patent Application No. 61/679,258, entitled,
                                                 28

WO 2014/052428                                                                 PCT/US2013/061642
   "Surgical Manipulator Capable of Controlling a Surgical Instrument in either a Semi
   Autonomous Mode or a Manual, Boundary Constrained Mode", hereby incorporated
  by reference.
   [00132]             In the described embodiment, control of the instrument 22 is
   accomplished by utilizing the data generated by the coordinate transformer 102 that
   indicates the position and orientation of the bur or other cutting tool relative to the target
   volume.     By knowing these relative positions, the surgical instrument 22 or the
   manipulator to which it is mounted, can be controlled so that only desired material is
  removed.
   [00133]             In other systems, the instrument 22 has a cutting tool that is movable
   in three degrees of freedom relative to a handheld housing and is manually positioned by
   the hand of the surgeon, without the aid of cutting jig, guide arm or other constraining
   mechanism.      Such systems are shown in U.S. Provisional Patent Application No.
   61/662,070, entitled, "Surgical Instrument Including Housing, a Cutting Accessory that
   Extends from the Housing and Actuators that Establish the Position of the Cutting
   Accessory Relative to the Housing", the disclosure of which is hereby incorporated by
  reference.
   [00134]             In these embodiments, the system includes a hand held surgical
   cutting instrument having a cutting tool. A control system controls movement of the
   cutting tool in at least 3 degrees of freedom using internal actuators/motors, as shown in
   U.S. Provisional Patent Application No. 61/662,070, entitled, "Surgical Instrument
   Including Housing, a Cutting Accessory that Extends from the Housing and Actuators
   that Establish the Position of the Cutting Accessory Relative to the Housing", the
   disclosure of which is hereby incorporated by reference.         The navigation system 20
   communicates with the control system. One tracker (such as tracker 48) is mounted to
   the instrument.     Other trackers (such as trackers 44, 46) are mounted to a patient's
   anatomy.
   [00135]             In this embodiment, the navigation system 20 communicates with the
   control system of the hand held surgical cutting instrument. The navigation system 20
   communicates position and/or orientation data to the control system. The position
   and/or orientation data is indicative of a position and/or orientation of the instrument 22
  relative to the anatomy. This communication provides closed loop control to control
   cutting of the anatomy such that the cutting occurs within a predefined boundary (the
                                                29

WO 2014/052428                                                              PCT/US2013/061642
   term predefined boundary is understood to include predefined trajectory, volume, line,
   other shapes or geometric forms, and the like).
   [00136]            Features of the invention may be used to track sudden or unexpected
   movements of the localizer coordinate system LCLZ, as may occur when the camera
  unit 36 is bumped by surgical personnel. An accelerometer (not shown) mounted to
   camera unit 36 monitors bumps and stops system 20 if a bump is detected. In this
   embodiment, the accelerometer communicates with the camera controller 42 and if
   the measured acceleration along any of the x, y, or z axes exceeds a predetermined
   value, then the camera controller 42 sends a corresponding signal to the navigation
   computer 26 to disable the system 20 and await the camera unit 36 to stabilize and
  resume measurements.         In some cases, the initialization step 200, 300, 400 would
  have to be repeated before resuming navigation.
   [00137]            In some embodiments, a virtual LED is positioned at the working tip
   of the instrument 22. In this embodiment, the virtual LED is located at the location of
   the working tip in the model of the instrument tracker 48 so that the working tip
   location is continuously calculated.
   [00138]            It is an object of the intended claims to cover all such modifications
   and variations that come within the true spirit and scope of this invention.
   Furthermore, the embodiments described above are related to medical applications,
  but the inventions described herein are also applicable to other applications such as
   industrial, aerospace, defense, and the like.
                                                30

IH:
  saf\Interwoven\NRPortbl\DCC\SAF\ 5985712 _ docx-20 I 2017
CLAIMS
 1.             A navigation system comprising:
                a localizer;
                a tracker for communicating with said localizer and including three markers and a non
                optical sensor; and
                a computing system having at least one processor configured to:
                                              determine positions of each of said markers based on optical and non
                               optical signals,
                                              determine if one or more of said markers is blocked from line-of-sight
                               with said localizer to create a blocked condition,
                                              track a time of the blocked condition,
                                              determine a position error tolerance associated with the one or more
                               blocked markers,
                                              determine a position error associated with the one or more blocked
                               markers based on the time of the blocked condition,
                                              determine if the position error is within the position error tolerance, and
                                             place the navigation system in an error condition if the position error
                               exceeds the position error tolerance.
2.              The navigation system of claim 1, comprising a second non-optical sensor in
communication with said at least one processor to measure acceleration of said tracker during
the blocked condition.
3.              The navigation system of claim 2, wherein said at least one processor is configured to
place the navigation system in the error condition if the acceleration measured by said second
non-optical sensor exceeds a predefined acceleration tolerance.
4.              The navigation system of claim 1, wherein said at least one processor is configured to
determine a velocity error associated with the one or more blocked markers during the blocked
condition.
                                                                     31

I: \saf\Interwovn\NRPortbl\DCC\SAF\ 5985712 _ docx-20 It 2017
 5.               The navigation system of claim 4, wherein the velocity error varies with distance of the
 one or more blocked markers from said localizer.
 6.               The navigation system of claim 4, wherein said at least one processor is configured to
 calculate the position error as a product of the velocity error and the time of the blocked
 condition, wherein the position error varies as a function of the velocity error and the time of
the blocked condition.
 7.               The navigation system of claim 1, wherein the position error tolerance ranges from
 0.001 to 1.0 millimeters.
 8.               A method of tracking an object using a navigation system comprising a localizer, a
tracker including three markers and a non-optical sensor, and a computing system having at
 least one processor, said method comprising the steps of:
                  determining positions of each of the markers based on optical and non-optical signals;
                  determining if one or more of the markers is blocked from line-of-sight with the
 localizer to create a blocked condition;
                  tracking a time of the blocked condition;
                  determining a position error tolerance associated with the one or more blocked
 markers;
                  determining a position error associated with the one or more blocked markers based on
 the time of the blocked condition;
                  determining if the position error is within the position error tolerance; and
                  placing the navigation system in an error condition if the position error exceeds the
position error tolerance.
 9.               The method of claim 8, comprising measuring acceleration of the tracker during the
blocked condition.
 10.              The method of claim 9, comprising placing the navigation system in the error condition
 if the measured acceleration exceeds a predefined acceleration tolerance.
                                                              32

I: \saf\Interwovn\NRPortbl\DCC\SAF\ 5985712 _ docx-20 It 2017
 11.              The method of claim 8, wherein determining the position error comprises determining
 a velocity error associated with the one or more blocked markers during the blocked condition.
 12.              The method of claim 11, wherein the velocity error varies with distance of the one or
more blocked markers from the localizer.
 13.              The method of claim 11, wherein determining the position error comprises calculating
the position error as a product of the velocity error and the time of the blocked condition,
wherein the position error varies as a function of the velocity error and the time of the blocked
 condition.
 14.              The method of claim 8, wherein the position error tolerance ranges from 0.001 to 1.0
millimeters.
 15.              A navigation system comprising:
                  a localizer;
                  a tracker for communicating with said localizer and including three markers and a non
 optical sensor; and
                  a computing system having at least one processor configured to:
                                       determine positions of each of said markers in a first coordinate system
based on optical and non-optical signals,
                                       determine a position of a virtual point in said first coordinate system, and
                                       match said positions of said markers and said virtual point in said first
 coordinate system with positions of said markers and said virtual point in a model of said
tracker established relative to a second coordinate system to obtain a transformation matrix to
transform said second coordinate system to said first coordinate system.
 16.              The navigation system of claim 15, wherein said at least one processor is configured to
 determine positions of a plurality of said virtual points and match said positions of said
markers and said plurality of said virtual points in said model of said tracker to obtain said
transformation matrix.
 17.              The navigation system of claim 16, wherein said plurality of said virtual points
 comprises a first set of virtual points positioned along each of x, y, and z axes of said second
 coordinate system.
                                                                 33

I: \saf\Interwovn\NRPortbl\DCC\SAF\ 5985712 _ docx-20 I 2017
 18.              The navigation system of claim 14, wherein said plurality of said virtual points
 comprises a second set of virtual points positioned along each of said x, y, and z axes of said
 second coordinate system wherein said second set of virtual points are positioned along said x,
y, and z axes on an opposite side of an origin of said second coordinate system from said first
 set of virtual points.
 19.              The navigation system of claim 16, wherein said plurality of said virtual points are
 located radially outwardly from said markers in said model of said tracker.
20.               The navigation system of claim 15, wherein said at least one processor is configured to
match said positions of said markers and said virtual point in said first coordinate system with
positions of said markers and said virtual point in a model of said tracker established relative
to said second coordinate system using a rigid body matching algorithm or point matching
 algorithm.
21.               The navigation system of claim 15, wherein said at least one processor is configured to
 change said position of said virtual point in said model based on movement of said tracker.
22.               The navigation system of claim 15, wherein said at least one processor is configured to
 determine said positions of said markers and said virtual point at a frequency of at least 500
Hz.
23.               The navigation system of claim 15, wherein said first coordinate system is further
 defined as a coordinate system of said localizer and said second coordinate system is further
 defined as a coordinate system of said tracker.
24.               The navigation system of claim 15, wherein said at least one processor is configured to
 determine a position of a first of said markers at a first time based on a first optical signal from
 said first marker.
25.               The navigation system of claim 24, wherein said at least one processor is configured to
 determine positions of a second and third of said markers and said virtual point at said first
time based on said first optical signal and a non-optical signal from said non-optical sensor.
                                                             34

I: \saf\Interwovn\NRPortbl\DCC\SAF\ 5985712 _ docx-20 I 2017
26.               A method of tracking an object using a navigation system comprising a localizer, a
tracker including three markers and a non-optical sensor, and a computing system having at
 least one processor, said method comprising the steps of:
                  determining, with the at least one processor, positions of each of the markers in a first
 coordinate system based on optical and non-optical signals;
                  determining, with the at least one processor, a position of a virtual point in the first
 coordinate system; and
                  matching the positions of the markers and the virtual point in the first coordinate
 system with positions of the markers and the virtual point in a model of the tracker established
relative to a second coordinate system to obtain a transformation matrix to transform the
 second coordinate system to the first coordinate system.
27.               The method of claim 26, including determining positions of a plurality of virtual points
 and matching the positions of the markers and the plurality of virtual points in the model of
the tracker to obtain the transformation matrix.
28.               The method of claim 27, including positioning the plurality of virtual points along each
 of x, y, and z axes of the second coordinate system.
29.               The method of claim 28, including positioning the plurality of virtual points radially
 outwardly from the markers in the model of the tracker.
 30.              The method claim 26, including changing the position of the virtual point in the model
based on movement of the tracker.
 31.              The method of claim 26, including determining a position of a first of the markers at a
 first time based on a first optical signal from the first marker.
 32.              The method of claim 31, including determining positions of a second and third of the
markers and the virtual point at the first time based on the first optical signal and a non-optical
 signal from the non-optical sensor.
                                                             35

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
