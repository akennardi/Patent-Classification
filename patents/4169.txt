     ABSTRACT
          An adaptive offset filter (60) adds an offset to the pixel value of each pixel forming an input
  image.    The adaptive offset filter (60) refers to offset-type specifying information, sets offset attributes
  for a subject unit area of the input image, decodes an offset having a bit width corresponding to an
5 offset value range included in the set offset attributes, and adds the offset to the pixel value of each
  pixel forming the input image.

          OFFSET DECODING DEVICE, OFFSET CODING DEVICE, IMAGE FILTERING DEVICE, AND DATA
                                                            STRUCTURE
   [0001]
 5            The present application is a divisional application from Australian Patent Application No. <removed-apn>, the
   entire disclosure of which is incorporated herein by reference.
   Technical Field
   [0002]
              The present invention relates to an image filtering device which performs filtering on images.     The invention
10 also relates to an offset decoding device which decodes offsets referred to by an image filter, and an offset coding device
   which codes offsets referred to by an image filter.    The invention also relates to a data structure of coded data.
   Background Art
   [0003]
              A video coding device (coding device) which generates coded data by coding video images in order to transmit
15 or record video images efficiently, and a video decoding device (decoding device) which generates decoded images by
   decoding the coded data are being used.      Specific examples of video coding methods are a method defined in H.
   264/MPEG-4. AVC, a method used in KTA software, which is a joint development codec in VCEG (Video Coding
   Expert Group), a method used in TMuC (Test Model under Consideration) software, which is a successor codec to the
   codec used in KTA software, and a method used in HM (HEVC TestModel) software.
20 [0004]
              In such coding methods, images (pictures) forming video images are managed in a hierarchical structure which
   is constituted by slices obtained by dividing an image, largest coding units (LCU: Largest Coding Unit, also called a tree
   block) obtained by dividing a slice, coding units (CU: Coding Unit, also called a coding node) obtained by dividing a
   largest coding unit, and blocks and partitions obtained by dividing a coding unit.     In many cases, images are coded by
25 using blocks as the smallest coding unit.
                                                                  1

   [0005]
              Additionally, in such coding methods, normally, a prediction image is generated on the basis of a locally
   decoded image obtained by coding and decoding an input image, and difference data indicating a difference between the
   prediction image and the input image is coded.         As a generation method for prediction images, inter-frame prediction
 5 (inter prediction) and intra-frame prediction (intra prediction) are known.
   [0006]
               In intra prediction, on the basis of a locally decoded image within the same frame, prediction images in this
   frame are sequentially generated.       More specifically, in intra prediction, normally, for each unit of prediction (for
   example, a block), one of prediction directions (prediction modes) included in a predetermined prediction direction
10 group is selected, and also, the pixel value of a reference pixel in a locally decoded image is extrapolated to the selected
   prediction direction, thereby generating a prediction pixel value in a subject area to be predicted.      On the other hand, in
   inter prediction, by applying motion compensation using motion vectors to a reference image within an entirely decoded
   reference frame (decoded image), a prediction image within a frame to be predicted is generated for each unit of
   prediction (for example, a block).
15 [0007]
              NPL 1 and NPL 2 disclose an adaptive offset filter disposed at a stage subsequent to a deblocking filter which
   reduces block distortion of a decoded image and at a stage prior to an adaptive loop filter (also referred to as an
   "adaptive filer") which performs filtering processing using an adaptively determined filter coefficient.        This adaptive
   offset filter adds an adaptively set offset to the pixel value of each pixel of an image output from the deblocking filter.
20 [0008]
               By providing such an adaptive offset filter, it is possible to suppress block distortion more effectively.
   Citation List
   Non Patent Literature
   [0009]
25            NPL 1:      "JCTVC-D122", Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and
                                                                     2

   ISO/IEC JTC1/SC29/WG11, 4th Meeting: Daegu, KR, 01/2011
            NPL 2:       "JCTVC-E049", Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and
   ISO/IEC JTC1/SC29/WG11, 5th Meeting: Geneva, CH, 03/2011
   [0010]
 5          A reference herein to a patent document or any other matter identified as prior art, is not to be taken as an
   admission that the document or other matter was known or that the information it contains was part of the common
   general knowledge as at the priority date of any of the claims.
   Summary of Invention
   [0011]
10          According to an aspect of the present invention, there is provided, An image filtering device for receiving a
   deblocked decoded image and adding an offset to a pixel value of each pixel of a unit area of the deblocked decoded
   image which is constituted by a plurality of unit areas, comprising:
             an offset attribute setting unit configured to set an offset bit depth and an offset value range by referring to
   coded data and a shift value, wherein a maximum bit length representing the offset value range is equal to the offset bit
15 depth-K, and the value of K is equal to 4;
             an offset decoding unit configured to decode an offset which is restricted to the offset value range;
             a bit shift unit configured to perform bitwise left shift on the decoded offset by an amount equal to the shift
   value; and
             a filtering unit configured to add the left-shifted offset to the pixel value of each pixel of a unit area of the
20 deblocked decoded image
   [0012]
            Accordingly, with the above-described configuration, it is possible to perform appropriate offset filtering
   processing while the memory size of a memory for storing offsets is reduced.
   [0013]
25          An offset decoding device according to the present invention is an offset decoding device for decoding each
                                                                   3

   offset which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.
   The offset decoding device includes: offset residual decoding means for decoding each offset residual from coded data;
   prediction value determining means for determining a prediction value of each offset from a decoded offset; and offset
   calculating means for calculating each offset from a prediction value determined by the prediction value determining
 5 means and an offset residual decoded by the offset residual decoding means.
   [0014]
              In the offset decoding device configured as described above, there are provided the offset residual decoding
   means for decoding each offset residual from coded data, the prediction value determining means for determining a
   prediction value of each offset from a decoded offset, and the offset calculating means for calculating each offset from a
10 prediction value determined by the prediction value determining means and an offset residual decoded by the offset
   residual decoding means.       Accordingly, an offset can be appropriately decoded from coded data having a smaller
   amount of data, compared with a case in which each offset itself is coded.
   [0015]
             An image filtering device according to the present invention is an image filtering device which operates on an
15 input image.     The image filtering device includes: calculating means for calculating a difference value between a pixel
   value of a subject pixel forming an input image and a pixel value of a pixel around the subject pixel; bit shift means for
   performing bitwise right shift on a pixel value referred to by the calculating means or the difference value calculated by
   the calculating means by an amount equal to a predetermined shift value; classifying means for classifying the subject
   pixel as one of a plurality of offset classes in accordance with a magnitude relation between the difference value
20 subjected to bitwise right shift by the bit shift means and 0; and offset means for adding an offset associated with the
   offset class of the subject pixel classified by the classifying means to the pixel value of the subject pixel.
   [0016]
              In the image filtering device configured as described above, the subject pixel is classified as one of a plurality
   of offset classes in accordance with a magnitude relation between the difference value subjected to bitwise right shift by
25 the bit shift means and 0, and an offset associated with the offset class of the subject pixel classified by the classifying
                                                                   4

   means is added to the pixel value of the subject pixel.     Thus, the classifying processing is less vulnerable to the
   influence of noise, thereby making it possible to improve the coding efficiency.
   [0017]
              An image filtering device according to the present invention is an image filtering device which operates on an
 5 input image.     The image filtering device includes: calculating means for calculating a difference value between a pixel
   value of a subject pixel forming an input image and a pixel value of a pixel around the subject pixel; classifying means
   for classifying the subject pixel as one of a plurality of offset classes in accordance with a magnitude relation between
   the difference value calculated by the calculating means and each of predetermined first and second thresholds; and
   offset means for adding an offset associated with the offset class of the subject pixel classified by the classifying means
10 to the pixel value of the subject pixel.
   [0018]
              In the image filtering device configured as described above, the subject pixel is classified as one of a plurality
   of offset classes in accordance with a magnitude relation between the difference value calculated by the calculating
   means and each of the predetermined first and second thresholds, and an offset associated with the offset class of the
15 subject pixel classified by the classifying means is added to the pixel value of the subject pixel.     Thus, the classifying
   processing is less vulnerable to the influence of noise, thereby making it possible to improve the coding efficiency.
   [0019]
              An image filtering device according to the present invention is an image filtering device which operates on an
   input image constituted by a plurality of unit areas.     The image filtering device includes: determining means for
20 determining, among first and second offset types, an offset type to which a subject unit area including a subject pixel
   forming the input image belongs; classifying means for classifying the subject pixel as one of an offset class in which an
   offset is not added and a plurality of offset classes in which an offset is added in accordance with the offset type to which
   the subject unit area belongs and a pixel value of the subject pixel; and offset means for adding an offset associated with
   the offset type to which the subject unit area belongs and the offset class of the subject pixel classified by the classifying
25 means to the pixel value of the subject pixel.     In a case in which the pixel value of the subject pixel is within a
                                                                   5

   predetermined range, the classifying means classifies the subject pixel as an offset class in which an offset is added,
   regardless of whether the offset type to which the unit area including the subject pixel belongs is the first offset type or
   the second offset type.
   [0020]
 5            In the image filtering device configured as described above, in a case in which the pixel value of the subject
   pixel is within a predetermined range, the subject pixel is classified as an offset class in which an offset is added,
   regardless of whether the offset type to which the unit area including the subject pixel belongs is the first offset type or
   the second offset type, thereby making it possible to effectively eliminate block noise.     Accordingly, with the above
   described configuration, the coding efficiency can be improved.
10 [0021]
             An image filtering device according to the present invention is an image filtering device for adding an offset to
   a pixel value of each pixel forming an input image which is constituted by a plurality of unit areas.     The image filtering
   device includes: determining means for determining an offset type to which a subject unit area belongs among a plurality
   of offset types; offset coding means for determining an offset having a bit width which differs depending on the offset
15 type and for coding the offset; and filtering means for adding the determined offset to the pixel value of each pixel
   forming the input image.
   [0022]
              In the image filtering device configured as described above, among a plurality of offset types, the offset type to
   which a subject unit area belongs is determined, an offset having a bit width which differs depending on the determined
20 offset type is determined, and the determined offset is added to the pixel value of each pixel forming the input image.
   The determined offset is also coded.
   [0023]
             Accordingly, with the above-described configuration, it is possible to perform appropriate offset filtering
   processing while the memory size of a memory for storing offsets is reduced.        With the above-described configuration,
25 since the amount of data required to code data is reduced, the coding efficiency is improved.
                                                                 6

   [0024]
             An offset coding device according to the present invention is an offset coding device for coding each offset
   which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.    The
   offset coding device includes: prediction value determining means for determining a prediction value of each offset from
 5 a coded offset; offset residual calculating means for calculating an offset residual from each offset and a prediction value
   determined by the prediction value determining means; and offset residual coding means for coding an offset residual
   calculated by the offset residual calculating means.
   [0025]
              In the offset coding device configured as described above, there are provided the prediction value determining
10 means for determining a prediction value of each offset from a coded offset, the offset residual calculating means for
   calculating an offset residual from each offset and a prediction value determined by the prediction value determining
   means, and the offset residual coding means for coding an offset residual calculated by the offset residual calculating
   means.     It is thus possible to reduce the amount of data required to code data can be reduced.
   [0026]
15           A data structure of coded data according to the present invention is a data structure of coded data which is
   referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas.    The data structure includes: offset-type specifying information which specifies
   an offset type to which each unit area belongs; and an offset having a bit width which differs depending on the offset
   type.   The image filter refers to the offset-type specifying information included in the coded data, and determines an
20 offset type to which a subject unit area belongs and also decodes an offset having a bit width which differs depending on
   the determined offset type.
   [0027]
              The coded data configured as describe above includes an offset having a bit width which differs depending on
   the offset type, thereby reducing the amount of data required to code data.       The image filter which decodes the coded
25 data refers to the offset-type specifying information, and determines the offset type to which a subject unit area belongs
                                                                   7

   and also decodes an offset having a bit width which differs depending on the determined offset type.        It is thus possible
   to perform appropriate offset filtering processing while the memory size of a memory for storing offsets is reduced.
   [0028]
              The offset-type specifying information may be determined for each of the input images or for each of the unit
 5 areas.   Alternatively, the offset-type specifying information may be determined for each predetermined set of the input
   images or for each predetermined set of the unit areas.
   [0029]
             According to another aspect of the present invention, there is provided a filtering method for receiving a
   deblocked decoded image and adding an offset to a pixel value of each pixel of a unit area of the deblocked decoded
10 image which is constituted by a plurality of unit areas, comprising: setting an offset bit depth and an offset value range
   by referring to coded data and a shift value, wherein a maximum bit length representing the offset value range is equal to
   the offset bit depth-K, and the value of K is equal to 4; perform bitwise left shift on the decoded offset by an amount
   equal to the shift value adding the offset to the pixel value of each pixel of a unit area of the deblocked decoded image.
   Advantageous Effects of Invention
15 [0030]
             As described above, an image filtering device according to the present invention is an image filtering device for
   adding an offset to a pixel value of each pixel forming an input image which is constituted by a plurality of unit areas.
   The image filtering device includes: offset attribute setting means for setting an offset value range by referring to coded
   data; offset decoding means for decoding an offset which is restricted to the set offset value range; and filtering means
20 for adding the offset to the pixel value of each pixel forming the input image.
   [0031]
             An image filtering device according to the present invention is an image filtering device for adding an offset to
   a pixel value of each pixel forming an input image which is constituted by a plurality of unit areas.     The image filtering
   device includes: determining means for determining an offset type to which a subject unit area belongs among a plurality
25 of offset types; offset coding means for determining an offset having a bit width which differs depending on the offset
                                                                  8

   type and for coding the offset; and filtering means for adding the determined offset to the pixel value of each pixel
   forming the input image.
   [0032]
              A data structure of coded data according to the present invention is a data structure of coded data which is
 5 referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas.     The data structure includes: offset-type specifying information which specifies
   an offset type to which each unit area belongs; and an offset having a bit width which differs depending on the offset
   type.    The image filter refers to the offset-type specifying information included in the coded data, and determines an
   offset type to which a subject unit area belongs and also decodes an offset having a bit width which differs depending on
10 the determined offset type.
   [0033]
              With the above-described configuration, it is possible to cause an image filter to perform appropriate offset
   filtering processing while the memory size of a memory for storing offsets is reduced.
   Brief Description of Drawings
15 [0034]
              [Fig. 1] Fig. 1 is a block diagram illustrating the configuration of an adaptive offset filter according to a first
   embodiment of the present invention.
              [Fig. 2] Fig. 2 illustrates a data structure of coded data generated by a video coding device according to the first
   embodiment of the present invention and decoded by a video decoding device according to the first embodiment of the
20 present invention: parts (a) through (d) respectively illustrate a picture layer, a slice layer, a tree block layer, and a CU
   layer; part (e) illustrates the configuration of QAOU information concerning a QAOU which is not a leaf; and part (f)
   illustrates the configuration of QAOU information concerning a QAOU which is a leaf
              [Fig. 3] Fig. 3 illustrates each syntax included in offset information 01 concerning coded data according to the
   first embodiment of the present invention.
25            [Fig. 4] Fig. 4 shows split modes of offset units according to the first embodiment of the present invention: part
                                                                    9

   (a) shows a split mode when sao curr depth=0; part (b) shows a split mode when saocurr depth=1; part (c) shows a
   split mode when sao curr depth=2; part (d) shows a split mode when sao curr depth=3; and part (e) shows a split mode
   when saocurrdepth=4.
              [Fig. 5] Fig. 5 is a block diagram illustrating the configuration of the video decoding device according to the
 5 first embodiment of the present invention.
              [Fig. 6] Fig. 6 illustrates the first embodiment of the present invention: part (a) shows QAOMUs having a split
   depth of three and forming a subject unit of processing and QAOU indexes assigned to QAOMUs; and part (b) shows
   offset types associated with QAOU indexes 0 to 9 and offsets with respect to individual classes that can be selected for
   each offset type.
10            [Fig. 7] Fig. 7 illustrates examples of QAOMU numbers appended to QAOMUs included in a subject unit of
   processing: part (a) shows a QAOMU number assigned to a QAOMU having a split depth of 0; part (b) shows QAOMU
   numbers assigned to QAOMUs having a split depth of one; part (c) shows QAOMU numbers assigned to QAOMUs
   having a split depth of two; part (d) shows QAOMU numbers assigned to QAOMUs having a split depth of three; and
   part (e) shows QAOMU numbers assigned to QAOMUs having a split depth of four.
15            [Fig. 8] Fig. 8 is a flowchart of a flow of processing performed by an adaptive offset filter processor according
   to the first embodiment of the present invention.
              [Fig. 9] Fig. 9 illustrates, together with pixel bit depths, examples of offset bit depths and shift values set by an
   offset attribute setting section included in an adaptive filter according to the first embodiment of the present invention:
   parts (a) through (d) respectively show examples associated with patterns SI through S4; and part (e) shows a case in
20 which STEP=2 in part (d).
              [Fig. 10] Fig. 10 illustrates offset processing performed by the adaptive offset filter according to the first
   embodiment of the present invention: parts (a) through (d) show pixels to be referred to when saotypeidx=1 to 4,
   respectively.
              [Fig. 11] Fig. 11 illustrates offset processing performed by the adaptive offset filter according to the first
25 embodiment of the present invention: part (a) shows graphs indicating the magnitude relations between a pixel value
                                                                    10

   pic[x] of a subject pixel x and the pixel value of a pixel a or b and also shows the values of a function Sign in accordance
   with the magnitude relation; part (b) shows graphs indicating the magnitude relations between the pixel value of the
   subject pixel x and each of the pixel values of the pixel a and the pixel b and also shows the values of EdgeType in
   accordance with the magnitude relation; part (c) shows the association between each graph shown in part (b) and
 5 class idx; and part (d) illustrates a transform table indicating transformation from EdgeType to classidx.
              [Fig. 12] Fig. 12 illustrates offset processing performed by the adaptive offset filter according to the first
   embodiment of the present invention: part (a) schematically shows classifying performed when saotype idx=5; part (b)
   schematically shows classifying performed when saotype_idx=6; and part (c) shows a table indicating an example of
   classifying performed when a band offset is specified.
10            [Fig. 13] Fig. 13 illustrates offset processing performed by the adaptive offset filter according to the first
   embodiment of the present invention and shows a table indicating another example of classifying performed when a
   band offset is specified.
              [Fig. 14] Fig. 14 is a block diagram illustrating the configuration of a video coding device according to the first
   embodiment of the present invention.
15            [Fig. 15] Fig. 15 is a block diagram illustrating the configuration of an adaptive offset filter included in the
   video coding device according to the first embodiment of the present invention.
              [Fig. 16] Fig. 16 is a flowchart of a flow of processing performed by an offset calculator of the adaptive offset
   filter included in the video coding device according to the first embodiment of the present invention.
              [Fig. 17] Fig. 17 is a flowchart of a flow of processing performed by an offset information selector of the
20 adaptive offset filter included in the video coding device according to the first embodiment of the present invention.
              [Fig. 18] Fig. 18 illustrates processing performed by the offset information selector of the adaptive offset filter
   included in the video coding device according to the first embodiment of the present invention: part (a) shows a split
   mode when the split depth is 0 and 1; part (b) shows a split mode when the split depth is 1; part (c) shows a split mode
   when the split depth is 2; and part (d) shows an example of split mode determined by the offset information selector.
25            [Fig. 19] Fig. 19 is a block diagram illustrating the configuration of an adaptive offset filter included in a video
                                                                   11

   decoding device according to a second embodiment of the present invention.
              [Fig. 20] Fig. 20 illustrates the adaptive offset filter according to the second embodiment of the present
   invention: part (a) illustrates a first specific example of a function mergetbl[saotype idx]; and part (b) illustrates a
   second specific example of the function merge tbl[saotypeidx].
 5            [Fig. 21] Fig. 21 is a block diagram illustrating the configuration of an adaptive offset filter included in a video
   coding device according to the second embodiment of the present invention.
              [Fig. 22] Fig. 22 is a block diagram illustrating the configuration of an adaptive offset filter according to a third
   embodiment of the present invention.
              [Fig. 23] Fig. 23 illustrates syntax of offset information 01 of coded data according to the third embodiment.
10            [Fig. 24] Fig. 24 illustrates the third embodiment: part (a) shows QAOMUs having a split depth of three and
   forming a subject unit of processing and also shows QAOU indexes assigned to QAOMUs; and part (b) shows an offset
   type associated with each of the QAOU indexes 0 to 9 and offsets with respect to individual classes that can be selected
   for each offset type.
              [Fig. 25] Fig. 25 illustrates transform tables used by the offset information decoding section according to the
15 third embodiment.
              [Fig. 26] Fig. 26 illustrates offset processing performed by the adaptive offset filter according to the third
   embodiment: part (a) shows graphs indicating the magnitude relations between the pixel value pix[x] of a subject pixel x
   and the pixel value of a pixel a or b and also shows the values of a function Sign in accordance with the magnitude
   relation; part (b) shows graphs indicating the magnitude relations between the pixel value of the subject pixel x and each
20 of the pixel values of the pixel a and the pixel b and also shows the value of EdgeType in accordance with the magnitude
   relation; part (c) shows the association between each graph shown in part (b) and classidx; and parts (d) through (f)
   illustrate transform tables used for transforming from EdgeType to classidx.
              [Fig. 27] Fig. 27 illustrates offset processing performed by the adaptive offset filter according to the third
   embodiment: part (a) schematically shows classifying performed when "sao typeidx="5; and part (b) schematically
25 shows classifying performed when "saotype idx=6".
                                                                    12

             [Fig. 28] Fig. 28 illustrates offset processing performed by the adaptive offset filter according to the third
   embodiment: part (a) schematically shows classifying performed when the hierarchical depth of a subject QAOU is
   smaller than a threshold; and part (b) schematically shows classifying performed when the hierarchical depth of a subject
   QAOU is equal to or greater than the threshold.
 5           [Fig. 29] Fig. 29 illustrates an example of classifying performed when a band offset is specified: part (a) shows
   an example of classifying performed when the hierarchical depth of a subject QAOU is smaller than a threshold; and
   part (b) shows an example of classifying performed when the hierarchical depth of a subject QAOU is equal to or greater
   than the threshold.
             [Fig. 30] Fig. 30 is a block diagram illustrating the configuration of an adaptive offset filter 80 included in a
10 video coding device according to the third embodiment.
             [Fig. 31] Fig. 31 shows an overview of a case in which a squared error concerning a QAOU having a QAOU
   index "x" is calculated for each offset type in the third embodiment.
             [Fig. 32] Fig. 32 illustrates the configuration in which EO and BO are switched in accordance with the pixel
   value in a fourth embodiment of the present invention: part (a) shows an overview of the configuration in which EO and
15 BO are switched in accordance with the pixel value; part (b) shows specific values to be switched; and part (c) shows the
   content of a list memory stored in the offset information storage section 621.
             [Fig. 33] Fig. 33 illustrates a case in which the EO type is restricted to the horizontal direction in a fifth
   embodiment of the present invention: part (a) shows an overview of advantages; part (b) shows the positions of pixels in
   the horizontal direction; part (c) shows a state different from the state in part (b); and part (d) shows an overview of
20 advantages obtained by the state in part (c).
             [Fig. 34] Fig. 34 illustrates a case in which the EO type is restricted to the horizontal direction in the fifth
   embodiment of the present invention: parts (a) and (b) show cases in which reference pixels are positioned
   asymmetrically; and part (c) shows an overview of horizontal edges.
             [Fig. 35] Fig. 35 shows an overview of a case in which the offset precision is improved in a sixth embodiment
25 of the present invention.
                                                                  13

               [Fig. 36] Fig. 36 illustrates an overview of a case in which more detailed classifying is performed in the sixth
   embodiment: part (a) shows a transform table; and parts (b) through (d) show classifying.
               [Fig. 37] Fig. 37 illustrates a case in which classifying is performed in accordance with the chrominance in the
   sixth embodiment; part (a) shows classifying performed by considering the pixel value of an achromatic color; part (b)
 5 shows classifying in which two value ranges are asymmetrically positioned with respect to the pixel values of an
   achromatic color; parts (c) and (d) show a case in which classifying is performed differently in accordance with the
   chrominance channel (Cr or Cb); and part (e) shows a case in which classifying is performed by using only one BO type.
               [Fig. 38] Fig. 38 is a block diagram illustrating the configuration of an offset information decoding section
   according to a seventh embodiment of the present invention.
10             [Fig. 39] Fig. 39 illustrates an overview of a case in which there are classes under which no pixels are classified
   in the seventh embodiment.
               [Fig. 40] Fig. 40 illustrates syntax to be used when a prediction candidate flag is used in the seventh
   embodiment.
               [Fig. 41] Fig. 41 is a block diagram illustrating the configuration of an offset information decoding section
15 according to the fifth embodiment.
               [Fig. 42] Part (a) of Fig. 42 is a block diagram illustrating the configuration of an offset type selector according
   to the fifth embodiment; and part (b) is a block diagram illustrating the configuration of another offset type selector.
               [Fig. 43] Fig. 43 is a block diagram illustrating the configuration of a classifying section according to the fifth
   embodiment.
20             [Fig. 44] Fig. 44 illustrates syntax of offset information and syntax of QAOU information according to the
   third embodiment: part (a) illustrates syntax of offset information; part (b) illustrates syntax of QAOU information; and
   part (c) illustrates the entire syntax of an adaptive offset filter which calls syntax of part (a) and syntax of part (b).
               [Fig. 45] Fig. 45 illustrates a case in which a video decoding device and a video coding device can be used for
   transmitting and receiving video images; part (a) is a block diagram illustrating the configuration of a transmitting
25 apparatus on which the video coding device is mounted; and part (b) is a block diagram illustrating the configuration of a
                                                                    14

   receiving apparatus on which the video decoding device is mounted.
              [Fig. 46] Fig. 46 illustrates a case in which a video decoding device and a video coding device can be used for
   recording and playing back video images; part (a) is a block diagram illustrating the configuration of a recording
   apparatus on which the video coding device 2 is mounted; and part (b) is a block diagram illustrating the configuration
 5 of a playback apparatus on which the video decoding device is mounted.
   Description of Embodiments
   [0035]
   [First Embodiment]
   (Coded Data #1)
10            Prior to a detailed description of a video coding device 2 and a video decoding device 1 according to a first
   embodiment, the data structure of coded data #1 to be generated by the video coding device 2 and to be decoded by the
   video decoding device 1 will be discussed below.
   [0036]
              Fig. 2 illustrates the data structure of the coded data #1.    The coded data #1 includes a sequence and a
15 plurality of pictures forming the sequence by way of example.
   [0037]
              The structures of hierarchical levels of a picture layer or lower layers of the coded data #1 are shown in Fig. 2.
   Parts (a) through (d) of Fig. 2 respectively illustrate a picture layer which defines a picture PICT, a slice layer which
   defines a slice S, a tree block layer which defines a tree block (Tree block) TBLK, and a CU layer which defines a
20 coding unit (Coding Unit; CU) included in the tree block TBLK.
   [0038]
   (Picture Layer)
              In the picture layer, a set of data items to be referred to by the video decoding device 1 in order to decode a
   picture PICT to be processed (hereinafter also referred to as a "subject picture") are defined.     The picture PICT includes,
25 as shown in part (a) of Fig. 2, a picture header PH and slices SI through SNS (NS is the total number of slices included in
                                                                    15

   the picture PICT).
   [0039]
             If it is not necessary to distinguish the individual slices S I through SNS from each other, the subscripts of the
   reference numerals may be omitted.         Other items of data provided with subscripts included in the coded data #1
 5 described below are handled in a similar manner.
   [0040]
             The picture header PH includes a coding parameter set to be referred to by the video decoding device 1 in order
   to determine a decoding method for a subject picture.        For example, coding mode information
   (entropy coding mode flag) which indicates the variable-length coding mode used when the video coding device 2 has
10 performed coding is an example of coding parameters included in the picture header PH.
   [0041]
             If entropy codingmodeflag is 0, the picture PICT has been coded by CAVLC (Context-based Adaptive
   Variable Length Coding).       If entropy codingmodeflag is 1, the picture PICT has been coded by CABAC (Context
   based Adaptive Binary Arithmetic Coding).
15 [0042]
             The picture header PH is also referred to as a "picture parameter set (PPS: Picture Parameter Set).
   [0043]
   (Slice Layer)
             In the slice layer, a set of data items to be referred to by the video decoding device 1 in order to decode a slice
20 S to be processed (hereinafter also referred to as a "subject slice") are defined.    The slice S includes, as shown in part
   (b) of Fig. 2, a slice header SH and a sequence of tree blocks TBLKI through TBLKNC (NC is the total number of tree
   blocks included in the slice S).
   [0044]
             The slice header SH includes a coding parameter set to be referred to by the video decoding device 1 in order
25 to determine a decoding method for a subject slice.       Slice-type specifying information (slice type) which specifies a
                                                                   16

   slice type is an example of coding parameters included in the slice header SH.
   [0045]
             Examples of slice types that can be specified by the slice-type specifying information are (1) I slice which uses
   only intra prediction when coding is performed, (2) P slice which uses unidirectional prediction or intra prediction when
 5 coding is performed, and (3) B slice which uses unidirectional prediction, bidirectional prediction, or intra prediction
   when coding is performed.
   [0046]
             The slice header SH also includes a filter parameter FP to be referred to by an adaptive filter provided in the
   video decoding device 1.      The filter parameter FP may be included in the picture header PH.
10 [0047]
   (Tree Block Layer)
             In the tree block layer, a set of data items to be referred to by the video decoding device 1 in order to decode a
   tree block TBLK to be processed (hereinafter also referred to as a "subject tree block") are defined.     The tree block may
   also be referred to as a largest coding unit (LCU: Largest Coding Unit).
15 [0048]
             The tree block TBLK includes a tree block header TBLKH and items of coding unit information CU Ithrough
   CUNL (NL is the total number of items of coding unit information included in the tree block TBLK).         The relationship
   between the tree block TBLK and the coding unit information CU will first be discussed below.
   [0049]
20           The tree block TBLK is split into partitions for specifying the block size which is used for each of processing
   operations, such as intra prediction, inter prediction, and transform.
   [0050]
             The partitions of the tree block TBLK are obtained by recursive quadtree splitting.     Hereinafter, the tree
   structure obtained by this recursive quadtree splitting will be referred to as a "coding tree".
25 [0051]
                                                                  17

              Hereinafter, a partition corresponding to a leaf, which is an end node of a coding tree, will be referred to as a
   coding node.     The coding node is a basic unit of coding processing, and thus, it will also be referred to as a "coding unit
   (CU)".
   [0052]
 5            That is, each of the items of coding unit information CUI through   CUNL   (hereinafter referred to as "CU
   information") is information corresponding to each coding node (coding unit) obtained by performing recursive quadtree
   splitting on the tree block TBLK.
   [0053]
              The root of the coding tree is associated with the tree block TBLK.    In other words, the tree block TBLK is
10 associated with the highest node of a quadtree splitting tree structure in which a plurality of coding nodes are included in
   a recursive manner.
   [0054]
              The vertical and horizontal sizes of each coding node are half the vertical and horizontal sizes of a higher
   coding node to which the coding node directly belongs (that is, a partition of a node one level higher than the coding
15 node).
   [0055]
              The possible sizes of each coding node are dependent on size specifying information and maximum
   hierarchical depth of the coding node included in a sequence parameter set SPS of the coded data #1.         For example, if
   the size of a tree block TBLK is 64    x 64 pixels and the maximum hierarchical depth is three, each of the coding nodes of
20 the tree block TBLK and lower levels may take one of the three sizes, that is, 64   x  64 pixels, 32 x  32 pixels, and 16 x  16
   pixels.
   [0056]
   (Tree Block Header)
              In the tree block header TBLKH, coding parameters referred to by the video decoding device 1 in order to
25 determine a decoding method for a subject tree block are included.       More specifically, as shown in part (c) of Fig. 2,
                                                                 18

   tree block split information SP TBLK which specifies a split pattern for splitting a subject tree block into CUs and a
   quantization parameter difference Aqp (qp delta) which specifies the quantization step size are included.
   [0057]
              The tree block split information SP TBLK is information indicating a coding tree for splitting a tree block, and
 5 more specifically, the configuration and the size of each of the CUs included in the subject tree block and the position of
   each CU within the subject tree block.
   [0058]
              The tree block split information SPTBLK does not have to explicitly include the configuration and the size of
   each CU.      For example, the tree block split information SPTBLK may be a set of flags (split coding unit flag)
10 indicating whether or not the entre subject tree block or sub-regions of the tree block will be subjected to quadtree
   splitting.   In this case, the configuration and the size of the tree block are utilized together with the set of flags, thereby
   making it possible to specify the configuration and the size of each CU.
   [0059]
              The quantization parameter difference Aqp is a difference qp-qp' between the quantization parameter qp used in
15 the subject tree block and the quantization parameter qp' used in a tree block which has been coded immediately before
   the subject tree block.
   [0060]
   (CU Layer)
              In the CU layer, a set of data items referred to by the video decoding device 1 for decoding a CU to be
20 processed (hereinafter also referred to as a "subject CU") are defined.
   [0061]
              Prior to a specific description of data items included in CU information CU, the tree structure of data items
   included in a CU will be discussed below.       The coding node is a node of a root of a prediction tree (PT) or a node of a
   root of a transform tree (TT).    The prediction tree and the transform tree will be discussed as follows.
25 [0062]
                                                                  19

             In the prediction tree, a coding node is split into one or a plurality of prediction blocks, and the position and the
   size of each prediction block are defined.      In other words, one prediction block or the prediction blocks are one or a
   plurality of non-overlapping regions forming a coding node.        The prediction tree includes one or a plurality of
   prediction blocks obtained by the above-described splitting.
 5 [0063]
             Prediction processing is performed on each prediction block.        The prediction block, which is a unit of
   prediction, will also be referred to as a "prediction unit (PU)".
   [0064]
             Broadly speaking, split methods used in a prediction tree are divided into two types, that is, one type using intra
10 prediction and the other type using inter prediction.
   [0065]
             In the type of intra prediction, the split method includes split patterns of 2Nx2N (the same size as that of a
   coding node) and NxN.
   [0066]
15           In the type of inter prediction, the split method includes split patterns of 2Nx2N (the same size as that of the
   coding node), 2NxN, Nx2N, and NxN.
   [0067]
             In the transform tree, a coding node is split into one or a plurality of transform blocks, and the position and the
   size of each transform block are defined.      In other words, one transform block or the transform blocks are one or a
20 plurality of non-overlapping regions forming a coding node.        The transform tree includes one or a plurality of transform
   blocks obtained by the above-described splitting.
   [0068]
             Split methods used in the transform tree are divided into the following types: assigning a region having the
   same size as that of a coding node as the transform block; and assigning regions obtained by performing recursive
25 quadtree splitting, as in the splitting of the above-described tree block, as transform blocks.
                                                                  20

   [0069]
              Transform processing is performed on each transform block.        The transform block, which is a unit of
   transform, will also be referred to as a "transform unit (TU)".
   [0070]
 5 (Data Structure of CU Information)
              Data included in the CU information CU will now be specifically discussed below with reference to part (d) of
   Fig. 2.   Specifically, the CU information CU includes, as shown in part (d) of Fig. 2, a skip flag SKIP, PT information
   PTI, and TT information TTI.
   [0071]
10            The skip flag SKIP is a flag indicating whether or not a skip mode is applied to a subject PU.    If the value of
   the skip flag SKIP is 1, that is, if the skip mode is applied to a subject CU, the PT information PTI and the TT
   information TTI of this CU information CU are omitted.         The skip flag SKIP is omitted in I slices.
   [0072]
              The PT information PTI is information concerning a PT included in the CU.        In other words, the PT
15 information PTI is a set of information items concerning one or a plurality of PUs included in the PT and is referred to
   by the video decoding device 1 when generating prediction images.          The PT information PTI includes, as shown in part
   (d) of Fig. 2, prediction type information PType and prediction information Plnfo.
   [0073]
              The prediction type information PType is information which specifies whether intra prediction or inter
20 prediction will be used as the prediction-image generating method for the subject PU.
   [0074]
              The prediction information Plnfo is constituted by intra-prediction information or inter-prediction information,
   depending on whether the prediction type information PType specifies intra prediction or inter prediction.       Hereinafter,
   a PU to which intra prediction is applied may also be referred to as an "intra PU", and a PU to which inter prediction is
25 applied may also be referred to as an "inter PU".
                                                                  21

   [0075]
              The prediction information Plnfo also includes information which specifies the configuration, size, and
   position of the subject PU.     As stated above, the generation of a prediction image is performed by using a PU as a unit.
   Details of the prediction information Plnfo will be discussed later.
 5 [0076]
              The TT information TTI is information concerning a TT included in the CU.         In other words, the TT
   information TTI is a set of information items concerning one or a plurality of TUs included in the TT and is referred to
   by the video decoding device 1 when decoding residual data.        Hereinafter, a TU may also be referred to as a "block".
   [0077]
10            The TT information TTI includes, as shown in part (d) of Fig. 2, TT split information SPTT which specifies a
   split pattern used for splitting the subject CU into transform blocks and quantized prediction residuals QD through
   QDNT    (NT is the total number of blocks included in the subject CU).
   [0078]
              The TT split information SPTT is information for determining the configuration and the size of each TU
15 included in the subject CU and the position of each TU within the subject CU.        For example, the TT split information
   SP_TT may be implemented by information (split transform unit flag) indicating whether or not a subject node will be
   split and information (trafoDepth) indicating the depth of the splitting.
   [0079]
              If the size of a CU is 64 x 64, the size of each of TUs obtained by splitting the CU may be in a range of 32 x 32
20 pixels to 2 x 2 pixels.
   [0080]
              Each quantized prediction residual QD is coded data generated as a result of the video coding device 2
   executing the following Processing 1 through Processing 3 on a subject block, which is a block to be processed.
   [0081]
25            Processing 1: performing DCT transform (Discrete Cosine Transform) on a prediction residual obtained by
                                                                  22

   subtracting a prediction image from an image to be coded;
             Processing 2: quantizing a transform coefficient obtained in Processing 1; and
             Processing 3: performing variable-length coding on the transform coefficient quantized in Processing 2.
   The above-described quantization parameter qp represents a quantization step size QP (QP=2qp/6) used when the video
 5 coding device 2 quantizes the transform coefficient.
   [0082]
   (Prediction Information Plnfo)
             As stated above, the prediction information Plnfo has two types, that is, inter-prediction information and intra
   prediction information.
10 [0083]
             The inter-prediction information includes coding parameters referred to by the video decoding device 1 when
   generating an inter-prediction image by using inter prediction.    More specifically, the inter-prediction information
   includes inter PU split information which specifies a split pattern used for splitting a subject CU into inter PUs and inter
   prediction parameters concerning individual inter PUs.
15 [0084]
             The inter-prediction parameters include a reference image index, an estimation motion-vector index, and a
   motion-vector residual.
   [0085]
             The intra-prediction information includes coding parameters referred to by the video decoding device 1 when
20 generating an intra-prediction image by using intra prediction.    More specifically, the intra-prediction information
   includes intra PU split information which specifies a split pattern used for splitting a subject CU into intra PUs and intra
   prediction parameters concerning individual intra PUs.     The intra-prediction parameter is a parameter for specifying an
   intra-prediction method (prediction mode) for each intra PU.
   [0086]
25 (Offset Unit)
                                                                23

              In this embodiment, each picture or each slice is recursively split into a plurality of offset units (may also be
   referred to as "QAOU: Quad Adaptive Offset Unit") in accordance with a quadtree structure.            The QAOU is a
   processing unit of offset filtering processing performed by an adaptive offset filter of this embodiment.
   [0087]
 5            As shown in parts (e) and (f) of Fig. 2, QAOU information, which is information concerning each QAOU,
   includes saosplit flag indicating whether or not the QAOU will be further split.         More specifically, saosplitflag is
   specified by arguments (saocurr depth, ys, xs), which will be discussed later, and is also represented by
   sao_split flag[saocurr depth][ys][xs].
   [0088]
10            If saosplitflag included in a QAOU indicates that the QAOU will be further split (that is, if the QAOU is not
   a leaf), the QAOU information concerning the QAOU includes, as shown in part (e) of Fig. 2, items of QAOU
   information concerning a plurality of QAOUs included in the QAOU.
   [0089]
              On the other hand, if saosplit flag included in a QAOU indicates that the QAOU will not be further split (that
15 is, if the QAOU is a leaf), the QAOU information concerning the QAOU includes, as shown in part (f) of Fig. 2, offset
   information 01 concerning the QAOU.          The offset information 01 includes, as shown in part (f) of Fig. 2, offset-type
   specifying information OTI which specifies an offset type and an offset group which is determined in accordance with
   the offset type.    As shown in part (f) of Fig. 2, the offset group includes a plurality of offsets.
   [0090]
20            An offset included in coded data is a quantized value.      An offset included in coded data may be a prediction
   residual obtained by using a certain prediction, for example, a linear prediction.      If the pixel bit depth, which will be
   discussed later, is variably set for each unit area, the pixel bit depth for an area to be processed may be included in the
   offset-type specifying information OTI, since the offset bit depth, the offset value range, and the shift value are changed
   in accordance with the pixel bit depth in this embodiment.
25
                                                                   24

   [0091]
               The offset information 01 will be discussed below by changing drawings for reference.
   [0092]
               Fig. 3 illustrates each syntax included in the offset information 01 (indicated by sao offsetparam() in Fig. 3).
 5 [0093]
               As shown in Fig. 3, the offset information 01 includes syntax saotype idx[saocurr depth][ys][xs].         If
   sao_typeidx[saocurr depth] [ys] [xs] is not 0, sao_offset[saocurr depth] [ys] [xs] [i] is included in the offset
   information 01.
   [0094]
10 (saocurr depth, ys, xs)
               An argument sao_curr depth, which is an argument of sao type idx and saooffset, is an index indicating the
   split depth of a QAOU, and ys and xs are indexes respectively indicating the position in the y direction and the position
   in the x direction of a QAOU (or a QAOMU, which will be discussed later).
   [0095]
15             Parts (a) through (e) of Fig. 4 show split modes of a QAOU in accordance with the values of saocurr depth:
   part (a) shows a split mode when saocurrdepth=0, part (b) shows a split mode when saocurrdepth=1, part (c) shows
   a split mode when saocurrdepth=2, part (d) shows a split mode when saocurrdepth=3, and part (e) shows a split
   mode when sao_curr depth=4.           As shown in parts (a) through (e) of Fig. 4, each QAOU is specified by saocurr depth
   and (xs, ys).
20 [0096]
               As shown in part (a) of Fig. 4, when sao_currdepth=0, xs and ys are both 0.       As shown in part (b) of Fig. 4,
   when saocurrdepth=1, xs and ys take 0 or 1.            As shown in part (c) of Fig. 4, when saocurrdepth=2, xs and ys take
   one of 0, 1, 2, and 3.      Generally, in response to a given value of sao_curr depth, xs and ys take values of 0 to
   2 saocurr depth_ I
25 [0097]
                                                                    25

   (saotype idx)
              Syntax saotype idx[saocurr depth] [ys] [xs] corresponds to the above-described offset-type specifying
   information OTI, and is syntax for specifying the offset type of each QAOU.         Hereinafter, sao typeidx may be simply
   referred to as an "offset type".
 5 [0098]
              In this embodiment, saotype idx[saocurr depth] [ys][xs] takes integers from 0 to 6.          When
   sao_typeidx[saocurr depth][ys] [xs]=0, it indicates that offset filtering processing is not performed on an image which
   has not been yet subjected to offset filtering (for example, a deblocked decoded image PDB, which will be discussed
   later) in a subject QAOU.      When saotype idx[sao curr depth] [ys][xs]=1 to 4, it indicates that edge offset processing
10 is performed on an image which has not been yet subjected to offset filtering in a subject QAOU.          When
   sao_typeidx[saocurr depth][ys] [xs]=5 or 6, it indicates that band offset processing is performed on an image which
   has not been yet subjected to offset filtering in a subject QAOU.      The edge offset processing and band offset processing
   will be discussed later specifically.
   [0099]
15 (sao offset)
              Syntax saooffset[saocurr depth] [ys] [xs][i] is syntax which represents a specific value of an offset to be
   added to each pixel included in a subject QAOU in offset filtering processing using an adaptive offset filter of this
   embodiment.       In this embodiment, saooffset may also be simply referred to as an "offset".
   [0100]
20            Syntax saooffset[saocurr depth] [ys][xs][i] is specified by the index i as well as the arguments
   sao curr depth, ys, and xs.      The index i is an index for specifying a class, and is also indicated by classidx. The
   index i takes one of integers 0 to 4 when the value of sao_type idx[saocurr depth][ys][xs] is one of I to 4 (that is, in
   the case of an edge offset).    The index i takes one of integers 0 to 16 when the value of
   saotypeidx[saocurr depth][ys][xs] is 5 or 6 (that is, in the case of a band offset).        In whichever case, when i is 0, it
25 indicates that an offset is not added, which will be discussed later.
                                                                  26

   [0101]
              The adaptive offset filter of this embodiment classifies a subject pixel included in a subject QAOU as one of
   the above-described plurality of classes, and adds an offset Offset associated with the class of the subject pixel to the
   subject pixel.    This will be discussed later.
 5 [0102]
              Descriptor ue(v) shown in Fig. 3 indicates that syntax associated with this descriptor is an unsigned numeric
   value and that variable-length coding will be performed on the numeric value.        Descriptor se(v) shown in Fig. 3
   indicates that syntax associated with this descriptor is a signed numeric value and that variable-length coding will be
   performed on a sign and an absolute value forming the numeric value.
10 [0103]
   (Video Decoding Device 1)
              The video decoding device 1 of this embodiment will be described below with reference to Figs. 1 and 5
   through 13.     The video decoding device 1 partially includes a method defined in H. 264/MPEG-4. AVC, a method used
   in KTA software, which is a joint development codec in VCEG (Video Coding Expert Group), a method used in TMuC
15 (Test Model under Consideration) software, which is a successor codec to the codec used in KTA software, and a
   technology used in HM (HEVC TestModel) software.
   [0104]
              Fig. 5 is a block diagram illustrating the configuration of the video decoding device 1. The video decoding
   device 1 includes, as shown in Fig. 5, a variable-length code decoder 13, a motion-vector reconstructing unit 14, a buffer
20 memory 15, an inter-prediction image generator 16, an intra-prediction image generator 17, a prediction-method
   determining unit 18, an inverse-quantize-and-inverse-transform unit 19, an adder 20, a deblocking filter 41, an adaptive
   filter 50, and an adaptive offset filter 60.   The video decoding device 1 is a device for generating a video image #2 by
   decoding coded data #1.
   [0105]
25            The variable-length code decoder 13 decodes a prediction parameter PP concerning each partition from the
                                                                  27

   coded data #1.     That is, concerning an inter-prediction partition, the variable-length code decoder 13 decodes a
   reference image index RI, an estimation motion-vector index PMVI, and a motion-vector residual MVD from the coded
   data #1, and supplies them to the motion-vector reconstructing unit 14.       On the other hand, concerning an intra
   prediction partition, the variable-length code decoder 13 decodes (1) size specifying information which specifies the size
 5 of the partition and (2) prediction-index specifying information which specifies a prediction index from the coded data
   #1, and supplies them to the intra-prediction image generator 17.       The variable-length code decoder 13 also decodes
   CU information from the coded data and supplies it to the prediction-method determining unit 18 (the flow of CU
   information is not shown).     The variable-length code decoder 13 also decodes a quantized prediction residual QD
   concerning each block and a quantization parameter difference Aqp concerning a tree block including this block from the
10 coded data #1 and supplies them to the inverse-quantize-and-inverse-transform unit 19.        The variable-length code
   decoder 13 also extracts QAOU information from the coded data #1 and supplies the extracted QAOU information to the
   adaptive offset filter 60.
   [0106]
              The motion-vector reconstructing unit 14 reconstructs a motion vector my concerning each inter-prediction
15 partition from the motion-vector residual MVD concerning this partition and from a reconstructed motion vector mv'
   concerning another partition.     More specifically, the motion-vector reconstructing unit 14 (1) determines an estimation
   motion vector pmv from the reconstructed motion vector mv' in accordance with an estimation method specified by the
   estimation motion vector index PMVI, and (2) obtains the motion vector my by adding the motion-vector residual MVD
   to the estimation motion vector pmv.     The reconstructed motion vector mv' concerning another partition may be read
20 from the buffer memory 15.       The motion-vector reconstructing unit 14 supplies the reconstructed motion vector my to
   the inter-prediction image generator 16, together with the associated reference image index RI.      In the case of an inter
   prediction partition on which bidirectional prediction (weighted prediction) will be performed, the motion-vector
   reconstructing unit 14 supplies two reconstructed motion vectors mv1 and mv2 to the inter-prediction image generator
   16, together with associated reference image indexes RI 1 and RI2.
25 [0107]
                                                                 28

             The inter-prediction image generator 16 generates a motion-compensated image mc concerning each inter
   prediction partition.   More specifically, the inter-prediction image generator 16 generates a motion-compensated image
   mc from a filtered decoded image PFL' specified by the reference image index RI supplied from the motion-vector
   reconstructing unit 14 by using the motion vector my supplied from the motion-vector reconstructing unit 14.          The
 5 filtered decoded image PFL' is an image obtained by performing deblocking processing by using the deblocking filter
   41, offset filtering processing by using the adaptive offset filter 60, and adaptive filtering processing by using the
   adaptive filter 50 on a decoded image P.     The inter-prediction image generator 16 may read pixel values of individual
   pixels forming the filtered decoded image PFL' from the buffer memory 15.            The motion-compensated image mc
   generated by the inter-prediction image generator 16 is supplied to the prediction-method determining unit 18 as an
10 inter-prediction image Pred Inter.     In the case of an inter-prediction partition on which bidirectional prediction
   (weighted prediction) will be performed, the inter-prediction image generator 16 performs: (1) generating a motion
   compensated image mc I from a filtered decoded image PFL1' specified by the reference image index RIl by using a
   motion vector mvl; (2) generating a motion-compensated image mc2 from a filtered decoded image PFL2' specified by
   the reference image index R12 by using a motion vector mv2; and (3) generating an inter-prediction image Pred Inter by
15 adding an offset value to a weighted average of the motion-compensated image mc I and the motion-compensated image
   mc2.
   [0108]
             The intra-prediction image generator 17 generates a prediction image Pred Intra concerning each intra
   prediction partition.   More specifically, the intra-prediction image generator 17 refers to a prediction mode decoded
20 from the coded data #1 and assigns the prediction mode to a subject partition, for example, in a raster scanning order.
   Then, the intra-prediction image generator 17 generates a prediction image Pred Intra from the decoded image P in
   accordance with the prediction method represented by the prediction mode.          The intra-prediction image PredIntra
   generated by the intra-prediction image generator 17 is supplied to the prediction-method determining unit 18.
   [0109]
25           The intra-prediction image generator 17 also supplies the size of the subject partition and intra-coding mode
                                                                  29

   information IEM, which is information indicating the prediction mode assigned to the subject partition, to the adaptive
   filter 50.
   [0110]
              The prediction-method determining unit 18 determines, on the basis of the CU information, whether each
 5 partition is an inter-prediction partition to be subjected to inter prediction or an intra-prediction partition to be subjected
   to intra prediction.   If the subject partition is an inter-prediction partition, the prediction-method determining unit 18
   supplies the inter-prediction image Pred Inter generated by the inter-prediction image generator 16 to the adder 20 as the
   prediction image Pred.      If the subject partition is an intra-prediction partition, the prediction-method determining unit
   18 supplies the intra-prediction image Pred Intra generated by the intra-prediction image generator 17 to the adder 20 as
10 the prediction image Pred.
   [0111]
              The inverse-quantize-and-inverse-transform unit 19 performs: (1) inverse-quantizing the quantized prediction
   residual QD; (2) inverse-DCT (Discrete Cosine Transform) transform on a DCT coefficient obtained by performing
   inverse quantization; and (3) supplying a prediction residual D obtained by performing inverse DCT transform to the
15 adder 20.    When performing inverse quantization on the quantized prediction residual QD, the inverse-quantize-and
   inverse-transform unit 19 determines a quantization step QP from the quantization parameter difference Aqp supplied
   from the variable-length code decoder 13.         The quantization parameter qp can be determined by adding the quantization
   parameter difference Aqp to a quantization parameter qp' concerning a tree block which has been subjected to inverse
   quantization and inverse-DCT transform immediately before the subject tree block, and the quantization step QP can be
20 determined by calculating QP= 2      ,16 from the quantization step qp.    The generation of a prediction residual D by the
   inverse-quantize-and-inverse-transform unit 19 is performed by using a block (transform unit) as a unit.
   [0112]
              The adder 20 generates a decoded image P by adding the prediction residual D supplied from the inverse
   quantize-and-inverse-transform unit 19 to the prediction image Pred supplied from the prediction-method determining
25 unit 18.
                                                                    30

   [0113]
              When the difference between the pixel values of pixels adjacent to each other with a block boundary or a CU
   boundary therebetween in the decoded image P is smaller than a predetermined threshold, the deblocking filter 41
   performs deblocking processing on the block boundary or the CU boundary in the decoded image P, thereby smoothing
 5 an image in the vicinity of the block boundary or the CU boundary.         The image subjected to deblocking processing by
   the deblocking filter 41 is output to the adaptive offset filter 60 as a deblocked decoded image PDB.
   [0114]
              The adaptive offset filter 60 performs offset filtering processing using an offset decoded from the coded data #1
   on the deblocked decoded image PDB supplied from the deblocking filter 41, by using a QAOU as a unit of processing,
10 thereby generating an offset-filtered decoded image POF.          The generated offset-filtered decoded image POF is
   supplied to the adaptive offset filter 60.   The specific configuration of the adaptive filter 60 will be discussed later, and
   thus, an explanation thereof is omitted here.
   [0115]
              The adaptive filter 50 performs filtering processing using a filter parameter FP decoded from the coded data #1
15 on the offset-filtered decoded image POF supplied from the adaptive offset filter 60, thereby generating a filtered
   decoded image PFL. The image subjected to filtering processing by the adaptive filter 50 is output to the exterior as
   the filtered decoded image PFL and is also stored in the buffer memory 15 in association with POC specifying
   information decoded from the coded data by the variable-length code decoder 13.
   [0116]
20 (Adaptive Offset Filter 60)
              Fig. 1 is a block diagram illustrating the configuration of the adaptive offset filter 60.  The adaptive offset
   filter 60 includes, as shown in Fig. 1, an adaptive offset filter information decoder 61 and an adaptive offset filter
   processor 62.
   [0117]
25            The adaptive offset filter information decoder 61 includes, as shown in Fig. 1, an offset information decoding
                                                                   31

   section 611, a QAOU structure decoding section 612, and an offset attribute setting section 613.
   [0118]
              The offset information decoding section 611 refers to QAOU information included in the coded data #1 and
   decodes offset information 01 included in the QAOU information.            The values sao_type idx[saocurr depth] [ys] [xs]
 5 and sao offset[saocurr depth][ys][xs][i] obtained by decoding the offset information 01 are supplied to an offset
   information storage section 621 in association with the arguments (saocurr depth, ys, xs) and the arguments
   (saocurr depth, ys, xs, i), respectively.
   [0119]
              The QAOU structure decoding section 612 determines a QAOU split structure by decoding
10 sao_split flag[saocurr depth][ys][xs] included in the QAOU information, and supplies QAOU structure information
   indicating the determined QAOU split structure to the offset information storage section 621.
   [0120]
              The offset attribute setting section 613 determines the offset bit depth (also referred to as "SAO DEPTH) and
   the offset value range.    In this case, the offset bit depth is determined from the pixel bit depth (also referred to as
15 "PIC DEPTH") (not shown) input into the offset attribute setting section 613.         The pixel bit depth represents a range of
   pixel values forming an image input into the adaptive offset filter 60 as the bit width.      When the pixel bit depth is N bits,
   the pixel values take a range of 0 to 2 N-1.    For example, when the pixel bit depth is 8 bits, the pixel values may take
   range of 0 to 255.    If the image input into the adaptive offset filter 60 is a decoded image obtained by the video
   decoding device 1 or a locally decoded image obtained by the video coding device 2, the bit depth of the decoded image
20 or the locally decoded image is used.       The precision of an image input into the adaptive offset filter 60 and the precision
   of an image output from the adaptive offset filter 60 are represented by the pixel bit depth.
   [0121]
             As the pixel bit depth, a value determined by using syntax in the coded data #1 can be decoded and used.          For
   example, in the case of H. 264/AVC, the pixel bit depth can be determined by using bit depth lumaminus8 in a
25 sequence parameter set.      In another mode, the pixel bit depth may be determined for each unit area to be processed, in
                                                                    32

   which case, the pixel bit depth is decoded from the header of each unit area.           The pixel bit depth in each unit area may
   be included in the parameter information or the header in the coded data #1, which is used for decoding an image input
   into the adaptive offset filter 60, or may be included as part of the QAOU information.            If the pixel bit depth is included
   in the parameter information or the header, which is used for decoding an input image, it may be included in a picture
 5 parameter header, a slice header, an LCU, or a CU.          If the pixel bit depth is included in part of the QAOU information,
   it may be included in a leaf QAOU or in QAOU information of a predetermined level (for example, the highest level
   QAOU or the first level QAOU).          Concerning the coding of the pixel bit depth, it is preferable that it is coded as a
   difference value from eight bits.      If part of the QAOU information is used, instead of using the pixel bit depth, it is also
   suitable that a shift value itself is coded.    In this case, the shift value may be suitably coded only when it exceeds eight
10 bits.   The shift value is a bit shift amount which is necessary for performing inverse quantization.            As a parameter
   used for inverse quantization, a step value may be used instead of the shift value.          In this case, inverse quantization of
   an offset is performed by multiplying the offset by a step value, and quantization of an offset is performed by dividing
   the offset by a step value.
   [0122]
15            The offset bit depth is a value indicating the precision of an offset coded in the coded data #1.          The offset
   included in the coded data #1 is a quantized value, and this quantized offset is decoded, and is then subjected to inverse
   quantization and is transformed into the bit depth corresponding to the pixel bit depth by an offset determining unit 625,
   which will be discussed later.      Then, the inverse-quantized offset is added by an offset adder 626, which will be
   discussed later.   The offset bit depth has a value equal to or smaller than the pixel bit depth.         If the offset bit depth is
20 smaller than the pixel bit depth by k bits, it means that the value of a coded offset is a value quantized by an offset
   quantization step of 2k.    Transformation from the decoded bit depth into the pixel bit depth is performed by the offset
   determining unit 625, which will be discussed later.
   [0123]
              The offset attribute setting section 613 sets an offset value range by using the determined offset bit depth.           The
25 offset attribute setting section 613 further sets a shift value from the offset pixel bit depth and the offset value range.
                                                                      33

   The set offset value range and the set shift value are supplied to the offset information storage section 621 and the offset
   information decoding section 611.      The determination of the offset bit depth and the setting of the shift value are
   performed by using one of patterns SI through S6, which will be discussed later.         The setting of the offset value range
   is performed by using one of patterns C1 and C2, which will be discussed later.        The offset value range and the shift
 5 value, which are both used as attributes of an offset, are referred to as "offset attributes".   The adaptive offset filter
   processor 62 includes, as shown in Fig. 1, the offset information storage section 621, a QAOU controller 622, an offset
   type determining section 623, a classifying section 624, the offset determining section 625, and an offset adder 626.
   [0124]
              The offset information storage section 621 manages and stores the offset type specified for each QAOU and
10 specific values of offsets with respect to individual classes that can be selected for the specified offset type, on the basis
   of the QAOU structure information, saotype idx[saocurr depth] [ys] [xs], and saooffset[saocurr depth] [ys] [xs] [i].
   The offset information storage section 621 has a map memory and a list memory.
   [0125]
              In the map memory, a QAOU index assigned to each offset minimum unit (also referred to as "QAOMU: Quad
15 Adaptive Offset Minimum Unit"), which is determined by the split depth, is stored.          The QAOU index will be
   discussed later.    The map memory will now be described below with reference to part (a) of Fig. 6.           Part (a) of Fig. 6
   illustrates examples of QAOU indexes stored in the map memory, and shows QAOMUs having a split depth of three and
   forming the unit of processing (for example, an LCU) and QAOU indexes assigned to QAOMUs.                   In part (a) of Fig. 6,
   indexes 0 to 9 specify QAOUs in a simple manner without considering the QAOU split depth.               In the example shown in
20 part (a) of Fig. 6, a QAOU specified by QAOU index = I is indicated by QAOUI.             In part (a) of Fig. 6, the thin lines
   indicate QAOMU boundaries, while the thick lines indicate QAOU boundaries.
   [0126]
              As shown in part (a) of Fig. 6, QAOUO is constituted by four QAOMUs, and 0 is assigned to these four
   QAOMUs as the QAOU index.           QAOU3 is constituted by one QAOMU, and 3 is assigned to this QAOMU as the
25 QAOU index.       In this manner, in the map memory, QAOU indexes assigned to the individual QAOMUs are stored.
                                                                 34

   [0127]
              In the list memory, each QAOU index, the offset type associated with each QAOU index, and specific values of
   offsets with respect to classes that can be selected for this offset type are stored in association with each other. In the
   offset information storage section 621, offsets having a restricted range of values set by the offset attribute setting
 5 section 613 are stored.
   [0128]
              Part (b) of Fig. 6 illustrates an example of information stored in the list memory, and shows an offset type
   associated with each of the QAOU indexes 0 to 9 and offsets with respect to individual classes that can be selected for
   each offset type.     In part (b) of Fig. 6, "xxx" indicates specific numeric values that may be different from each other.
10 [0129]
              In part (b) of Fig. 6, "BO 1" indicates an offset type specified by saotypeidx=5.          "EO 1"indicates an offset
   type specified by saotypeidx=1.           In this manner, edge offsets, which are of an offset type specified by
   sao_typeidx=1, 2, 3, 4, are also indicated by EO_l, 2, 3, 4, respectively, and band offsets, which are of an offset type
   specified by saotypeidx=5, 6, are also indicated by BOl, 2, respectively.
15 [0130]
              As shown in part (b) of Fig. 6, when the offset type is a band offset, a total of sixteen offsets, that is, the offset
   1 through the offset 16, are stored in the list memory for this offset type.      The offset 1 through the offset 16 are values
   specified by saooffset[saocurr depth] [ys] [xs] [1] through sao offset[saocurr depth] [ys] [xs] [16], respectively, when
   the value of sao_typeidx[saocurr depth] [ys] [xs] is 5 or 6.          On the other hand, when the offset type is an edge offset,
20 a total of four offsets, that is, the offset 1 through the offset 4, are stored in the list memory for this offset type.  The
   offset 1 through the offset 4 are values specified by saooffset[saocurr depth] [ys] [xs] [1] through
   sao_offset[sao curr depth][ys][xs][4], respectively, when the value of saotypeidx[saocurr depth][ys][xs] is one of 1,
   2, 3, and 4.    No value is stored in the offset 5 through the offset 16.
   [0131]
25            The memory size of each offset stored in the list memory is determined by the offset value range supplied from
                                                                    35

   the offset attribute setting section 613.     When the offset value range is -24 to 24-41, each offset can be represented by five
   bits, and thus, a memory size having five bits is required.
   [0132]
             A QAOMU number is appended to each QAOMU, and the QAOMUs can be distinguished from each other by
 5 the QAOMU numbers.           Hereinafter, QAOMU having a QAOMU number NQ will also be referred to as "QAOMUNQ".
   [0133]
              QAOU indexes are QAOU block numbers which are serially specified when the split depth is 0 to the
   maximum.       If the maximum split depth is four, values of 0 to 340 are specified for all the blocks
   (1 +4+16+64+256=341) of all the split depths by the QAOU indexes, as shown in Fig. 7.
10 [0134]
              Since it is not necessary to store all the QAOU offsets of all the split depths in the offset information storage
   section 621 used in the video decoding device 1, there is no need to secure 341 memory areas, and instead, it is sufficient
   if the same number of memory areas as that of QAOUs which are actually used in the structure specified by the input
   QAOU information are provided.           When the maximum split depth is four, the number of blocks is 256 or smaller, and
15 thus, the provision of 256 map memories and 256 list memories is sufficient.          In this case, as the QAOU indexes used
   in the offset information storage section 621, unique indexes for identifying leaf QAOUs, for example, indexes from 0 to
   255 or smaller which are incremented by one every time a leaf QAOU is decoded, are used.               It is also possible to store,
   in the offset information storage section 621, a map list by using QAOU information corresponding to the maximum
   split depth as a unit.    In this case, 0 to 255 are used as the QAOU index numbers used in the offset information storage
20 section 621, and they correspond to QAOMU numbers 85 to 340 in Fig. 7.
   [0135]
              Parts (a) through (e) of Fig. 7 show examples of QAOMU numbers appended to QAOMUs included in the unit
   of processing (for example, an LCU).         Part (a) of Fig. 7 shows a QAOMU number assigned to a QAOMU having a
   split depth of 0.    Part (b) of Fig. 7 shows QAOMU numbers assigned to QAOMUs having a split depth of one.                  Part
25 (c) of Fig. 7 shows QAOMU numbers assigned to QAOMUs having a split depth of two.                  Part (d) of Fig. 7 shows
                                                                     36

   QAOMU numbers assigned to QAOMUs having a split depth of three.             Part (e) of Fig. 7 shows QAOMU numbers
   assigned to QAOMUs having a split depth of four.
   [0136]
             The QAOU controller 622 controls the individual components included in the adaptive offset filter processor
 5 62.   The QAOU controller 622 also refers to the QAOU structure information, and then splits the deblocked decoded
   image PDB into one or a plurality of QAOUs and scans the individual QAOUs in a predetermined order.               The QAOU
   controller 622 also supplies a QAOMU number representing a subject QAOMU to the offset-type determining section
   623.
   [0137]
10           The offset-type determining section 623 refers to the map memory and the list memory of the offset
   information storage section 621 and determines the offset type specified by the QAOMU number supplied from the
   QAOU controller 622.       The offset-type determining section 623 also supplies the determined offset type to the
   classifying section 624.
   [0138]
15           The classifying section 624 classifies each pixel included in the subject QAOU as one of the classes that can be
   selected for the offset type supplied from the offset-type determining section 623.     The classifying section 624 also
   supplies the offset type and the class index indicating the class of each pixel to the offset determining section 625.
   Specific classifying processing performed by the classifying section 624 will be discussed later, and thus, an explanation
   thereof is omitted here.
20 [0139]
             The offset determining section 625 refers to the list memory of the offset information storage section 621, and
   determines, for each pixel included in the subject QAOU, an offset specified by the offset type and the class index
   supplied from the classifying section 624.     The offset determining section 625 includes an offset reverse shifter (not
   shown) which performs bitwise left shift on an offset by an amount equal to a shift value set by the offset attribute
25 setting section 613.    The offset reverse shifter performs inverse quantization on the offset so that the offset bit depth
                                                                 37

   may match the pixel bit depth.     By performing such inverse quantization, in addition processing performed by the offset
   adder 626, which will be discussed later, the offset can be added to the pixel value by using the same bit depth.    The
   inverse-quantized offset for each pixel is supplied to the offset adder 626.
   [0140]
 5           The offset adder 626 adds the offset supplied from the offset determining section 625 to the associated pixel
   which forms the subject QAOU of the deblocked decoded image PDB.             The offset adder 626 outputs an image
   obtained by performing processing on all the QAOUs included in the deblocked decoded image PDB as an offset
   filtered decoded image POF.
   [0141]
10           Fig. 8 is a flowchart of a flow of processing performed by the adaptive offset filter processor 62.
   [0142]
   (Step S101)
             First, the QAOU controller 622 obtains QAOU structure information from the offset information storage
   section 621.
15 [0143]
   (Step S102)
             Then, the QAOU controller 622 starts a first loop using the QAOMU number of a subject QAOMU as a loop
   variable.
   [0144]
20 (Step S103)
             The QAOU controller 622 supplies the QAOMU number to the offset-type determining section 623.             Under
   the control of the QAOU controller 622, the offset-type determining section 623 reads an offset type specified by the
   QAOMU number supplied from the QAOU controller 622 from the map memory and the list memory of the offset
   information storage section 621.     The offset-type determining section 623 also supplies the read offset type to the
25 classifying section 624.
                                                                 38

   [0145]
   (Step S104)
              Then, the QAOU controller 622 starts a second loop using the pixel number of each pixel included in the
   subject QAOMU as a loop variable.        The pixel numbers are used for distinguishing pixels included in the subject
 5 QAOMU from each other, and numbers assigned to pixels included in the subject QAOMU in a predetermined scanning
   order, for example, may be used.      Instead of using the pixel numbers, x and y coordinates of each pixel included in the
   subject QAOMU may be used as loop variables.
   [0146]
   (Step S105)
10            Then, under the control of the QAOU controller 622, the classifying section 624 classifies the subject pixel as
   one of a plurality of classes that can be selected for the offset type supplied from the offset-type determining section 623.
   The classifying section 624 also supplies the offset type and the class index indicating the class of the subject pixel to the
   offset determining section 625.
   [0147]
15 (Step S106)
              Then, under the control of the QAOU controller 622, the offset determining section 625 reads an offset to be
   added to the subject pixel from the offset information storage section 621.      That is, the offset determining section 625
   reads the offset specified by the offset type and the class index supplied from the classifying section 624.     The offset
   determining section 625 also inverse-quantizes the offset by executing bitwise left shift on the offset determined for the
20 subject pixel by an amount equal to a shift value supplied from the offset attribute setting section 613.     The offset
   determining section 625 then supplies the inverse-quantized offset to the offset adder 626.
   [0148]
   (Step S107)
              Then, under the control of the QAOU controller 622, the offset adder 626 adds the offset supplied from the
25 offset determining section 625 to the pixel value of the subject pixel which forms the deblocked decoded image PDB.
                                                                  39

   [0149]
   (Step S108)
              This step is the termination of the second loop.
   [0150]
 5 (Step S109)
              This step is the termination of the first loop.
   [0151]
              If it is found in step S103 that the offset read by the offset-type determining section 623 is 0 (offset type=0), the
   QAOU controller 622 controls the offset adder 626 so that the offset adder 626 will not add an offset to any of the pixels
10 forming the subject QAOMU.
   [0152]
              If it is found in step S105 that the subject pixel is classified as class 0 (class index=0), the QAOU controller
   622 controls the offset adder 626 so that the offset adder 626 will not add an offset to the subject pixel.
   [0153]
15 (The Number of Bits Necessary for Storing Offset)
              The number of bits necessary for storing offsets (saooffset) will now be discussed below.           When the pixel bit
   depth is ten bits and the offset bit depth is nine bits, the offset may take a range of values of -29 to 29-1, and the number
   of bits per offset is as large as ten bits.  If an offset having such a large number of bits is decoded, it is necessary that
   the offset information storage section 621 have the following maximum memory size for storing offsets for each picture:
20            (the total number of QAOMUs per picture) x (the number of classes) x (the number of bits per offset) = 256 x
   16 x 10 (bits)      40960 (bits).
   Although the total number of QAOMUs per picture is 256 in the decoding device, 341 QAOMUs are used in a coding
   device, which will be discussed later, and thus, an even larger memory is required.          In this manner, if the value range
   that can be taken with the offset bit depth is used without restricting the offset value range, a large memory size is
25 required for storing offsets since each offset has a large number of bits.
                                                                   40

   [0154]
   (Relationship between SAODEPTH and Coding Efficiency)
              SAODEPTH and PIC_DEPTH are closely related to each other in terms of quantization errors.                 The bit
   depth of an image output from the adaptive offset filter 60 is represented by the pixel bit depth PIC_DEPTH, and
 5 SAO DEPTH represents the bit depth of an offset to be added to a pixel.           Accordingly, even if an offset having a higher
   precision than that of the pixel bit depth is used, it is discarded in the output process.   It is thus preferable that
   SAODEPTH, which indicates the offset precision, is set to be equal to or lower than PIC_DEPTH.                 On the other hand,
   if SAO DEPTH is lower than PIC_DEPTH, an input image is corrected merely with a precision lower than the precision
   (PIC_DEPTH) with which the input image could be corrected by using a filter, thereby decreasing the filtering effect.
10 [0155]
              In contrast, if the offset precision SAODEPTH is higher, the amount of data required to code offsets increases.
   Generally, as is seen from the fact that the coding efficiency is optimized by minimizing the rate-distortion cost D + XR
   represented by using the amount of data (rate) R required for coding, the distortion D of an input image, and the weight
   2, the offset precision decreases the distortion D and increases the rate R.      Thus, concerning the level of precision, there
15 is a tradeoff between a decrease in the distortion D and an increase in the rate R in which a specific optimal value of the
   precision is provided.
   [0156]
              Further, in this embodiment, by restricting a quantized offset to an offset value range that can be represented by
   a certain bit width, it is possible to reduce the bit width for storing a quantized offset in the offset information storage
20 section 621.    With this arrangement, the effect of reducing the memory size can be obtained, compared with the above
   described restriction is not imposed.       However, if the offset value range is excessively restricted, the effect of correcting
   distortion of a decoded image by using an offset is decreased, and it is not possible to remove distortion of a decoded
   image in offset addition processing, which may decrease the coding efficiency.         Thus, it is preferable that the offset
   value range is set in an optimal range in such a degree as not to decrease the coding efficiency.
25 [0157]
                                                                   41

             In this embodiment, the offset attribute setting section 613 sets the offset bit depth and the offset shift value by
   using one of the following patterns SI through S6 and sets the offset value range by using one of the following patterns
   C1 through C3.
   [0158]
 5 (Pattern SI)
             In pattern Si, as shown in part (a) of Fig. 9, the offset bit depth SAODEPTH is set to be equal to the pixel bit
   depth PIC_DEPTH.        Since the maximum value of the offset precision is equal to the pixel bit depth, an offset is coded
   with the maximum precision in the pattern 51.
   [0159]
10 (Pattern S2)
             In pattern S2, as shown in part (b) of Fig. 9, when PIC_DEPTH is ten bits or smaller, SAODEPTH is set to be
   equal to PIC_DEPTH, and when PIC_DEPTH is eleven bits or greater, SAODEPTH is set to be ten bits.                  In pattern S2,
   the upper limit of the offset bit depth is set to be ten bits. The inventors have found that, when the quantization step QP
   of a decoded image is small (when the bit rate is high), the coding efficiency becomes higher if the offset bit depth is
15 equal to the pixel bit depth than a case in which the offset bit depth is smaller than the pixel bit depth, and that,
   conversely, when the quantization step QP is large, the coding efficiency becomes higher if the offset bit depth is smaller
   than the pixel bit depth than a case in which the offset bit depth is equal to the pixel bit depth.   It has been found
   through the inventors' experiments that, when the quantization parameter qp ranged from 12 to 27, the coding efficiency
   was improved if the offset bit depths were determined as in pattern S2, compared with a case in which the offset bit
20 depth was eight bits when the pixel bit depth was nine bits or smaller and the offset bit depth was nine bits when the
   pixel bit depth was ten bits or greater.   Accordingly, in pattern S2, by changing the dependency of the offset bit depth
   on the pixel bit depth in the boundary of ten bits, the amount of data required to code offsets can be decreased, compared
   with a case in which the offset bit depth is equal to the pixel bit depth, as in pattern S1, thereby achieving a high coding
   efficiency.
25 [0160]
                                                                  42

   (Pattern S3)
              In pattern S3, as shown in part (c) of Fig. 9, when PIC_DEPTH is nine bits or smaller, SAODEPTH is set to
   be equal to PIC DEPTH, and when PIC_DEPTH is ten bits or greater, SAODEPTH is set to be nine bits.                    In pattern
   S3, the upper limit of the offset bit depth is set to be nine bits.     In pattern S3 as well as in pattern S2, the amount of data
 5 required to code offsets can be decreased, thereby achieving a high coding efficiency.
   [0161]
   (Pattern S4)
              In pattern S4, as shown part (d) of Fig. 9, when PIC_DEPTH is ten bits or smaller, SAODEPTH is set to be
   equal to PIC_DEPTH, and when PIC_DEPTH is eleven bits or greater, SAODEPTH is set to be 10
10 floor((PICDEPTH-10)/STEP).            The function floor(x) is a function that provides the largest integer not greater than x.
   In pattern S4, when the pixel bit depth is ten bits or greater, every time the pixel bit increase (decreases) by STEP bits,
   the offset bit depth increases (decreases) by one bit.       Part (e) of Fig. 9 shows a case in which STEP is two in pattern S4,
   and every time the pixel bit depth increases by two bits, the offset bit depth increases by one bit.        With this
   configuration, it is possible to more flexibly respond to the level of the bit rate than in pattern S2 or S3 while also
15 considering the level of the pixel bit depth.
   [0162]
              For all of pattern SI through pattern S4, the shift value is represented by the difference value PIC_DEPTH
   SAODEPTH between PIC_DEPTH and SAODEPTH.                           By changing the offset bit depth and the shift value as in the
   above-described pattern SI through S4, the offset bit depth and the shift value can be set without increasing the memory
20 size or the amount of processing, thereby making it possible to improve the coding efficiency.
   [0163]
   (Pattern S5)
              In pattern S5, the offset bit depth is explicitly coded.     More specifically, the difference between the offset bit
   depth and a predetermined value is coded.        It is suitable that the predetermined value is eight or the pixel bit depth.   If
25 the predetermined value is eight, SAODEPTH - 8 is coded.              If the predetermined value is the pixel bit depth,
                                                                     43

   PIC_DEPTH - SAO DEPTH is coded.               The offset bit depth may be safely coded as various items of parameter
   information or the header of coded data or as part of QAOU information.            If the bit depth is coded as part of QAOU
   information, it may be safely included in a leaf QAOU or in QAOU information of a predetermined level (for example,
   the highest level QAOU or the first level QAOU).         By coding the bit depth as part of coded data, the bit depth can be
 5 set to be an optimal value in the decoding device and in the coding device, thereby obtaining the effect of maximizing
   the coding efficiency.    If the bit depth is coded as QAOU information, the bit depth is changed in accordance with the
   QAOU depth saocurr depth, thereby making it possible to reduce the memory size for storing offsets.                It is likely that
   many offsets will appear when saocurr depth is large.          Accordingly, by setting the bit depth to be small when
   saocurr depth is large and by setting the bit depth to be large when sao curr depth is small, it is possible to reduce the
10 required memory size.      For example, it is suitable that, when sao_curr depth=0 to 1, the offset bit depth is set to be
   equal to the pixel bit depth (pattern SI) and when saocurr depth=2 to 4, the upper limit is set for the offset bit depth
   (pattern S2).    The bit width is coded in this manner.     It is also suitable that a flag indicating whether the offset bit
   width will be coded in response to the QAOU depth or only one offset bit width will be used regardless of the QAOU
   depth may be coded, thereby switching between a case in which the offset bit width is coded in response to the QAOU
15 depth and a case in which only one offset bit width is used regardless of the QAOU depth.
   [0164]
   (Pattern S6)
              Without coding the bit depth explicitly, the bit depth is determined in accordance with saocurr depth.             For
   example, it is suitable that, when saocurrdepth=0 to 1, the offset bit depth is set to be equal to the pixel bit depth
20 (pattern SI), and when saocurr depth=2 to 4, the upper limit is set for the offset bit depth (pattern S2).
   [0165]
   (Pattern C1)
              In pattern C1, the offset value range is set in accordance with SAODEPTH.              Hereinafter, the maximum bit
   length representing the offset value range is indicated by CLIPBIT.           More specifically, by calculating CLIPBIT =
25 SAO DEPTH - K, the offset value range is determined to be -2 CLIPBIT-1 to 2 CLIP-BIT-11.           It has been found through the
                                                                    44

   inventors' experiments that K=4 is suitable.      That is, it has been validated that, in the case of K=4, the coding efficiency
   was not decreased even if the offset range was restricted by the offset value range.        K=4 is suitable when the pixel bit
   depth is eight, which is most commonly used.         When the pixel bit depth is eight, the offset bit depth SAODEPTH is
   also eight, and thus, CLIPBIT=8-K=4.          If one offset can be stored by using four bits, in software, for example, which
 5 handles one byte constituted by eight bits as the unit, one offset can be packed and stored in one byte, thereby easily
   reducing the memory size.
   [0166]
   (Pattern C2)
              In pattern C2, the offset value range is set independently of SAODEPTH.             More specifically, CLIP BIT is
10 set to be eight, and the offset value range is set to be -27 to 27-1.
   [0167]
             Alternatively, generally, a constant N which is independent of SAODEPTH may be used, and CLIPBIT may
   be set to be N.    When the offset value range is set independently of SAODEPTH, it is preferable that the offset value
   range is set to be smaller than the offset bit depth in order to achieve the effect of reducing the memory size.
15 [0168]
   (Pattern C3)
              In pattern C3, the offset value range is determined in accordance with the QAOU hierarchical depth.          It is
   suitable that, when saocurrdepth is small (for example, 0 to 1), the offset value range is determined independently of
   the offset bit depth, and when sao curr depth is large (for example, 2 to 4), the offset value range is determined in
20 accordance with the offset bit depth.     If the offset value range is determined independently of the offset bit depth,
   CLIP BIT may be set to be eight (pattern C2).        If the offset value range is determined in accordance with the offset bit
   depth, CLIPBIT may be set to be SAODEPTH-K bits (pattern Cl).                 If the offset bit length is determined in
   accordance with the QAOU hierarchical depth, a fixed bit number may be safely used for CLIPBIT.                 For example,
   CLIP BIT=4 is suitable.
25 [0169]
                                                                    45

   (First Specific Example of the Number of Offset Bits)
              A first specific example of the number of bits of an offset (saooffset) according to this embodiment will be
   discussed below.       In this example, a case in which the pixel bit depth is set to be ten bits, and the shift value is set by
   using pattern S2 and the offset value range is set by using pattern C1 will be discussed.        In pattern S2, the offset bit
 5 depth is set to be ten bits, and, in pattern C1, the offset value range is set to be 10-4=6 bits.      If the number of bits per
   offset is six bits, it means that the offset values are restricted to -32 to 31.  In this example, as the memory size for
   storing offsets in the offset information storage section 621, the following maximum size for each picture is sufficient:
              (the total number of QAOMUs per picture) x (the number of classes) x (the number of bits per offset)           = 256 x
   16 x 6 (bits)  = 24576 (bits).
10 [0170]
              Thus, with the configuration in which the offset value range is set as in this example, the memory size
   necessary for the offset information storage section 621 can be reduced to substantially 3/5 of the memory size of an
   example of the related art.
   [0171]
15            Since the amount of data required to code offsets included in the coded data #1 can be reduced, it is possible to
   improve the coding efficiency.       Additionally, since excessive offsets are not added, a suitable level of image quality can
   be guaranteed.
   [0172]
   (Second Specific Example of the Number of Offset Bits)
20            A second specific example of the number of bits of an offset (saooffset) according to this embodiment will be
   discussed below.       In this example, a case in which the pixel bit depth is set to be ten bits, and the shift value is set by
   using pattern S2 and the offset value range is set by using pattern C2 will be discussed.         In pattern S2, the offset bit
   depth is set to be ten bits, and, in pattern C2, the offset value range is set to be eight bits.    If the number of bits per
   offset is eight bits, it means that the offset values are restricted to -128 to 127.   In this example, as the memory size for
25 storing offsets in the offset information storage section 621, the following maximum size for each picture is sufficient:
                                                                    46

             (the total number of QAOMUs per picture) x (the number of classes) x (the number of bits per offset) = 256 x
   16 x 8 (bits)    32768 (bits).
   [0173]
             Thus, with the configuration in which the number of bits of an offset is set as in this example, the memory size
 5 necessary for the offset information storage section 621 can be reduced to substantially 4/5 of the memory size of an
   example of the related art.
   [0174]
             Since the amount of data required to code offsets included in the coded data #1 can be reduced, it is possible to
   improve the coding efficiency.     Additionally, since excessive offsets are not added, a suitable level of image quality can
10 be guaranteed.
   [0175]
             If the number of offset bits is restricted, the amount of data required to code offset information included in the
   coded data #1 can be reduced.      However, if the number of offset bits is excessively restricted, the effect of providing an
   adaptive offset filter is decreased, thereby increasing the amount of data required to code residual data (the pixel value of
15 a residual image) included in the coded data.
   [0176]
   (Third Specific Example of the Number of Offset Bits)
             A third specific example of the number of bits of an offset (saooffset) will be discussed below.         In any one of
   the patterns C1 and C2 of this example, the value to be set for the number of bits of an offset is changed depending on
20 whether the offset type is an edge offset (offset type is one of I to 4) or a band offset (offset type is 5 or 6).   In this
   example, a method for setting the shift value by using pattern S2 and for setting the offset value range of an edge offset
   by using pattern C2 and the offset value range of a band offset by using pattern C1 when the pixel bit depth is set to be
   ten bits will be discussed.
   [0177]
25           When the pixel bit depth is ten bits, in pattern S2, the offset bit depth is set to be ten bits. The value range of
                                                                 47

   an offset belonging to the edge offset type (hereinafter referred to as an "edge-offset offset") is set to be eight bits in
   pattern C2.     The value range of an offset belonging to the band offset type (hereinafter referred to as a "band-offset
   offset") is set to be six bits in pattern Cl.   More generally, when the number of bits of an edge-offset offset is N bits and
   the number of bits of a band-offset offset is M bits, the numbers of bits of offsets are determined so that the condition
 5 N M can be satisfied.
   [0178]
              The memory size required to be secured in the offset information storage section 621 is the number of bits
   represented by (the number of classes of an offset type x the number of offset bits) for each QAOU.          Accordingly, the
   number of bits of a band offset, which has a greater number of classes than an edge offset, is set to be smaller, thereby
10 making it possible to effectively utilize the memory area for storing offsets in the offset information storage section 621.
   [0179]
              In this manner, by varying the number of bits of an offset in accordance with the offset type, the coding
   efficiency can be improved without requiring an excessive memory size of the offset information storage section 621.
   The memory area can also be utilized most effectively.
15 [0180]
              When the threshold th for restricting values that can be taken as an offset is greater than 2 '" but not greater
   than 2'", m-bit fixed-length coding/decoding method may be used as the coding method for coding the offset.            The
   variable-length coding/decoding method, such as Truncated unary coding or Truncated Rice coding having th as the
   maximum value, may be used.           The above-described maximum value th is determined by the offset value range
20 supplied from the offset attribute setting section 613.     The video decoding device 1 is capable of decoding offsets
   which have been coded as described above.
   [0181]
              With the above-described configuration, in the offset attribute setting section 613, the offset bit depth, the offset
   value range, and the shift value are set.     In the adaptive offset filter information decoder 61, a quantized offset having a
25 value within the offset value range is decoded and is stored in the offset information storage section 621 in which a
                                                                   48

   storage area having a bit width equal to or larger than the offset value range is secured for each offset.       In this
   embodiment, it is characterized in that the offset value range is determined in accordance with the offset bit depth.         The
   offset bit depth is determined in accordance with the pixel bit depth.      Accordingly, in this embodiment, it is also
   characterized in that the offset bit depth is determined in accordance with the pixel bit depth.
 5 [0182]
              The adaptive offset filter information decoder 61 of the first embodiment may include a storage section for
   storing decoded offsets therein and an inverse-quantizing section which inverse-quantizes offsets obtained from the
   storage section.     Inverse-quantization processing performed by the offset determining section 625 may be omitted.           In
   this case, it is characterized in that the storage section stores an offset which is restricted to the offset value range set by
10 the offset attribute setting section 613 and in that the inverse-quantizing section performs inverse-quantization
   processing by performing bitwise left shift on an offset in accordance with the shift value set by the offset attribute
   setting section 613.
   [0183]
              The offset information decoding section 611 decodes, from the coded data #1, each offset to be referred to by
15 the offset adder 626, which adds an offset to the pixel value of each pixel forming an input image constituted by a
   plurality of unit areas.    In other words, the offset information decoding section 611 includes offset decoding means (not
   shown) for decoding an offset having an offset value range and a shift value which are set in accordance with the pixel
   bit depth and which is restricted to the offset value range.
   [0184]
20            The adaptive offset filter 60 of this embodiment is an image filtering device which adds an offset to the pixel
   value of each pixel forming an input image constituted by a plurality of unit areas.        In other words, the adaptive offset
   filter 60 may be an image filtering device including the offset attribute setting section 613 that refers to offset-type
   specifying information included in coded data and sets offset attributes in a subject unit area, the offset information
   decoding section 611 that decodes an offset having a bit width corresponding to an offset value range included in the
25 offset attributes set as described above, and the offset adder 626 that adds the offset to the pixel value of each pixel
                                                                   49

   forming the input image.
   [0185]
             In addition to the above-described offset decoding means, the offset information decoding section 611 may
   include determining means for determining the offset type to which a subject unit area belongs from among a plurality of
 5 offset types, and offset decoding means for decoding offsets having different bit widths depending on the offset type
   determined by the determining means.
   [0186]
             The above-described offset-type specifying information includes information concerning the bit depth of pixel
   values in each unit area of the input image, and the offset information decoding section 611 may decode an offset having
10 a bit width corresponding to the bit depth of the pixel values.
   [0187]
             A description will now be given of specific examples of classifying processing performed by the classifying
   section 624.    It is preferable that the classifying section 624 performs, among the following examples of classifying
   processing, classifying processing corresponding to classifying processing performed by the video coding device that
15 has generated the coded data #1.
   [0188]
   (First Example of Classifying Processing of Classifying Section 624)
             A first example of classifying processing performed by the classifying section 624 will be described below with
   reference to parts (a) through (f) of Fig. 10 through Fig. 12.
20 [0189]
   (When Offset Type is one of 1 to 4 (Edge Offset))
             When the offset type supplied from the offset-type determining section 623 is one of 1 to 4, the classifying
   section 624 determines whether or not there is an edge near a subject pixel and, if there is an edge, determines the type
   of edge.   The classifying section 624 then classifies the subject pixel as one of a plurality of classes in accordance with
25 the determination results.
                                                                   50

   [0190]
              More specifically, the classifying section 624 first determines the sign of the difference between the pixel value
   pic[x] of a subject pixel x and each of the pixel values pic[a] and pic[b] of two pixels a and b which are adjacent to the
   subject pixel or which have the same vertex as the subject pixel.     That is, the classifying section 624 calculates:
 5            Sign(pic[x] - pic[a]) and
              Sign(pic[x] - pic[b]):
   where Sign(z) is a function which takes the following values:
              Sign(z) - +1        (when z > 0)
              Sign(z) = 0         (when z = 0)
10            Sign(z) = -1        (when z < 0).
   Specifically, a determination as to which pixels are used as the pixel a and the pixel b is dependent on the offset type,
   and is made as follows.
   [0191]
              - When offset type is 1 (saotype idx=1)
15            As shown in part (a) of Fig. 10, the pixel positioned immediately on the left side of the subject pixel x is set to
   be the pixel a, and the pixel positioned immediately on the right side of the subject pixel x is set to be the pixel b.
   [0192]
              - When offset type is 2 (saotype idx=2)
              As shown in part (b) of Fig. 10, the pixel positioned immediately on the top side of the subject pixel x is set to
20 be the pixel a, and the pixel positioned immediately on the bottom side of the subject pixel x is set to be the pixel b.
   [0193]
              - When offset type is 3 (saotype idx=3)
              As shown in part (c) of Fig. 10, the pixel having the same vertex as that of the top left side of the subject pixel
   x is set to be the pixel a, and the pixel having the same vertex as that of the bottom right side of the subject pixel x is set
25 to be the pixel b.
                                                                  51

   [0194]
              - When offset type is 4 (saotype idx=4)
             As shown in part (d) of Fig. 10, the pixel having the same vertex as that of the bottom left side of the subject
   pixel x is set to be the pixel a, and the pixel having the same vertex as that of the top right side of the subject pixel x is
 5 set to be the pixel b.
   [0195]
              Part (a) of Fig. 11 shows graphs indicating the magnitude relations between the pixel value pic[x] of the subject
   pixel x and the pixel value of the pixel a or b and also shows the values of the function Sign in accordance with the
   magnitude relation.      In the graphs shown in part (a) of Fig. 11, the black circle with pic[x] indicates the pixel value of
10 the subject pixel x, and the black circle without pic[x] indicates the pixel value of the subject pixel a or b.     The vertical
   direction in the graphs shown in part (a) of Fig. 11 indicates the magnitude of the pixel value.
   [0196]
              Then, the classifying section 624 finds EdgeType according to the following mathematical equation (1-1) on
   the basis of Sign(pic[x]-pic[a]) and Sign(pic[x]-pic[b]).
15 [0197]
   EdgeType = Sign(pic[x]-pic[a]) + Sign(pic[x]-pic[b]) + 2
                                             ... (1-1)
              Part (b) of Fig. 11 shows graphs indicating the magnitude relations between the pixel value of the subject pixel
   x and each of the pixel values of the pixel a and the pixel b and also shows the values of EdgeType in accordance with
20 the magnitude relation.      In part (b) of Fig. 11, the center black circle in each graph indicates the pixel value of the
   subject pixel x, and the black circles on both sides indicate the pixel values of the pixel a and the pixel b.      The vertical
   direction in the graphs shown in part (b) of Fig. 11 indicates the magnitude of the pixel value.
   [0198]
              Then, the classifying section 624 determines as follows, on the basis of the determined EdgeType, the class
25 index (classidx) of the class to which the subject pixel x belongs.
                                                                    52

   [0199]
             class_idx = EoTbl[EdgeType]
   where EoTbl[EdgeType] is a transform table used for determining class_idx from EdgeType.              A specific example of the
   transform table EoTbl is shown in part (d) of Fig. 11.
 5 [0200]
             As shown in part (d) of Fig. 11, when there is no edge in an area constituted by the subject pixel x, the pixel a,
   and the pixel b (hereinafter, such a case will also be referred to as a case in which an area is flat), the subject pixel x is
   classified as class 0 (class idx=0).   Part (c) of Fig. 11 shows the association between each graph shown in part (b) of
   Fig. 11 and classidx.
10 [0201]
   (When Offset Type is 5 or 6 (Band Offset))
             When the offset type supplied from the offset-type determining section 623 is 5 or 6, the classifying section
   624 classifies the pixel value of the subject pixel x as one of a plurality of classes in accordance with the pixel value
   pic[x] of the subject pixel x.
15 [0202]
             - When offset type is 5 (sao type idx=5)
             When the pixel value pic[x] of the subject pixel x satisfies: (max x 1/4)      pic[x]    (max x 3/4), the classifying
   section 624 classifies the subject pixel as a class other than 0.   That is, when the pixel value of the subject pixel is
   contained within a range indicated by the hatched portion in part (a) of Fig. 12, the classifying section 624 classifies the
20 subject pixel as a class other than 0.  In the above-described condition, max indicates the maximum value that can be
   taken by the subject pixel x, and for example, max=255.       When max=255, the above-described condition may be
   represented by 8     pic[x]/8   23.
   [0203]
             - When offset type is 6 (saotype idx=6)
25           When the pixel value pic[x] of the subject pixel x satisfies: pic[x]     (max  x  1/4) or (max x 3/4)   pic[x], the
                                                                 53

   classifying section 624 classifies the subject pixel as a class other than 0.  That is, when the pixel value of the subject
   pixel is contained within a range indicated by the hatched portion in part (b) of Fig. 12, the classifying section 624
   classifies the subject pixel as a class other than 0.   In the above-described condition, max indicates the maximum value
   that can be taken by the subject pixel x, and for example, max=255.       When max=255, the above-described condition
 5 may be represented by (pic[x]/8) s 7 or 24       (pic[x]/8).
   [0204]
              The classifying processing performed by the classifying section 624 will be more specifically described below.
   [0205]
              When the offset type is 5 or 6, the classifying section 624 determines the class index (classidx) of the class to
10 which the subject pixel x belongs as follows.
   [0206]
              class_idx = EoTbl[saotypeidx][pic[x]/8]
   where EoTbl[saotypeidx][pic[x]/8] is a transform table used for determining class_idx from the pixel value pic[x] of
   the subject pixel x and saotype idx.       A specific example of the transform table EoTbl is shown in Fig. 12.    In part (c)
15 of Fig. 12, "BO 1" indicates that saotype index=5, and "BO 2" indicates that sao_type index=6.
   [0207]
              As shown in part (c) of Fig. 12, when saotypeindex=5, the classifying section 624 classifies the subject pixel
   x as one of class indexes I to 16 in accordance with the magnitude of pic[x] if the pixel value pic[x] of the subject pixel
   x satisfies the condition 8    (pic[x]/8)   23.
20 [0208]
              When saotypeindex=6, the classifying section 624 classifies the subject pixel x as one of class indexes I to
   16 in accordance with the magnitude of pic[x] if the pixel value pic[x] of the subject pixel x satisfies the condition
   (pic[x]/8)    7 or 24   (pic[x]/8).
   [0209]
25            Generally, when the bit depth of an image is PIC_DEPTH, the maximum value of the subject pixel can be
                                                                   54

   represented by max= 2 PICDEPTH-1, and thus, classifying is performed by using pic/ 2 (PICDEPTH-5 ) instead of pic/8 in part (c)
   of Fig. 12.
   [0210]
   (Second Example of Classifying Processing of Classifying Section 624)
 5           A second example of classifying processing performed by the classifying section 624 will be described below.
   [0211]
              In this example of the processing, the classifying section 624 determines EdgeType by using the following
   mathematical expression (1-2) instead of mathematical expression (1-1).       The other features are similar to those of the
   first example of classifying processing.
10 [0212]
              EdgeType = Sign((pic[x]>>shift) - (pic[a]>>shift))
   + Sign((pic[x]>>shift) - (pic[b]>>shift)) + 2
                                                                                     (1-2)
   where "     " denotes bitwise right shift, and "shift" denotes the magnitude of bit shift.  The specific value of "shift" may
15 be determined such that it has a positive correlation with the bit depth of the pixel value.
   [0213]
              In the classifying processing of the first example, even when the gradient of the pixel value is very small, if it is
   not 0, the value of Sign is not 0.   Accordingly, in the classifying processing of the first example, the value of EdgeType
   is vulnerable to the influence of noise.
20 [0214]
              In this example of the processing, after performing bitwise right shift on the pixel value, the difference between
   the pixel values is calculated.    Thus, the value of EdgeType is less vulnerable to the influence of noise, thereby
   achieving the effect of improving the coding efficiency.
   [0215]
25            In this example of the processing, the following mathematical expression (1-3) may be used instead of
                                                                  55

   mathematical expression (1-2).
   [0216]
             EdgeType = Sign((pic[x]-pic[a])>>shift) + Sign((pic[x]-pic[b])>>shift)) + 2
                                                                                   ... (1-3)
 5           That is, after the difference between the pixel values is calculated, the bitwise right shift may be performed.
   By using mathematical expression (1-3), advantages similar to obtained by using mathematical expression (1-2) can be
   achieved.
   [0217]
   (Third Example of Classifying Processing of Classifying Section 624)
10           A third example of classifying processing performed by the classifying section 624 will be described below.
   [0218]
             In this example of the processing, the definition of the function Sign discussed in the first example of the
   classifying processing calculated by the classifying section 624 is changed as follows.      The other features are similar to
   those of the first example of the classifying processing.
15 [0219]
             Sign(z) - +1         (when z > th)
             Sign(z) = 0          (when -th   z  th)
             Sign(z) = -1         (when z < -th)
   where th denotes a threshold having a predetermined value.       The specific value of the threshold th may be determined
20 such that the absolute value of the threshold th has a positive correlation with the bit depth of the pixel value.
   [0220]
             In this example of the processing, too, the value of EdgeType is less vulnerable to the influence of noise,
   thereby achieving a high coding efficiency.
   [0221]
25 (Fourth Example of Classifying Processing of Classifying Section 624)
                                                                 56

              A fourth example of classifying processing performed by the classifying section 624 will be described below.
   [0222]
              In this example of the processing, the classifying section 624 utilizes EoTbl[saotypeidx] [pic[x]/8] shown in
   Fig. 13 instead of EoTbl[saotypeidx] [pic[x]/8] shown in part (c) of Fig. 12.
 5 [0223]
              As shown in Fig. 13, in this example of the processing, when the value of pic[x]/8 is 8 or 9, the subject pixel x
   is classified as a class having a class index other than 0, regardless of whether saotype index=5 or saotype index=6.
   Moreover, when the value of pic[x]/8 is 22 or 23, the subject pixel x is classified as a class having a class index other
   than 0, regardless of whether saotypeindex=5 or saotype index=6.
10 [0224]
              In this example of the processing, when the pixel value of a subject pixel is 15 or smaller (when pic[x]/8 is 0 or
   1), the subject pixel is clipped to MIN.     Moreover, when the pixel value of a subject pixel is 240 or greater (when
   pic[x]/8 is 30 or 31), the subject pixel is clipped to MAX.     As MIN and MAX, one of the following combinations is
   preferably used.
15              MIN = 15, MAX = 240
              - MIN = 16, MAX = 239
              - MIN = 16, MAX = 235
   [0225]
              In the first example of the classifying processing, a subject pixel which is classified as class 0 when
20 sao_typeindex=5 is classified as a class other than 0 when saotype index=6.          Moreover, a subject pixel which is
   classified as class 0 when sao_type index=6 is classified as a class other than 0 when saotype index=5.
   [0226]
              Accordingly, in the first example of the classifying processing, the pixel value to which an offset is added may
   be very different depending on whether sao_typeindex=5 or saotypeindex=6, which may lead to a problem in that the
25 coding efficiency is not sufficiently improved as expected.      Such a problem may be noticeable when pic[x]/8 of the
                                                                  57

   pixel value of an image which has not yet been subjected to offset filtering is 8 or 9 or 22 or 23.
   [0227]
             In this example of the processing, when pic[x]/8 is 8 or 9, the subject pixel x is classified as a class other than 0,
   regardless of whether sao type index=5 or saotypeindex=6.             Moreover, when pic[x]/8 is 22 or 23, the subject pixel x
 5 is classified as a class other than 0, regardless of whether sao typeindex=5 or saotype index=6.           Accordingly, it is
   unlikely that the above-described problem will occur.       Thus, by performing processing of this example, the coding
   efficiency can be improved.
   [0228]
              In this example of the processing, in a case in which the value of pic[x]/8 is 8 or 9 and in a case in which the
10 value of pic[x]/8 is 22 or 23, the subject pixel x is classified as a class other than 0, regardless of whether
   sao_typeindex=5 or saotypeindex=6.             However, this is not a limitation to implement this example of the processing.
   In short, processing is performed such that, when the value of pic[x]/8 is within a predetermined range, the subject pixel
   x is classified as a class other than 0, regardless of whether sao type index=5 or sao type index=6.
   [0229]
15            Generally, when the pixel bit depth of an image is PIC_DEPTH, the maximum value of the subject pixel can be
   represented by max=2 PIC-DEPTH- 1, and thus, classifying is performed by using pic/2 (PICDEPTH-5) instead of pic/8 in Fig. 13.
   [0230]
              In this manner, when the pixel value of the above-described subject pixel is within a predetermined range, the
   classifying section 624 of this example may classify the subject pixel as an offset class in which an offset will be added,
20 regardless of whether the offset type of a unit area containing the subject pixel is the above-described first offset type or
   the above-described second offset type.
   [0231]
   (Video Coding Device 2)
              A description will now be given below, with reference to Figs. 14 through parts (a) to (d) of Fig. 18, of the
25 video coding device 2 which generates coded data #1 by coding a subject image.            The video coding device 2 partially
                                                                   58

   includes a method defined in H. 264/MPEG-4. AVC, a method used in KTA software, which is a joint development
   codec in VCEG (Video Coding Expert Group), a method used in TMuC (Test Model under Consideration) software,
   which is a successor codec to the codec used in KTA software, and a technology used in HM (HEVC TestModel)
   software.
 5 [0232]
             Fig. 14 is a block diagram illustrating the configuration of the video coding device 2 of this embodiment.   The
   video coding device 2 includes, as shown in Fig. 14, a transform-and-quantize unit 21, a variable-length code coder 22,
   an inverse-quantize-and-inverse-transform unit 23, a buffer memory 24, an intra-prediction image generator 25, an inter
   prediction image generator 26, a motion-vector detector 27, a prediction-method controller 28, a motion-vector
10 redundancy eliminating unit 29, an adder 31, a subtractor 32, a deblocking filter 33, an adaptive filter 70, and an
   adaptive offset filter 80.  The video coding device 2 codes a video image #10 (subject image) so as to generate coded
   data #1.
   [0233]
             The transform-and-quantize unit 21 performs: (1) DCT (Discrete Cosine Transform) transform on each block
15 of a prediction residual D obtained by subtracting a prediction image Pred from a subject image, (2) quantizing a DCT
   coefficient obtained by performing DCT transform, and (3) supplying a quantized prediction residual QD obtained by
   performing quantization to the variable-length code coder 22 and the inverse-quantize-and-inverse-transform unit 23.
   The transform-and-quantize unit 21 also performs: (1) selecting a quantization step QP to be used for quantization for
   each tree block; (2) supplying a quantization parameter difference Aqp representing the level of the selected quantization
20 step QP to the variable-length code coder 22; and (3) supplying the selected quantization step QP to the inverse
   quantize-and-inverse-transform unit 23.     The quantization parameter difference Aqp is a difference value obtained by
   subtracting the value of the quantization parameter qp' concerning a tree block which has been subjected to DCT
   transform and quantization immediately before a subject tree block from the quantization parameter qp (QP=      2p/6)
   concerning the subject tree block to be subjected to DCT-transform and quantization.
25 [0234]
                                                                59

              The variable-length code coder 22 generates coded data #1 by performing variable-length coding on: (1) the
   quantized prediction residual QD and Aqp supplied from the transform-and-quantize unit 21; (2) a quantization
   parameter PP supplied from the prediction-method controller 28, which will be discussed later; and (3) a filter set
   number, a filter coefficient group, area specifying information, and ON/OFF information supplied from the adaptive
 5 filter 70, which will be discussed later.   The variable-length code coder 22 also codes QAOU information supplied from
   the adaptive offset filter 80 and inserts the coded QAOU information into the coded data #1.
   [0235]
              The inverse-quantize-and-inverse-transform unit 23 performs: (1) inverse-quantizing the quantized prediction
   residual QD, (2) inverse-DCT (Discrete Cosine Transform) on a DCT coefficient obtained by performing inverse
10 quantization, and (3) supplying a prediction residual D obtained by performing inverse DCT to the adder 31.       When
   inverse-quantizing the quantized prediction residual QD, the quantization step QP supplied from the transform-and
   quantize unit 21 is utilized.   The prediction residual D output from the inverse-quantize-and-inverse-transform unit 23
   is the prediction residual D input into the transform-and-quantize unit 21 to which a quantization error is added, however,
   for a simple description, the same name will be used.
15 [0236]
              The intra-prediction image generator 25 generates a prediction image Pred Intra concerning each partition.
   More specifically, the intra-prediction image generator 25 performs: (1) selecting a prediction mode concerning each
   partition used for intra prediction; and (2) generating the prediction image Pred Intra from the decoded image P by using
   the selected prediction mode.     The intra-prediction image generator 25 supplies the generated prediction image
20 PredIntra to the prediction-method controller 28.
   [0237]
              The intra-prediction image generator 25 also specifies a prediction index PI concerning each partition from a
   selected prediction mode selected for the partition and from the size of the partition, and then supplied the prediction
   index PI to the prediction-method controller 28.
25 [0238]
                                                                 60

               The intra-prediction image generator 25 also supplies the size of a subject partition and intra-prediction mode
   information IEM, which is information indicating a prediction mode assigned to the subject partition, to the adaptive
   filter 70.
   [0239]
 5             The motion-vector detector 27 detects a motion vector my concerning each partition.       More specifically, the
   motion-vector detector 27 detects a motion vector my concerning a subject partition by: (1) selecting a filtered decoded
   image PFL' to be used as a reference image; and (2) by searching for an area which best approximates to the subject
   partition in the selected filtered decoded image PFL'.       The filtered decoded image PFL' is an image obtained by
   performing deblocking processing by using the deblocking filter 33, adaptive offset processing by using the adaptive
10 offset filter 80, and adaptive filtering processing by using the adaptive filter 70 on a decoded image.     The motion-vector
   detector 27 may read pixel values of individual pixels forming the filtered decoded image PFL' from the buffer memory
   24.    The motion-vector detector 27 supplies the detected motion vector my to the inter-prediction image generator 26
   and the motion-vector redundancy eliminating unit 29, together with a reference image index RI which specifies the
   filtered decoded image PFL' utilized as a reference image.        Concerning a partition on which bidirectional prediction
15 (weighted prediction) will be performed, the motion-vector detector 27 selects two filtered decoded images PFL1' and
   P_FL2' as reference images, and supplies motion vectors mvl and mv2 and reference image indexes RI 1 and R12
   associated with the two filtered decoded images PFLl' and PFL2' to the inter-prediction image generator 26 and the
   motion-vector redundancy eliminating unit 29.
   [0240]
20             The inter-prediction image generator 26 generates a motion-compensated image mc concerning each inter
   prediction partition.    More specifically, the inter-prediction image generator 26 generates a motion-compensated image
   mc from the filtered decoded image PFL' specified by the reference image index RI supplied from the motion-vector
   detector 27 by using the motion vector my supplied from the motion-vector detector 27.          As in the motion-vector
   detector 27, the inter-prediction image generator 26 may read pixel values of individual pixels forming the filtered
25 decoded image PFL' from the buffer memory 24.            The inter-prediction image generator 26 supplies the generated
                                                                   61

   motion-compensated image mc (inter-prediction image Pred Inter) to the prediction-method controller 28, together with
   the reference image index RI supplied from the motion-vector detector 27.        Concerning a partition on which
   bidirectional prediction (weighted prediction) will be performed, the inter-prediction image generator 26 performs: (1)
   generating a motion-compensated image mcI from a filtered decoded image PFL 1' specified by the reference image
 5 index RI 1 by using a motion vector my1; (2) generating a motion-compensated image mc2 from a filtered decoded
   image PFL2' specified by the reference image index R12 by using a motion vector mv2; and (3) generating an inter
   prediction image Pred Inter by adding an offset value to a weighted average of the motion-compensated image mc I and
   the motion-compensated image mc2.
   [0241]
10           The prediction-method controller 28 compares each of the intra-prediction image Pred Intra and the inter
   prediction image PredInter with a subject image to be coded, and determines whether to perform intra prediction or
   inter prediction.   If intra prediction is selected, the prediction-method controller 28 supplies the intra-prediction image
   PredIntra to the adder 31 and the subtractor 32 as the prediction image Pred, and also supplies the prediction index PI
   supplied from the intra-prediction image generator 25 to the variable-length code coder 22 as the prediction parameter
15 PP.   In contrast, if inter prediction is selected, the prediction-method controller 28 supplies the inter-prediction image
   PredInter to the adder 31 and the subtractor 32 as the prediction image Pred, and also supplies the reference image
   index RI supplied from the inter-prediction image generator 26 and an estimation motion-vector index PMVI and
   motion-vector residual MVD supplied from the motion-vector redundancy eliminating unit 29 (discussed later) to the
   variable-length code coder as the prediction parameters PP.
20 [0242]
             By subtracting the prediction image Pred selected in the prediction-method controller 28 from the subject
   image to be coded, a prediction residual D is generated in the subtractor 32.     The prediction residual D generated in the
   subtractor 32 is subjected to DCT-transform and quantization by the transform-and-quantize unit 21, as stated above.
   Meanwhile, by adding the prediction image Pred selected in the prediction-method controller 28 to the prediction
25 residual D generated in the inverse-quantize-and-inverse-transform unit 23, a locally decoded image P is generated in the
                                                                    62

   adder 31.     The locally decoded image P generated in the adder 31 passes through the deblocking filter 33, the adaptive
   offset filter 80, and the adaptive filter 70.   Then, the locally decoded image P is stored in the buffer memory 24 as a
   filtered decoded image PFL and is utilized as a reference image for performing inter prediction.
   [0243]
 5             The motion-vector redundancy eliminating unit 29 eliminates a redundancy in a motion vector my detected by
   the motion-vector detector 27.       More specifically, the motion-vector redundancy eliminating unit 29 performs: (1)
   selecting an estimation method used for estimating the motion vector mv; (2) determining an estimation motion vector
   pmv in accordance with the selected estimation method; and (3) generating a motion-vector residual MVD by
   subtracting the estimation motion vector pmv from the motion vector mv.          The motion-vector redundancy eliminating
10 unit 29 supplies the generated motion-vector residual MVD to the prediction-method controller 28, together with the
   estimation motion-vector index PMVI indicating the selected estimation method.
   [0244]
               When the difference between the pixel values of pixels adjacent to each other with a block boundary or a CU
   boundary therebetween in the decoded image P is smaller than a predetermined threshold, the deblocking filter 33
15 performs deblocking processing on the block boundary or the CU boundary in the decoded image P, thereby smoothing
   an image in the vicinity of the block boundary or the CU boundary.         The image subjected to deblocking processing by
   the deblocking filter 33 is output to the adaptive offset filter 80 as a deblocked decoded image PDB.
   [0245]
               The adaptive offset filter 80 performs adaptive offset filtering processing on the deblocked decoded image
20 PDB supplied from the deblocking filter 33, thereby generating an offset-filtered decoded image P_OF.             The generated
   offset-filtered decoded image POF is supplied to the adaptive filter 70.        The specific configuration of the adaptive
   offset filter 80 will be discussed later, and thus, an explanation thereof is omitted here.
   [0246]
               The adaptive filter 70 performs adaptive filtering processing on the offset-filtered decoded image POF
25 supplied from the adaptive offset filter 80, thereby generating a filtered decoded image PFL.         The filtered decoded
                                                                   63

   image PFL subjected to filtering processing by the adaptive filter 70 is stored in the buffer memory 24.          A filter
   coefficient used by the adaptive filter 70 is determined so that the error between the filtered decoded image PFL and the
   subject image #10 can be minimized, and the filter coefficient determined in this manner is coded as a filter parameter
   FP and is transmitted to the video decoding device 1.
 5 [0247]
   (Adaptive Offset Filter 80)
              Fig. 15 is a block diagram illustrating the configuration of the adaptive offset filter 80.   The adaptive offset
   filter 80 includes, as shown in Fig. 15, an adaptive offset filter information setting unit 81 and an adaptive offset filter
   processor 82.
10 [0248]
              The adaptive offset filter information setting unit 81 includes, as shown in Fig. 15, an offset calculator 811, an
   offset shifter 816, an offset clipping section 812, an offset information selector 813, and an offset attribute setting section
   815.
   [0249]
15 (Offset Calculator 811)
              The offset calculator 811 calculates offsets concerning all the offset types and all the classes for all QAOMUs
   up to a predetermined split depth included in the unit of processing (for example, an LCU).         In this case, the offset
   types and the classes are the same as those discussed in a description of the video decoding device 1.
   [0250]
20            Fig. 16 is a flowchart of a flow of processing performed by the offset calculator 811.
   [0251]
   (Step S201)
              First, the offset calculator 811 starts a first loop using the QAOMU number of a subject QAOMU as a loop
   variable.    For example, in the example shown in parts (a) through (e) of Fig. 7, the first loop is a loop from the
25 QAOMU number=0 to the QAOMU number=340.
                                                                    64

   [0252]
   (Step S202)
              Then, the offset calculator 811 starts a second loop using the offset type that can be selected for the subject
   QAOMU as a loop variable.        The second loop is a loop from offset type 1 to offset type 6.
 5 [0253]
   (Step S203)
              Then, the offset calculator 811 starts a third loop using a pixel included in the subject QAOMU as a unit.
   [0254]
   (Step S204)
10            Then, the offset calculator 811 classifies a subject pixel under one of a plurality of classes.   More specifically,
   when the offset type, which is the loop variable of the second loop, is one of 1 to 4, the offset calculator 811 classifies
   the subject pixel under one of class 1 through class 4.      The classifying processing in this step is the same processing as
   the classifying processing in one of the first through fourth examples performed by the classifying section 624 of the
   adaptive offset filter 60 of the video decoding device 1.
15 [0255]
              Concerning the subject QAOMU, the offset calculator 811 also calculates the classify number
   count[partidx] [saotype index] [classidx], which is the number of times pixels are classified for each class.         In this
   classify number, part idx indicates the QAOMU number.
   [0256]
20 (Step S205)
              Then, the offset calculator 811 calculates a difference pixel value for the subject pixel by calculating the
   difference between the pixel value of the subject pixel in the deblocked decoded image PDB and the pixel value of the
   subject pixel in the subject image #10 to be coded.      More specifically, when the position of the subject pixel is
   represented by (x, y), the offset calculator 811 calculates PDB(x, y) - Org(x, y).      In this expression, PDB(x, y)
25 denotes the pixel value of the subject pixel in the deblocked decoded image PDB, and Org(x, y) denotes the pixel value
                                                                  65

   of the subject pixel in the subject image #10.
   [0257]
   (Step S206)
              This step is the termination of the third loop.  At the time point at which this step has been completed,
 5 difference pixel values have been calculated for all the pixels included in the subject QAOMU.
   [0258]
   (Step S207)
              Then, the offset calculator 811 calculates an offset by dividing the sum of the difference pixel values of the
   pixels included in the subject QAOMU for each class by the above-described classify number concerning this class.
10 More specifically, the offset calculator 811 calculates an offset offset[part idx][sao type idx][classidx] for the subject
   QAOMU, the subject offset type, and the subject class by using the following equation.
   [0259]
             offset[partidx][saotype idx][classidx] = E(P_DB(x, y)-Org(x, y))/count[part idx][saotypeidx][classidx],
   where the symbol X denotes the sum of the pixels classified as the subject class specified by class idx in the subject
15 QAOMU specified by part idx and the subject offset type specified by saotype idx.
   [0260]
   (Step S208)
              This step is the termination of the first loop.
   [0261]
20 (Step S209)
              This step is the termination of the first loop.
   [0262]
              By performing the above-described processing, the offset calculator 811 calculates offsets concerning all the
   offset types and all the classes for all QAOMUs up to a predetermined split depth included in a subject LCU.        For
25 example, in the case of the example shown in parts (a) through (e) of Fig. 7, the offset calculator 811 calculates a total of
                                                                 66

   16368 offsets as represented as follows:
              ((the total number of QAOMUs in the split depth 0) +        ... + (the total number of QAOMUs in the split depth 4))
   x (the number of EO offset types) x (the number of classes of EO) + (the number of BO offset types) x (the number of
   classes of BO)) = (1+4+16+64+256) x ((4x4) + (2x16)) = 16368.
 5 The number of bits of each offset is, for example, ten bits.
   [0263]
              The offset calculator 811 supplies offset information indicating offsets calculated by the above-described
   processing, offset types, classes, and QAOU structure information indicating the QAOU split structure to the offset
   shifter 816.
10 [0264]
              In step S204, the video coding device 2 may code a flag indicating which type of classifying processing has
   been performed, and then, the adaptive offset filter 60 of the video decoding device 1 may refer to this flag and may
   perform the same classifying processing as that indicated by the flag.          Alternatively, without using such a flag, the same
   classifying processing which has been determined in the video coding device 2 and the video decoding device 1 in
15 advance may be performed.
   [0265]
   (Offset Shifter 816)
              The offset shifter 816 quantizes each of the offsets included in the offset information supplied from the offset
   calculator 811.     The offset shifter 816 quantizes the offsets by performing bitwise right shift on the offsets so that the
20 precision of the offsets may be transformed from the pixel bit depth to the offset bit depth.        The amount by which an
   offset is shifted in the shifting processing is determined by the shift value supplied from the offset attribute setting
   section 815, which will be discussed later.
   [0266]
   (Offset Clipping Section 812)
25            In order to restrict the offsets to the offset value range supplied from the offset attribute setting section 815,
                                                                     67

   which will be discussed later, the offset clipping section 812 performs clip processing by using one of the following first
   clip processing and second clip processing on the offsets supplied from the offset shifter 816.
   [0267]
   (First Clip Processing)
 5            The offset clipping section 812 performs clip processing on each of the offsets included in the offset
   information supplied from the offset shifter 816.     The offset clipping section 812 clips each offset supplied from the
   offset shifter 816 to, for example, values from -8 to 7, thereby expressing each offset by four bits.      The clipped offsets
   are supplied to the offset information selector 813.    The bit width used for clipping is set in accordance with the image
   bit depth and the offset bit depth, as in the video decoding device 1.
10 [0268]
              By clipping each offset in this manner, the memory size of a memory (not shown) to store each offset can be
   reduced.     The amount of data required to code offsets included in the coded data #1 can also be decreased, thereby
   making it possible to improve the coding efficiency.      Additionally, since excessive offsets are not added, a suitable level
   of image quality can be guaranteed.
15 [0269]
   (Second Clip Processing)
              The offset clipping section 812 may change the value for the clipping range of an offset supplied from the
   offset shifter 816 in accordance with the offset type.
   [0270]
20            For example, if the offset type is an edge offset, the number of bits of an offset is set to be eight bits, and if the
   offset type is a band offset, the number of bits of an offset is set to be four bits. More generally, when the number of
   bits of an edge-offset offset is N bits and the number of bits of a band-offset offset is M bits, the numbers of bits of
   offsets are determined so that the condition N>M can be satisfied.
   [0271]
25            In this manner, by varying the number of bits of an offset in accordance with the offset type, the coding
                                                                   68

   efficiency can be improved without requiring an excessive memory size of a memory for storing each offset.
   [0272]
             When the threshold th for restricting values that can be taken as an offset is greater than  2  ' but not greater
   than 2'", m-bit fixed-length coding method can be used as the coding method for coding the offset.         More specifically,
 5 Truncated unary coding or Truncated Rice coding having th as the maximum value may be used.              The above-described
   maximum value th is determined by the offset value range supplied from the offset attribute setting section 815.
   [0273]
             Clip processing performed by a combination of the above-described first clip processing and second clip
   processing is also included in this embodiment.       The adaptive offset filter 80 may not include the offset clipping section
10 812.
   [0274]
   (Offset Information Selector 813)
             The offset information selector 813 determines a combination of an offset type, a class, and an offset which
   minimize the RD cost (Rate-Distortion cost) and the associated QAOU split structure, and supplies QAOU information
15 indicating the determined offset types, classes, and offsets, and the associated QAOM split structure to the variable
   length code coder 22.     The offset information selector 813 also supplies the determined offsets for each QAOU or each
   QAOMU to the adaptive offset filter processor 82.
   [0275]
             Processing performed by the offset information selector 813 will be discussed more specifically with reference
20 to Figs. 17 and 18.    Fig. 17 is a flowchart of a flow of processing performed by the offset information selector 813.
   [0276]
   (Step S301)
             The offset information selector 813 starts a first loop using the QAOMU number of a subject QAOMU as a
   loop variable.
25 [0277]
                                                                  69

   (Step S302)
              Then, the offset information selector 813 starts a second loop using the offset type that can be selected for the
   subject QAOMU as a loop variable.           The second loop is a loop from offset type 1 to offset type 6.
   [0278]
 5 (Step S303)
              Then, the offset information selector 813 calculates, for a subject offset type, the squared error between the
   offset-filtered decoded image POF and the subject coded image #10 concerning the subject QAOMU.
   [0279]
   (Step S304)
10            This step is the termination of the second loop.
   [0280]
   (Step S305)
              This step is the termination of the first loop.    At the time point at which the first loop and the second loop
   have been completed, the squared errors concerning each QAOMU for all the offset types have been calculated.
15 [0281]
   (Step S306)
              Then, the offset information selector 813 determines, among QAOU split structures which may be obtained by
   dividing the subject unit of processing (for example, an LCU) into QAOUs, the QAOU split structure that minimizes the
   RD cost.
20 [0282]
              Specific examples of the processing performed by the offset information selector 813 in this step will be
   described below with reference to parts (a) through (d) of Fig. 18.
   [0283]
              First, the offset information selector 813 calculates the RD cost when the split depth is 0 and the RD cost when
25 the split depth is 1 (part (a) of Fig. 18).   In part (a) of Fig. 18, it is assumed that the RD cost of the split depth 1 is
                                                                     70

   smaller than that of the split depth 0 (part (b) of Fig. 18).
   [0284]
              Then, the offset information selector 813 calculates the RD cost when the split depth is 2 (part (c) of Fig. 18).
   [0285]
 5            Then, the offset information selector 813 compares the RD cost of a QAOMU of the split depth 1 with that of
   QAOMUs of the split depth 2 contained in the QAOMU of the split depth 1.            If the RD cost of the QAOMUs of the
   split depth 2 is smaller than that of the QAOMU of the split depth 1, the offset information selector 813 updates the
   QAOMU of the split depth I to the QAOMUs of the split depth 2 (part (d) of Fig. 18).           The offset information selector
   813 repeats this processing up to the largest split depth.    In this manner, the QAOU split structure which minimizes the
10 RD Cost can be determined.
   [0286]
   (Offset Attribute Setting Section 815)
              The offset attribute setting section 815 receives the pixel bit depth (not shown) and determines the offset bit
   depth.    The offset attribute setting section 815 then sets the offset value range and the shift value by using the
15 determined offset bit depth.     The offset value range is supplied to the adaptive offset filter processor 82, and the shift
   value is supplied to the offset shifter 816.   The setting of the offset value range and the shift value are the same
   processing as that performed by the above-described offset attribute setting section 613, and thus, an explanation thereof
   is omitted here.
   [0287]
20 (Adaptive Offset Filter Processor 82)
              The adaptive offset filter processor 82 adds an offset supplied from the offset information selector 813 to each
   pixel of a subject QAOU in the deblocked decoded image P_DB.             The adaptive offset filter processor 82 outputs, as an
   offset-filtered decoded image POF, an image obtained by performing processing on all the QAOUs included in the
   deblocked decoded image PDB.           The configuration of the adaptive offset filter processor 82 is the same as that of the
25 adaptive offset filter processor 62, and thus, an explanation thereof is omitted here.     Each offset stored in an offset
                                                                  71

   information storage section (not shown) included in the adaptive offset filter processor 82 is restricted to the offset value
   range set by the offset attribute setting section 815.
   [0288]
   [Second Embodiment]
 5            In the first embodiment, sao_offset[saocurr depth] [ys] [xs] [i] included in the coded data #1 is syntax which
   represents a specific value of an offset to be added to each pixel included in a subject QAOU in the offset filtering
   processing performed by using the adaptive offset filter.
   [0289]
              Meanwhile, the present inventors have found that the amount of data required for coding data can be further
10 reduced by performing predictive coding on an offset value used for offset filtering processing, that is, by coding an
   offset residual calculated by using an offset value and a prediction value of the offset value.
   [0290]
              In a second embodiment, a description will be given, with reference to Figs. 19 through 21, of a video decoding
   device which decodes an offset subjected to predictive coding and which performs offset filtering processing, and of a
15 video coding device which performs predictive coding on an offset used for offset filtering processing.      Portions
   discussed in the first embodiment will not be described.
   [0291]
   (Coded Data)
              Coded data of this embodiment includes an offset residual saooffsetresidual[saocurr depth][ys][xs][i]
20 instead of saooffset[saocurr depth][ys][xs][i] included in the coded data #1 of the first embodiment.         The other
   portions of the configuration of the coded data of this embodiment are similar to those of the coded data #1 of the first
   embodiment.       Hereinafter, the coded data of this embodiment may also be indicated as coded data #3.
   [0292]
   (sao_offsetresidual)
25            The offset residual saooffsetresidual[saocurr depth] [ys] [xs][i] represents a weighted difference value
                                                                72

   between an offset value to be added to each pixel included in a QAOU in offset filtering processing performed by an
   adaptive offset filter of this embodiment and a prediction value of the offset value, and is also indicated by
   saooffsetresidual[saotypeidx] [classidx].
   [0293]
 5             If an offset to be added to a subject pixel included in a subject QAOU is indicated by
   Offset[sao type idx][classidx], the offset residual saooffsetresidual[saotypeidx][classidx] is represented as
   follows:
               saooffset residual [saotype idx][classidx]
   =Offset[sao type idx][classidx]
10 - a*pred offset[merge tbl[sao type idx]][class idx]
   where a is a weight coefficient to be multiplied by the prediction value pred offset and merge tbl is a function using
   sao_typeidx as an argument.         Specific examples of a and merge tbl will be discussed later, and thus, an explanation
   thereof is omitted here.
   [0294]
15 (Video Decoding Device)
               The video decoding device of this embodiment includes an adaptive offset filter 60' instead of the adaptive
   offset filter 60 of the video decoding device 1 of the first embodiment.      The other elements of the configuration of the
   video decoding device of this embodiment are similar to those of the video decoding device 1 of the first embodiment.
   [0295]
20             Fig. 19 is a block diagram illustrating the configuration of the adaptive offset filter 60' of this embodiment.
   As shown in Fig. 19, the adaptive offset filter 60' includes an offset information decoding section 611' instead of the
   offset information decoding section 611 of the adaptive offset filter 60.
   [0296]
   (Offset Information Decoding Section 611')
25             The offset information decoding section 611' refers to QAOU information included in the coded data #3 and
                                                                   73

   decodes offset information 01 included in the QAOU information.        By using an offset residual
   saooffsetresidual[saotypeidx] [classidx] and a prediction value pred offset[mergetbl [saotypeidx] ] [classidx]
   obtained by decoding the offset information 01, the offset information decoding section 611' calculates an offset
   Offset[sao type idx][classidx] to be used for adaptive offset filtering processing by the following expression:
 5           Offset[sao type idx] [class idx]
      a*pred offset[merge tbl[saotype idx]][classidx]
   + saooffsetresidual[saotype idx][classidx].
   The offset information decoding section 611' then stores the calculated Offset[saotypeidx] [classidx] in the offset
   information storage section 621.     In the above-described expression, pred offset[merge tbl[saotypeidx]][classidx]
10 is a prediction value of Offset[sao type idx][class idx], and merge tbl[sao type idx] represents a table for providing an
   index to saotype_idx=1 to 6, and one or more items of sao type idx may be considered as the same group.
   [0297]
   (First Specific Example of pred offset)
             A first specific example of pred offset[merge tbl[saotype idx]][classidx] will be discussed below.      In this
15 example, the prediction value pred offset[merge tbl[saotype idx]][classidx] is determined by:
             pred offset[mergetbl[saotype idx]][classidx]
   = Offset'[sao type idx][classidx]
   where Offset'[saotypeidx][classidx] is a decoded offset, and represents an offset associated with the offset type index
   saotypeidx and the class index classidx.
20 [0298]
             In this manner, in this example, as the prediction value of Offset[saotype idx][classidx], an offset
   Offset'[saotypeidx][classidx], which is a decoded offset associated with the offset type index saotypeidx and the
   class index classidx is used.
   [0299]
25 (Second Specific Example of pred offset)
                                                                74

             A second specific example of pred offset[merge tbl[saotype-idx]] [classidx] will be discussed below.          In
   this example, the prediction value pred offset[merge tbl[saotypeidx]] [classidx] is determined by:
             pred offset[mergetbl[saotype idx]][classidx]
   = (pred offset'[merge tbl[saotypeidx]][classidx]* W1 + Offset'[saotypeidx] [class_idx] *W2 >> log 2 (W1+W2)
 5 where pred offset'[merge tbl[saotype idx]][classidx] denotes a prediction value used for calculating a decoded offset
   Offset'[saotypeidx] [classidx], "*" denotes an operation symbol representing multiplication, and "         " denotes a
   bitwise right shift.  W1 and W2 denote weight coefficients, and may take values, such as W1=3 and W2=1.           Specific
   values of W1 and W2 may be determined so that the coding efficiency can be increased.
   [0300]
10           As is seen from the above-described expression, in this example, reference is made to decoded prediction
   values and offsets recursively, such as reference is made to pred offset' and Offset' in order to determine pred offset,
   reference is made to pred offset" and Offset" in order to determine predoffset', and so on.     Accordingly, a plurality of
   decoded offsets contribute to predoffset, thereby suppressing excessive fluctuations in a prediction value.     With this
   arrangement, for example, even if an inappropriate prediction value has been calculated due to the influence of noise, the
15 influence of such an inappropriate prediction value can be suppressed, thereby making it possible to improve the coding
   efficiency.
   [0301]
   (Third Specific Example of pred offset)
             A third specific example of pred offset[merge tbl[sao type idx]][classidx] will be discussed below.         In this
20 example, the prediction value pred offset[merge tbl[saotype idx]][classidx] is determined by:
             pred offset[mergetbl[saotype idx]][classidx]
   =clip3(-th, th, pred offset[mergetbl[saotype idx]][classidx])
   where clip3(A, B, C) denotes that the value C is clipped by the lower limit value A and the upper limit value B.     The
   argument pred offset[merge tbl[saotype idx]] of clip3 is determined, for example, as in the above-described first or
25 second specific example.     The threshold th is determined as follows while being dependent on the bit depth bit depth of
                                                                75

   the pixel value.
   [0302]
             th = 4 (bit depth = 8)
             th = 8 (bit depth > 8)
 5           In this manner, in this example, by using a prediction value clipped by the upper limit value and the lower limit
   value, excessively large prediction values and excessively small prediction values are not generated, thereby making it
   possible to improve the coding efficiency.       The absolute values of the upper limit value and the lower limit value are set
   such that they become large when the bit of a pixel value is large.        Thus, appropriate clip processing is performed in
   accordance with the bit depth of a pixel value, thereby preventing degradation of the image quality.
10 [0303]
   (First Specific Example of mergetbl)
             Part (a) of Fig. 20 is a table illustrating a first specific example of the function mergetbl[saotype idx].         As
   shown in part (a) of Fig. 20, when saotypeidx=0, mergetbl[saotypeidx] of this example does not take any value,
   and when saotype_idx=1 to 6, merge tbl[saotype idx] of this example takes 0 to 5, respectively.                Accordingly,
15 merge tbl[sao type idx] of this example may also be represented as:
             mergetbl[saotypeidx] = saotypeidx - 1.
   [0304]
             By using merge tbl[saotypeidx] of this example, the offset information decoding section 611' individually
   determines a prediction value predoffset for each saotypeidx and for each classidx.               It is thus possible to reduce the
20 amount of data required to code offset residuals sao offset residual.
   [0305]
   (Second Specific Example of mergetbl)
             Part (b) of Fig. 20 is a table illustrating a second specific example of the function merge tbl[saotypeidx].
   As shown in part (b) of Fig. 20, in the case of an edge offset (saotype idx=1 to 4), mergetbl[saotype idx] of this
25 example takes 0, and in the case of a band offset (saotypeidx=5 or 6), merge tbl[saotypeidx] of this example takes 1
                                                                     76

   or 2, respectively.
   [0306]
              For example, if saotype idx=1 (mergetbl[saotype idx=1]=0) and class idx=1 have been specified when
   calculating the previous offset, and if saotype idx=2 (mergetbl[saotype idx=2]=0) and class idx=1 have been
 5 specified when calculating the subsequent offset, the prediction value to be used for calculating the subsequent offset is
   the same as that used for calculating the previous offset.
   [0307]
              By using merge tbl[saotypeidx] of this example, the offset information decoding section 611' performs the
   following processing depending on whether an edge offset is specified or a band offset is specified.
10 [0308]
              - When edge offset is specified
              The prediction value of an offset to be decoded is calculated from a decoded offset classified as the same class
   as that of the offset to be decoded.   In this case, the offset type of the offset to be decoded and that of the prediction
   value may be different as long as the class is the same.     Accordingly, a prediction value which is set for calculating an
15 offset of a certain offset type may be used for calculating an offset of an offset type different from this certain offset type.
   As a result, the amount of processing for setting prediction values can be reduced.
   [0309]
              - When band offset is specified
              The prediction value of an offset to be decoded is calculated from an offset having the same offset type and
20 classified as the same class as those of the offset to be decoded.
   [0310]
              By using merge tbl[saotypeidx] of this example, an appropriate prediction value can be calculated while the
   amount of processing is reduced.
   [0311]
25 (Specific Example of Coefficient a)
                                                                   77

              The weight coefficient a to be multiplied by the prediction value pred offset may be 1 regardless of the offset
   type, or may vary depending on the offset type.
   [0312]
              For example, the weight coefficient a may vary as:
 5            a=1 (in the case of an edge offset)
              a=0.5 (in the case of a band offset).
   More generally, if the coefficient a to be used when an edge offset is specified is indicated by a(edge) and if the
   coefficient a to be used when a band offset is specified is indicated by a(band), the coefficient a which satisfies the
   following condition is used:
10            a(edge) > a(band).
   [0313]
              The present inventors have found that the correlation between a decoded offset and an offset to be decoded
   when an edge offset is specified is greater than that when a band offset is specified.    In the above-described specific
   example, the influence of the correlation between a decoded offset and an offset to be decoded can be appropriately
15 reflected, thereby reducing the amount of data to code offset residuals.
   [0314]
   (Video Coding Device)
              The video coding device of this embodiment includes an adaptive offset filter 80' instead of the adaptive offset
   filter 80 of the video coding device 2 of the first embodiment.     The other elements of the configuration of the video
20 coding device of this embodiment are similar to those of the video coding device 2 of the first embodiment.
   [0315]
              Fig. 21 is a block diagram illustrating the configuration of the adaptive offset filter 80' of this embodiment.
   As shown in Fig. 21, the adaptive offset filter 80' includes an offset residual determining section 814 in addition to the
   components of the adaptive offset filter 60.
25 [0316]
                                                                 78

   (Offset Residual Determining Section 814)
              The offset residual determining section 814 calculates an offset residual by taking the difference between an
   offset supplied from the offset information selector 813 and a prediction value of the offset.      The offset residual is
   coded by the variable-length code coder 22 as part of QAOU information.
 5 [0317]
              The prediction value set by the offset residual determining section 814 is similar to that set by the offset
   information decoding section 611' of the video decoding device of this embodiment, and thus, an explanation thereof is
   omitted here.
   [0318]
10 (Appendix 1)
              The present invention may be described as follows.
   [0319]
              An image filtering device according to the present invention is an image filtering device for adding an offset to
   a pixel value of each pixel forming an input image which is constituted by a plurality of unit areas.       The image filtering
15 device includes: offset attribute setting means for setting offset attributes for a subject unit area by referring to offset
   type specifying information included in coded data; offset decoding means for decoding an offset having a bit width
   corresponding to an offset value range included in the set offset attributes; and filtering means for adding the offset to the
   pixel value of each pixel forming the input image.
   [0320]
20            With the image filtering device configured as described above, the offset attribute setting means refers to the
   offset-type specifying information included in the coded data and sets offset attributes for the subject unit area, and the
   offset decoding means decodes an offset having a bit width corresponding to an offset value range included in the set
   offset attributes.  It is thus possible to effectively reduce the memory size of a memory for storing offsets.
   [0321]
25            Accordingly, with the above-described configuration, it is possible to perform appropriate offset filtering
                                                                  79

   processing while the memory size of a memory for storing offsets is reduced.
   [0322]
             The offset-type specifying information may be determined for each of the input images or for each of the unit
   areas.   Alternatively, the offset-type specifying information may be determined for each predetermined set of the input
 5 images or for each predetermined set of the unit areas.
   [0323]
             The offset-type specifying information may preferably include information concerning a bit depth of the pixel
   value in each unit area forming the input image, and the offset decoding means may preferably decode an offset having a
   bit width corresponding to the bit depth of the pixel value.
10 [0324]
             With the above-described configuration, since the offset decoding means decodes an offset having a bit width
   corresponding to the bit depth of the pixel value, the memory size of a memory for storing offsets can effectively be
   reduced.
   [0325]
15           The offset-type specifying information may include information concerning a bit depth of the pixel value in
   each unit area forming the input image, and the offset decoding means may decode an offset having a bit width which
   can express a value range corresponding to the bit depth.
   [0326]
             With the above-described configuration, since the offset decoding means decodes an offset having a bit width
20 which can express a value range corresponding to the pixel value in each unit area forming the input image, the memory
   size of a memory for storing offsets can be reduced effectively.
   [0327]
             The bit width that can express a value range corresponding to the bit depth is a bit width when values included
   in the value range are represented in binary notation.   For example, if the value range is -23 to 23-1, the bit width that
25 can express the value range is four bits.
                                                                80

   [0328]
              The above-described decoded offset may preferably be a quantized value, and the filtering means may
   preferably add a value obtained by inverse-quantizing the offset by using a parameter included in the offset attributes to
   the pixel value of each pixel.
 5 [0329]
              With the above-described configuration, the decoded offset is a quantized value, and the filtering means adds a
   value obtained by inverse-quantizing the offset by using a parameter included in the offset attributes to the pixel value of
   each pixel.   Accordingly, an offset corresponding to a parameter included in the offset attributes is added to each image
   value.
10 [0330]
              Accordingly, with the above-described configuration, it is possible to improve the coding efficiency while the
   memory size of a memory for storing offsets is reduced.
   [0331]
              The offset-type specifying information may include information concerning a shift value of the pixel value, and
15 the filtering means may add a value obtained by inverse-quantizing the offset by using the shift value instead of adding
   the above-described offset.
   [0332]
              With the above-described configuration, the offset-type specifying information includes information
   concerning a shift value of the pixel value, and the filtering means adds a value obtained by inverse-quantizing the offset
20 by using the shift value instead of adding the above-described offset, thereby making it possible to obtain an offset
   corresponding to the shift value of the pixel value.    Thus, it is possible to improve the coding efficiency while the
   memory size of a memory for storing offsets is reduced.
   [0333]
              The shift value of the pixel value indicates a difference value between the pixel bit depth and the offset bit
25 depth, and inverse-quantizing of the offset by using the shift value means that the offset is subjected to bitwise left shift
                                                                  81

   by an amount equal to the shift value, thereby performing transformation from the offset bit depth to the pixel bit depth.
   [0334]
             The offset-type specifying information may preferably be determined for the input image.
   [0335]
 5           With the above-described configuration, since the offset-type specifying information is determined for the input
   image, the image filtering device is capable of performing appropriate offset processing for the input image.
   [0336]
             An offset decoding device according to the present invention is an offset decoding device for decoding each
   offset which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.
10 The offset decoding device includes: offset residual decoding means for decoding each offset residual from coded data;
   prediction value determining means for determining a prediction value of each offset from a decoded offset; and offset
   calculating means for calculating each offset from a prediction value determined by the prediction value determining
   means and an offset residual decoded by the offset residual decoding means.
   [0337]
15           In the offset decoding device configured as described above, there are provided the offset residual decoding
   means for decoding each offset residual from coded data, the prediction value determining means for determining a
   prediction value of each offset from a decoded offset, and the offset calculating means for calculating each offset from a
   prediction value determined by the prediction value determining means and an offset residual decoded by the offset
   residual decoding means.     Accordingly, an offset can be appropriately decoded from coded data having a smaller
20 amount of data, compared with a case in which each offset itself is coded.
   [0338]
             The input image may preferably be constituted by a plurality of unit areas.      The offset residual decoding
   means may preferably decode each offset residual in association with an offset type, which is determined for each unit
   area, and an offset class, which is determined for each pixel.   The prediction value determining means may preferably
25 determine a prediction value of each offset from a decoded offset associated with the same offset type and the same
                                                                82

   offset class as those of the offset for which a prediction value will be determined.
   [0339]
              With the above-described configuration, the prediction value of each offset is determined from a decoded offset
   associated with the same offset type and the same offset class as those of the offset for which a prediction value will be
 5 determined, thereby making it possible to improve the prediction precision.       Accordingly, with the above-described
   configuration, it is possible to appropriately decode an offset from coded data having a smaller amount of data.
   [0340]
              The input image may preferably be constituted by a plurality of unit areas.    The offset residual decoding
   means may preferably decode each offset residual in association with an offset type, which is determined for each unit
10 area, and an offset class, which is determined for each pixel.    The prediction value determining means may preferably
   determine a prediction value of each offset from a decoded offset associated with the same first offset type group and the
   same offset class as those of the offset for which a prediction value will be determined in a case in which the offset type
   associated with the offset belongs to a first offset type group, and the prediction value determining means may preferably
   determine a prediction value of each offset from a decoded offset associated with the same offset type and the same
15 offset class as those of the offset for which a prediction value will be determined in a case in which the offset type
   associated with the offset belongs to a second offset type group.
   [0341]
              With the above-described configuration, the prediction value of each offset is determined from a decoded offset
   associated with the same offset type as that of the offset for which a prediction value will be determined in a case in
20 which the offset type associated with the offset belongs to a first offset type group, and the prediction value of each
   offset is determined from a decoded offset associated with the same offset type and the same offset class as those of the
   offset for which a prediction value will be determined in a case in which the offset type associated with the offset
   belongs to a second offset type group, thereby making it possible to improve the prediction precision while the amount
   of processing is reduced.    Thus, with the above-described configuration, it is possible to appropriately decode an offset
25 from coded data having a smaller amount of data while the amount of processing is reduced.
                                                                 83

   [0342]
             The first offset type is a type in which each pixel in a unit area associated with the first offset type is classified
   as one of a plurality of classes in accordance with, for example, the mode of an edge in the vicinity of the pixel.       The
   second offset type is a type in which each pixel in a unit area associated with the second offset type is classified as one
 5 of a plurality of classes in accordance with, for example, the pixel value of the pixel.
   [0343]
             The offset calculating means may preferably calculate each offset as a linear function of a prediction value
   determined by the prediction value determining means and an offset residual decoded by the offset residual decoding
   means.    A coefficient to be multiplied by the prediction value may preferably differ depending on whether the offset
10 type associated with the offset belongs to the first offset type group or the second offset type group.
   [0344]
             With the above-described configuration, the coefficient to be multiplied by the prediction value differs
   depending on whether the offset type associated with the offset belongs to the first offset type group or the second offset
   type group.    It is thus possible to calculate the offset by using a more suitable coefficient depending on the offset type,
15 thereby making it possible to improve the coding efficiency.
   [0345]
             The prediction value determining means may preferably determine a prediction value of each offset by
   calculating a weighted average of a decoded offset and a prediction value of the decoded offset.
   [0346]
20           With the above-described configuration, a prediction value of each offset is determined by calculating a
   weighted average of a decoded offset and a prediction value of the decoded offset.        Accordingly, a plurality of decoded
   offsets contribute to a prediction value of each offset, thereby suppressing excessive fluctuations in the prediction value.
   With this arrangement, for example, even if an inappropriate prediction value has been calculated due to the influence of
   noise, the influence of such an inappropriate prediction value can be suppressed, thereby making it possible to improve
25 the coding efficiency.
                                                                   84

   [0347]
              The prediction value determining means may preferably include clipping means for clipping each of the
   determined prediction values by using an upper limit value and a lower limit value which correspond to a bit depth of the
   pixel value of each pixel forming the input image.
 5 [0348]
              With the above-described configuration, since each of the determined prediction values is clipped by using an
   upper limit value and a lower limit value which correspond to a bit depth of the pixel value of each pixel forming the
   input image, excessively large prediction values and excessively small prediction values are not generated, thereby
   making it possible to improve the coding efficiency.
10 [0349]
             An image filtering device according to the present invention is an image filtering device which operates on an
   input image.     The image filtering device includes: calculating means for calculating a difference value between a pixel
   value of a subject pixel forming an input image and a pixel value of a pixel around the subject pixel; bit shift means for
   performing bitwise right shift on a pixel value referred to by the calculating means or the difference value calculated by
15 the calculating means by an amount equal to a predetermined shift value; classifying means for classifying the subject
   pixel as one of a plurality of offset classes in accordance with a magnitude relation between the difference value
   subjected to bitwise right shift by the bit shift means and 0; and offset means for adding an offset associated with the
   offset class of the subject pixel classified by the classifying means to the pixel value of the subject pixel.
   [0350]
20            In the image filtering device configured as described above, the subject pixel is classified as one of a plurality
   of offset classes in accordance with a magnitude relation between the difference value subjected to bitwise right shift by
   the bit shift means and 0, and an offset associated with the offset class of the subject pixel classified by the classifying
   means is added to the pixel value of the subject pixel.      Thus, the classifying processing is less vulnerable to the
   influence of noise, thereby making it possible to improve the coding efficiency.
25 [0351]
                                                                  85

              The predetermined shift value may preferably have a positive correlation with a bit depth of the pixel value of
   the subject pixel.
   [0352]
              With the above-described configuration, since the predetermined shift value may preferably have a positive
 5 correlation with a bit depth of the pixel value of the subject pixel, the coding efficiency can be more effectively
   improved.
   [0353]
              An image filtering device according to the present invention is an image filtering device which operates on an
   input image.     The image filtering device includes: calculating means for calculating a difference value between a pixel
10 value of a subject pixel forming an input image and a pixel value of a pixel around the subject pixel; classifying means
   for classifying the subject pixel as one of a plurality of offset classes in accordance with a magnitude relation between
   the difference value calculated by the calculating means and each of predetermined first and second thresholds; and
   offset means for adding an offset associated with the offset class of the subject pixel classified by the classifying means
   to the pixel value of the subject pixel.
15 [0354]
              In the image filtering device configured as described above, the subject pixel is classified as one of a plurality
   of offset classes in accordance with a magnitude relation between the difference value calculated by the calculating
   means and each of the predetermined first and second thresholds, and an offset associated with the offset class of the
   subject pixel classified by the classifying means is added to the pixel value of the subject pixel.   Thus, the classifying
20 processing is less vulnerable to the influence of noise, thereby making it possible to improve the coding efficiency.
   [0355]
              Absolute values of the first and second thresholds may preferably have a positive correlation with a bit depth of
   the pixel value of the subject pixel.
   [0356]
25            With the above-described configuration, since the absolute values of the first and second thresholds have a
                                                                  86

   positive correlation with the bit depth of the pixel value of the subject pixel, the coding efficiency can be more
   effectively improved.
   [0357]
              An image filtering device according to the present invention is an image filtering device which operates on an
 5 input image constituted by a plurality of unit areas.    The image filtering device includes: determining means for
   determining, among first and second offset types, an offset type to which a subject unit area including a subject pixel
   forming the input image belongs; classifying means for classifying the subject pixel as one of an offset class in which an
   offset is not added and a plurality of offset classes in which an offset is added in accordance with the offset type to which
   the subject unit area belongs and a pixel value of the subject pixel; and offset means for adding an offset associated with
10 the offset type to which the subject unit area belongs and the offset class of the subject pixel classified by the classifying
   means to the pixel value of the subject pixel.    In a case in which the pixel value of the subject pixel is within a
   predetermined range, the classifying means classifies the subject pixel as an offset class in which an offset is added,
   regardless of whether the offset type to which the unit area including the subject pixel belongs is the first offset type or
   the second offset type.
15 [0358]
              In the image filtering device configured as described above, in a case in which the pixel value of the subject
   pixel is within a predetermined range, the subject pixel is classified as an offset class in which an offset is added,
   regardless of whether the offset type to which the unit area including the subject pixel belongs is the first offset type or
   the second offset type, thereby making it possible to effectively eliminate block noise.     Accordingly, with the above
20 described configuration, the coding efficiency can be improved.
   [0359]
              An image filtering device according to the present invention is an image filtering device for adding an offset to
   a pixel value of each pixel forming an input image which is constituted by a plurality of unit areas.     The image filtering
   device includes: determining means for determining an offset type to which a subject unit area belongs among a plurality
25 of offset types; offset coding means for determining an offset having a bit width which differs depending on the offset
                                                                  87

   type and for coding the offset; and filtering means for adding the determined offset to the pixel value of each pixel
   forming the input image.
   [0360]
             In the image filtering device configured as described above, among a plurality of offset types, the offset type to
 5 which a subject unit area belongs is determined, an offset having a bit width which differs depending on the determined
   offset type is determined, and the determined offset is added to the pixel value of each pixel forming the input image.
   The determined offset is also coded.
   [0361]
             Accordingly, with the above-described configuration, it is possible to perform appropriate offset filtering
10 processing while the memory size of a memory for storing offsets is reduced.       With the above-described configuration,
   since the amount of data required to code data is reduced, the coding efficiency is improved.
   [0362]
             An offset coding device according to the present invention is an offset coding device for coding each offset
   which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.    The
15 offset coding device includes: prediction value determining means for determining a prediction value of each offset from
   a coded offset; offset residual calculating means for calculating an offset residual from each offset and a prediction value
   determined by the prediction value determining means; and offset residual coding means for coding an offset residual
   calculated by the offset residual calculating means.
   [0363]
20           In the offset coding device configured as described above, there are provided the prediction value determining
   means for determining a prediction value of each offset from a coded offset, the offset residual calculating means for
   calculating an offset residual from each offset and a prediction value determined by the prediction value determining
   means, and the offset residual coding means for coding an offset residual calculated by the offset residual calculating
   means.    It is thus possible to reduce the amount of data required to code data can be reduced.
25 [0364]
                                                                88

             A data structure of coded data according to the present invention is a data structure of coded data which is
   referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas.  The data structure includes: offset-type specifying information which specifies
   an offset type to which each unit area belongs; and an offset having a bit width which differs depending on the offset
 5 type.   The image filter refers to the offset-type specifying information included in the coded data, and determines an
   offset type to which a subject unit area belongs and also decodes an offset having a bit width which differs depending on
   the determined offset type.
   [0365]
              The coded data configured as describe above includes an offset having a bit width which differs depending on
10 the offset type, thereby reducing the amount of data required to code data.     The image filter which decodes the coded
   data refers to the offset-type specifying information, and determines the offset type to which a subject unit area belongs
   and also decodes an offset having a bit width which differs depending on the determined offset type.      It is thus possible
   to perform appropriate offset filtering processing while the memory size of a memory for storing offsets is reduced.
   [0366]
15            The offset-type specifying information may be determined for each of the input images or for each of the unit
   areas.   Alternatively, the offset-type specifying information may be determined for each predetermined set of the input
   images or for each predetermined set of the unit areas.
   [0367]
   [Third Embodiment]
20            Offset information 01 of a third embodiment will first be described below with reference to Fig. 23.     Part (a)
   of Fig. 23 illustrates syntax (indicated by "sao offsetparam()" in part (a) of Fig. 23) of offset information 01.
   [0368]
             As shown in part (a) of Fig. 23, the offset information 01 includes a parameter
   "saotypeidx[saocurr depth] [ys] [xs]".         If the parameter "saotype idx[saocurr depth] [ys] [xs]"is not 0, a parameter
25 sao_offset[sao curr depth][ys][xs][i] is included in the offset information 01.
                                                                  89

   [0369]
   (saocurr depth, ys, xs)
             An argument "sao_curr depth", which is an argument of "saotype idx" and "saooffset", is a parameter
   indicating the split depth of a QAOU, and "ys" and "xs" are parameters respectively indicating the position in the y
 5 direction and the position in the x direction of a QAOU (or a QAOMU, which will be discussed later).
   [0370]
             The split modes of a QAOU in accordance with the values of "sao curr depth" are the same as those discussed
   with reference to Fig. 4.
   [0371]
10           Part (b) of Fig. 23 illustrates syntax (indicated by "saosplit_param(" in part (b) of Fig. 23) of QAOU
   information.    As indicated by the syntax shown in part (b) of Fig. 23, if the split depth "saocurr depth" is smaller than
   a predetermined maximum value set by "saoMaxDepth", it is determined by the parameter "sao_split flag" whether a
   QAOU will be further split.       If the QAOU will be further split, a subsequent hierarchical depth "sao split-param(" is
   recursively called.    If the split depth has reached the maximum value ("saocurr depth" is not smaller than
15 "saoMaxDepth"), "0" is set in "saosplitflag[saocurr depth][ys][xs]", and the QAOU will not be further split.
   [0372]
             Fig. 44 illustrates another example of syntax of offset information and syntax of QAOU information.
   [0373]
             Part (a) of Fig. 44 illustrates syntax of offset information.   The configuration of the syntax is similar to that
20 shown in part (a) of Fig. 23, but "component" indicating the value of a color component is added as an argument of
   "sao-offset-paramo" and as an index of an array of "sao split flag", "sao typeidx", and "saooffset".            By the
   addition of "component", a QAOU may be split differently according to a color component, such as a luminance
   component or a chrominance component, and different offsets may be applied to split QAOUs.
   [0374]
25           Part (b) of Fig. 44 illustrates syntax of QAOU information.      As in part (a) of Fig. 44, a color component
                                                                   90

   "component" is added as an argument to the syntax shown in part (b) of Fig. 23.
   [0375]
              Part (c) of Fig. 44 illustrates the entire syntax of an adaptive offset filter which calls syntax of part (a) and
   syntax of part (b) of Fig. 44.     The parameter "saoflag" is a flag indicating whether or not an adaptive offset filter is
 5 applied.    Only when this flag is true, will the following parameters concerning the adaptive offset filter be used.        If
   this flag is true, the syntax "saosplitparam(" and the syntax "saooffsetparam(" are called by specifying the highest
   hierarchical level for each color component.        Since the highest hierarchical level is specified, arguments provided to
   each syntax are saocurr depth=0, ys=0, and xs=0.           The color components are distinguished from each other by setting
   the values of "component" as follows: the value is 0 in the case of the luminance (Y), the value is 1 in the case of the
10 chrominance (Cb), and the value is 2 in the case of the chrominance (Cr).         Other values may be used as the values of
   the component as long as the color components can be distinguished from each other.
   [0376]
              Concerning the chrominance (Cb) and the chrominance (Cr), flags "saoflag cb" and "saoflagcr" are
   respectively used to determine whether an adaptive offset filter will be applied, and if it is not applied, QAOU
15 information and offset information concerning the associated color component are not stored.
   [0377]
              Since the argument "component" is added in the syntax shown in Fig. 44, in the description using Fig. 23,
   processing is performed by replacing the argument [saocurr depth] [ys] [xs] by [saocurr depth] [ys] [xs] [component].
   In the subsequent description, processing is performed in a similar manner.
20 [0378]
   (Video Decoding Device 1')
              A video decoding device l' of the third embodiment will be described below with reference to Figs. 22 and 24
   through 29.     Elements having the same functions as those discussed in the above-described embodiments are
   designated by like reference numerals, and an explanation thereof will thus be omitted.
25 [0379]
                                                                    91

              As in the video decoding device 1, the video decoding device 1' partially includes a method defined in H.
   264/MPEG-4. AVC, a method used in KTA software, which is a joint development codec in VCEG (Video Coding
   Expert Group), a method used in TMuC (Test Model under Consideration) software, which is a successor codec to the
   codec used in KTA software, and a technology used in HM (HEVC TestModel) software.               The video decoding device 1'
 5 of this embodiment is different from the video decoding device 1 of the first embodiment in that it includes an adaptive
   offset filter 60' instead of the adaptive offset filter 60 of the video decoding device 1.  The other elements of the
   configuration of the video decoding device 1' are similar to those of the video decoding device 1.
   [0380]
   (Adaptive Offset Filter 60')
10             Details of the adaptive offset filter 60' will now be discussed below with reference to Fig. 22.   Fig. 22 is a
   block diagram illustrating the configuration of the adaptive offset filter 60'.    As shown in Fig. 22, the adaptive offset
   filter 60' includes an adaptive offset filter information decoder 61' and an adaptive offset filter processor 62'.
   [0381]
               The adaptive offset filter information decoder 61' includes, as shown in Fig. 22, an offset information decoding
15 section 611 and a QAOU structure decoding section 612.
   [0382]
               The offset information decoding section 611 refers to QAOU information included in the coded data #1 and
   decodes offset information 01 included in the QAOU information.            The values
   "saotypeidx[saocurr depth] [ys] [xs] [component]" and "saooffset[saocurr depth] [ys] [xs] [i]" obtained by decoding
20 the offset information 01 are supplied to the offset information storage section 621 in association with the arguments
   (saocurr depth, ys, xs) and the arguments (saocurr depth, ys, xs, i).
   [0383]
               More specifically, the offset information decoding section 611 decodes code from the coded data #1 and
   transforms the decoded code to the value of "saotype idx", and supplies it to the offset information storage section 621
25 in association with the arguments.       The offset information decoding section 611 changes the code decoding method and
                                                                    92

   transform from the code into the value of "saotype idx" in accordance with the hierarchical depth of a subject QAOU
   to be processed.     Conditions, such as the hierarchical depth of a QAOU, are referred to as "parameter conditions".
   Decoding of offset information in accordance with general parameter conditions will be discussed later with reference to
   Figs. 41 and 42.
 5 [0384]
              In the code decoding method, different maximum values may be used depending on whether the hierarchical
   depth of a QAOU is smaller than a threshold or is equal to or greater than the threshold.    Alternatively, the maximum
   value may be used only when the hierarchical depth of a subject QAOU is equal to or greater than the threshold.        In the
   code decoding method, different binarization techniques may be used depending on whether the hierarchical depth of a
10 subject QAOU is smaller than a threshold or is equal to or greater than the threshold.    Alternatively, different contexts
   may be used depending on whether the hierarchical depth of a subject QAOU is smaller than a threshold or is equal to or
   greater than the threshold.
   [0385]
              For example, if the hierarchical depth of a subject QAOU is smaller than a threshold, code may be decoded by
15 using variable-length coding (ue(v)), and if the hierarchical depth of a subject QAOU is equal to or greater than the
   threshold, code may be decoded by using truncated coding (te(v)) corresponding to the number of offset types.        If the
   number of offset types is a power of two, code may be decoded by using fixed-length coding.        If the number of offset
   types is four, it can be represented by two bits, and thus, code may be decoded by using fixed-length coding with two
   bits.
20 [0386]
              The offset information decoding section 611 transforms decoded code into the value of "saotypeidx" by
   using transform tables 801 and 802 shown in Fig. 25.       The transform table 801 shown in part (a) of Fig. 25 indicates
   two transform patterns.     Among the two transform patterns, one pattern is used depending on the hierarchical depth of a
   subject QAOU.       That is, when the hierarchical depth of a subject QAOU is smaller than a threshold, the offset
25 information decoding section 611 utilizes a transform table 80 1A, and when the hierarchical depth of a subject QAOU is
                                                                 93

   equal to or greater than the threshold, the offset information decoding section 611 utilizes a transform table 801B.
   [0387]
              In the transform table 801A shown in part (a) of Fig. 25, an offset type "EO_0" ("saotypeidx"=1) is
   associated with a code "1", "EO_1" ("saotypeidx"=2) is associated with a code "2", and "EO_2" ("saotype idx"=3) is
 5 associated with a code "3".       Offset types are associated with codes 4 through 6 in a similar manner.
   [0388]
              In the transform table 801B shown in part (b) of Fig. 25, an offset type "EO_0" ("saotypeidx"=1) is
   associated with a code "1", "BO_0" ("saotypeidx"=5) is associated with a code "2", and "BO_1" ("saotypeidx"=6)
   is associated with a code "3".      In the transform table 801B, codes 4 through 6 are not used.
10 [0389]
              In this manner, in the transform table 801A, all the offset types used for the adaptive offsets (SAOs) are
   included.     In contrast, in the transform table 801B, only some of the offset types used for the adaptive offsets (SAOs)
   are included, and thus, when the hierarchical depth of a subject QAOU is equal to or greater than the threshold, some
   offset types cannot be used.
15 [0390]
              The reason for this is as follows.     Since the area of a QAOU becomes smaller in a deeper hierarchical level,
   the characteristics of pixel values within such a QAOU are almost uniform, and appropriate offsets can be determined
   without using many offset types.        Accordingly, the number of offset types can be decreased, thereby reducing a
   required memory space.        Additionally, the data length of coded data indicating the offset types can be decreased,
20 thereby making it possible to improve the coding efficiency.
   [0391]
              If the order of the associations between the codes and the offset types in the transform table 801B is the same
   as that in the transform table 801A, only the transform table 801A may be used.
   [0392]
25            A transform table 802 shown in part (b) of Fig. 25 indicates various examples of transform tables which are
                                                                    94

   similar to the transform table 801B and which may be used instead of the transform table 801B.           In all the transform
   tables, the mode of types is restricted, as in the transform table 801B.    The blank indicates that the associated code is
   not used.
   [0393]
 5            A transform table 802A is an example in which only edge offsets are used and band offsets are not included.
   The number of offsets of an edge offset type is generally 4, and the number of offsets of a band offset type is generally
   16.    In this manner, the number of offsets of an edge offset type is smaller than that of a band offset.     Accordingly, by
   restricting the use of band offsets, the memory space used for storing offsets can be reduced.        In particular, when the
   hierarchical depth becomes deeper, the number of offsets is increased, and thus, the memory space is also increased.
10 When the hierarchical depth becomes deeper, it is less likely that a band offset will be selected.       Accordingly, the
   hierarchical depth is used as a parameter condition, and when the hierarchical depth is deep, the transform table 802A
   including only edge offsets is used, and when the hierarchical depth is not deep, a transform table (for example, the
   transform table 801A) including edge offsets and band offsets is used.       It is thus possible to reduce the memory space
   without decreasing the coding efficiency.      Additionally, the cost calculations for unnecessary options can be omitted in
15 the coding device, thereby reducing the amount of processing.
   [0394]
              A transform table 802B is an example in which only band offsets are used and edge offsets are not included.
   A band offset requires a smaller amount of calculation than an edge offset.        Additionally, a band offset does not utilize a
   pixel around a subject pixel, and thus, a line memory for storing a reference pixel is not necessary.       Accordingly, the
20 transform table 802B is used in accordance with the parameter condition, thereby achieving the above-described
   advantages.     A transform table 802C is an example in which only one band offset "BO_0" ("saotypeidx"=5) is used.
   [0395]
              A transform table 802D is an example in which one edge offset and two band offsets are used, as in the
   transform table 801B.     More specifically, the transform table 802D is an example in which offset types "EO_0"
25 ("saotype idx"=1), "BO_0" ("saotype idx"=5), and "BO_1" ("saotype idx"=6) are used and shorter codes (smaller
                                                                  95

   code numbers) are preferentially assigned to the band offset types.        When the hierarchical depth is not deep, it is more
   likely that a band offset will be selected than when the hierarchical depth is deep.      Accordingly, the hierarchical depth
   is used as a parameter condition, and when the hierarchical depth is not deep, the transform table 802D is used, and a
   shorter code is assigned to the type which is more frequently used, thereby making it possible to improve the coding
 5 efficiency.
   [0396]
              A table other than those indicated by the examples shown in Fig. 25 may be used, and the association between
   the codes and the offset types may be changed in accordance with the parameter condition.           An offset type which is
   different from any of the edge offsets and band offsets shown in the transform table 801A may be used singly or together
10 with another offset type in accordance with a condition, such as the hierarchical depth.       Specific examples of such an
   offset type are an offset type having characteristics both of EO and BO, an edge offset which detects an edge at a
   horizontal sample position different from that of a known edge offset "EO 0", or a band offset to which a band is
   allocated in a manner different from known band offsets "BO 0" and "BO 1".              These specific examples will be
   discussed in another embodiment later.
15 [0397]
              The QAOU structure decoding section 612 decodes "saosplit flag[saocurr depth][ys][xs]" included in the
   QAOU information so as to determine the QAOU split structure, and then supplies QAOU structure information
   indicating the determined QAOU split structure to the offset information storage section 621.
   [0398]
20            An offset attribute setting section 613 which is discussed below may be included.        The offset attribute setting
   section 613 determines the shift value of an offset.     An offset in coded data is coded with the offset bit depth (also
   referred to as "SAO DEPTH") having a lower precision than the pixel bit depth (also referred to as "PIC_DEPTH").
   That is, an offset in the coded data is quantized.     The shift value is a bit shift amount which is necessary for performing
   inverse quantization.     The offset attribute setting section 613 also determines the offset bit depth and the offset value
25 range.    In this case, the offset bit depth is determined from the pixel bit depth (also referred to as "PIC DEPTH") (not
                                                                   96

   shown) input into the offset attribute setting section 613.   The pixel bit depth represents the range of pixel values of
   pixels forming an image input into the adaptive offset filter 60 as the bit width.    When the pixel bit depth is N bits, the
   pixel values take a range of 0 to 2N_
   [0399]
 5           The SAO bit depth and the shift value are determined by using the following equations.           However, other
   values may be used in accordance with parameter conditions, which will be discussed later.
   [0400]
             SAODEPTH = MIN(PICDEPTH, 10)
             shift value =PIC_DEPTH - MIN(PICDEPTH, 10)
10           The quantized offset value range is determined by the following equation.
   [0401]
             The adaptive offset filter processor 62' includes, as shown in Fig. 22, the offset information storage section 621,
   a QAOU controller 622, an offset-type determining section 623, a classifying section 624, an offset determining section
   625, and an offset adder 626.
15 [0402]
             The offset information storage section 621 manages and stores the offset type specified for each QAOU and
   specific values of offsets with respect to individual classes that can be selected for the specified offset type, on the basis
   of the QAOU structure information, "saotypeidx[saocurr depth] [ys][xs]", and
   "saooffset[saocurr depth][ys][xs][i]".       The offset information storage section 621 has a map memory and a list
20 memory.
   [0403]
             The map memory and the list memory will be discussed below with reference to Fig. 24.             Fig. 24 illustrates
   examples of information stored in the map memory and the list memory.          Part (a) of Fig. 24 illustrates examples of
   QAOU indexes stored in the map memory, and part (b) of Fig. 24 illustrates examples of information stored in the list
25 memory.
                                                                 97

   [0404]
             In a map memory 601, a QAOU index assigned to each offset minimum unit (also referred to as "QAOMU:
   Quad Adaptive Offset Minimum Unit"), which is determined by the split depth, is stored.             The QAOU index will be
   discussed later. Part (a) of Fig. 24 shows QAOMUs having a split depth of three and forming the unit of processing
 5 (for example, an LCU) and QAOU indexes assigned to QAOMUs.                 In part (a) of Fig. 24, indexes 0 to 9 are assigned to
   QAOUs in a simple manner without considering the QAOU split depth.              In the example shown in part (a) of Fig. 24, a
   QAOU specified by QAOU index = I is indicated by QAOUI.              In part (a) of Fig. 24, the thin lines indicate QAOMU
   boundaries, while the thick lines indicate QAOU boundaries.
   [0405]
10           As shown in part (a) of Fig. 24, QAOU0 is constituted by four QAOMUs, and 0 is assigned to these four
   QAOMUs as the QAOU index.            QAOU3 is constituted by one QAOMU, and 3 is assigned to this QAOMU as the
   QAOU index.       In this manner, in the map memory, QAOU indexes assigned to the individual QAOMUs are stored.
   [0406]
             In a list memory 602, the offset type associated with each QAOU index and specific values of offsets with
15 respect to classes that can be selected for this offset type are stored in association with the QAOU index.
   [0407]
             This will be more specifically discussed below with reference to part (b) of Fig. 24.         Part (b) of Fig. 24 shows
   an offset type associated with each of the QAOU indexes 0 to 9 and offsets with respect to individual classes that can be
   selected for each offset type.    In part (b) of Fig. 24, "xxx" indicates specific numeric values that can be different from
20 each other.
   [0408]
             In part (b) of Fig. 24, "BO 1" indicates an offset type specified by "saotype idx"=5.           "EO 1" indicates an
   offset type specified by "saotypeidx"=1.         In this manner, edge offsets, which are of an offset type specified by
   "saotypeidx"=1, 2, 3, 4, are also indicated by EO_l, 2, 3, 4, respectively, and band offsets, which are of an offset type
25 specified by "saotype idx"=5, 6, are also indicated by BOl, 2, respectively.
                                                                  98

   [0409]
              As shown in part (b) of Fig. 24, when the offset type is a band offset, a total of sixteen offsets, that is, the offset
   1 through the offset 16, are stored in the list memory for this offset type.    The offset 1 through the offset 16 are values
   specified by "saooffset[saocurr depth] [ys] [xs][1]" through "sao offset[saocurr depth][ys][xs][16]", respectively,
 5 when the value of "saotype idx[saocurr depth][ys][xs]" is "5" or "6".
   [0410]
              On the other hand, when the offset type is an edge offset, a total of four offsets, that is, the offset 1 through the
   offset 4, are stored in the list memory for this offset type.  The offset 1 through the offset 4 are values specified by
   "saooffset[saocurr depth] [ys] [xs] [1]" through "sao offset[saocurr depth] [ys] [xs] [4]", respectively, when the value
10 of"saotype idx[saocurr depth][ys][xs]" is one of "1, 2, 3, and 4".         In the case of an edge offset, no value is stored in
   the offset 5 through the offset 16.
   [0411]
              A QAOMU number is appended to each QAOMU, and the QAOMUs can be distinguished from each other by
   the QAOMU numbers.          Hereinafter, QAOMU having a QAOMU number NQ will also be referred to as "QAOMUNQ".
15 [0412]
              In this embodiment, the offset precision may be varied in accordance with the hierarchical depth of a subject
   QAOU.       The shift value used for inverse quantization also differs depending on the offset precision.       In this case, too,
   the offset precision and the shift value are determined by using the pixel bit depth PIC_DEPTH.
   [0413]
20            As an example of the offset precision, the offset precision may be varied, for example, as follows.         When the
   hierarchical depth of a QAOU is smaller than a threshold, the offset precision and shift value are set:
              SAODEPTH = MIN(PICDEPTH, 10)
              shift value = PIC DEPTH - MIN(PICDEPTH, 10); and when the hierarchical depth of a QAOU is equal to or
   greater than the threshold, the offset precision and shift value are set:
25            SAODEPTH = MIN(PICDEPTH, 8)
                                                                 99

              shift value = PICDEPTH - MIN(PIC_DEPTH, 8).
   [0414]
             As stated above, the offset precision (offset bit depth, SAODEPTH) and the pixel bit depth (PIC_DEPTH),
   which expresses the range of pixel values of pixels forming an input image as the bit width, are closely related to each
 5 other in terms of quantization errors.     The bit depth of an image output from the adaptive offset filter 60 is represented
   by the pixel bit depth PICDEPTH, and SAODEPTH represents the bit depth of an offset to be added to a pixel.
   Accordingly, even if an offset having a higher precision than that of the pixel bit depth is used, it is discarded in the
   output process.     It is thus meaningless to set SAODEPTH which exceeds PIC_DEPTH.               On the other hand, if
   SAODEPTH is lower than PIC_DEPTH, an input image is corrected merely with a precision lower than the precision
10 (PIC_DEPTH) with which the input image could be corrected by using a filter, thereby decreasing the filtering effect.
   [0415]
             Accordingly, in this embodiment, by restricting a quantized offset to an offset value range that can be
   represented by a certain bit width, it is possible to reduce the bit width for storing a quantized offset in the offset
   information storage section 621.      With this arrangement, the effect of reducing the memory size can be obtained,
15 compared with the above-described restriction is not imposed.        However, if the offset value range is excessively
   restricted, the effect of correcting distortion of a decoded image by using an offset is decreased.      Accordingly, it is not
   possible to remove distortion of a decoded image even by offset addition processing, which may decrease the coding
   efficiency.
   [0416]
20            Thus, by setting the offset value range in an optimal range in such a degree as not to decrease the coding
   efficiency, the filtering effect can be maintained while a memory space to be used is reduced.
   [0417]
              The maximum bit length representing the offset value range is indicated by CLIPBIT, and by calculating
   CLIPBIT = SAODEPTH - K, the offset value range is determined to be -2CLIPBIT-1 to            2 CLIPBIT- 1 -1. Then, it has been
25 found through the inventors' experiments that in the case of K=4, the coding efficiency was not decreased even if the
                                                                 100

   offset range was restricted by the offset value range.
   [0418]
               When the pixel bit depth is eight, the offset bit depth SAO DEPTH is also eight, and thus, CLIPBIT=8-K=4.
   If one offset can be stored by using four bits, in software, for example, which handles one byte constituted by eight bits
 5 as the unit, one offset can be packed and stored in one byte, thereby easily reducing the memory size.
   [0419]
               If the offset precision to be set differs depending on the hierarchical depth of a subject QAOU, the offset
   information storage section 621 secures an offset storage area by changing the unit size of the offset storage area in
   accordance with the hierarchical depth of the subject QAOU.
10 [0420]
               More specifically, it is now assumed that when the hierarchical depth of a subject QAOU is smaller than a
   threshold, the offset precision is set to be   nA  bits (for example, nA= 8 or 6), and that when the hierarchical depth of the
   subject QAOU is equal to or greater than the threshold, the offset precision is set to be np bits (nA>nB, for example,
   nB-nA- 2 ).     In this case, the offset information storage section 621 secures an area for storing offsets when the
15 hierarchical depth of the subject QAOU is smaller than the threshold by using the         nA bits as the unit, and secures an area
   for storing offsets when the hierarchical depth of the subject QAOU is equal to or greater than the threshold by using the
   nB bits as the unit.
   [0421]
               If the hierarchical depth of the subject QAOU is equal to or greater than the threshold, when reading and
20 writing an offset from and into the offset information storage section 621, it is preferable that an offset is written by
   rounding it down by (nA-nB) bits and that an offset is read by rounding it up by (nA-nB) bits.         With this input/output
   configuration, it is not necessary for other modules to perform processing by considering the difference in the offset
   precision.
   [0422]
25             If the required number of classes differs depending on the hierarchical depth of a subject QAOU, the offset
                                                                    101

   information storage section 621 changes an area of a list memory to be secured in accordance with the required number
   of classes.   For example, a band offset which is split into sixteen classes if the hierarchical depth of a subject QAOU is
   smaller than a threshold and which is split into eight classes if the hierarchical depth of the subject QAOU is equal to or
   greater than the threshold, will be considered (such a band offset will be discussed later).    In this case, the memory
 5 space necessary for securing classes when the hierarchical depth of the subject QAOU is equal to or greater than the
   threshold is half the memory space when the hierarchical depth of the subject QAOU is smaller than the threshold.
   Accordingly, when the hierarchical depth of the subject QAOU is equal to or greater than the threshold, the offset
   information storage section 621 changes the size of the list memory to be secured to be half the memory size to be set
   when the hierarchical depth of the subject QAOU is smaller than the threshold.
10 [0423]
              The QAOU controller 622 controls the individual components included in the adaptive offset filter processor
   62.   The QAOU controller 622 also refers to the QAOU structure information, and then splits the deblocked decoded
   image PDB into one or a plurality of QAOUs and scans the individual QAOUs in a predetermined order.                The QAOU
   controller 622 also supplies a QAOMU number representing a subject QAOMU to the offset-type determining section
15 623.
   [0424]
              The offset-type determining section 623 refers to the map memory and the list memory of the offset
   information storage section 621 and determines the offset type specified by the QAOMU number supplied from the
   QAOU controller 622.       The offset-type determining section 623 also supplies the determined offset type to the
20 classifying section 624.
   [0425]
              The classifying section 624 classifies each pixel included in the subject QAOU as one of the classes that can be
   selected for the offset type supplied from the offset-type determining section 623.      The classifying section 624 also
   supplies the offset type and the class index indicating the class of each pixel to the offset determining section 625.
25 Specific classifying processing performed by the classifying section 624 will be discussed later, and thus, an explanation
                                                                102

   thereof is omitted here.
   [0426]
             The offset determining section 625 refers to the list memory of the offset information storage section 621, and
   determines, for each pixel included in the subject QAOU, an offset specified by the offset type and the class index
 5 supplied from the classifying section 624.     The offset determining section 625 includes an offset reverse shifter (not
   shown) which performs bitwise left shift on an offset by an amount equal to the shift value set by the offset attribute
   setting section 613.   The offset reverse shifter performs inverse quantization on the offset so that the offset bit depth
   may match the pixel bit depth.      By performing such inverse quantization, in addition processing performed by the offset
   adder 626, which will be discussed later, the offset can be added to the pixel value by using the same bit depth.     The
10 inverse-quantized offset for each pixel is supplied to the offset adder 626.
   [0427]
             The offset adder 626 adds an offset supplied from the offset determining section 625 to each pixel which forms
   the subject QAOU of the deblocked decoded image PDB.            The offset adder 626 outputs an image obtained by
   performing processing on all the QAOUs included in the deblocked decoded image PDB as an offset-filtered decoded
15 image P_OF.
   [0428]
             A description will now be given of classifying processing performed by the classifying section 624 with
   reference to Figs. 26 through 29.     Classifying processing performed by using a condition, such as the hierarchical depth
   of a QAOU, as a parameter condition, will be discussed below.       The configuration of a classifying section based on
20 general parameter conditions will be discussed later with reference to Fig. 34.
   [0429]
             Fig. 26 illustrates offset processing performed by the adaptive offset filter 60. Part (a) of Fig. 26 shows
   graphs indicating the magnitude relations between the pixel value pix[x] of a subject pixel x and the pixel value of a
   pixel a or b and also shows the values of a function Sign in accordance with the magnitude relation.      Part (b) of Fig. 26
25 shows graphs indicating the magnitude relations between the pixel value of the subject pixel x and each of the pixel
                                                                103

   values of the pixel a and the pixel b and also shows the value of EdgeType in accordance with the magnitude relation.
   Part (c) of Fig. 26 shows the association between each graph shown in part (b) of Fig. 26 and classidx.        Parts (d)
   through (f) of Fig. 26 illustrate transform tables used for transforming from EdgeType to classidx.
   [0430]
 5           Fig. 27 illustrates offset processing performed by the adaptive offset filter 60: part (a) schematically shows
   classifying performed when "saotypeidx="5; and part (b) schematically shows classifying performed when
   "saotypeidx=6".
   [0431]
             Fig. 28 illustrates offset processing performed by the adaptive offset filter 60: part (a) schematically shows
10 classifying performed when the hierarchical depth of a subject QAOU is smaller than a threshold; and part (b)
   schematically shows classifying performed when the hierarchical depth of the subject QAOU is equal to or greater than
   the threshold.
   [0432]
             Fig. 29 illustrate an example of classifying processing when a band offset is specified: part (a) shows an
15 example of classifying processing when the hierarchical depth of a subject QAOU is smaller than a threshold; and part
   (b) shows an example of classifying processing when the hierarchical depth of the subject QAOU is equal to or greater
   than the threshold.
   [0433]
   (When Offset Type is one of 1 to 4 (Edge Offset))
20           When the offset type supplied from the offset-type determining section 623 is one of I to 4, the classifying
   section 624 performs the same processing as that discussed with reference to Fig. 10.
   [0434]
             Accordingly, as shown in part (a) of Fig. 26, concerning the magnitude relation between the pixel value pix[x]
   and the pixel value of the pixel a or b, when the pixel value pix[x] is smaller than the pixel value of the pixel a or b,
25 Sign(pix[x]-pix[a]) = -1, when the pixel value pix[x] is the same as the pixel value of the pixel a or b, Sign(pix[x]-pix[a])
                                                                 104

   = 0, and when the pixel value pix[x] is greater than the pixel value of the pixel a or b, Sign(pix[x]-pix[a])     1.   In the
   graphs shown in part (a) of Fig. 26, the black circle with pix[x] indicates the pixel value of the subject pixel x, and the
   black circle without pix[x] indicates the pixel value of the subject pixel a or b.   The vertical direction in the graphs
   shown in part (a) of Fig. 26 indicates the magnitude of the pixel value.
 5 [0435]
             Then, the classifying section 624 finds EdgeType according to the following mathematical equation (1-1) on
   the basis of Sign(pix[x] -pix[a]) and Sign(pix[x] -pix[b]).
   [0436]
   EdgeType      Sign(pix[x]-pix[a]) + Sign(pix[x]-pix[b]) + 2
10                                          ... (1-1)
             According to this equation, as shown in part (b) of Fig. 26, when the pixel values of both of the pixels a and b
   are greater than the pixel value pix[x], EdgeType=0.       When the pixel value of one of the pixels a and b is greater than
   the pixel value pix[x] and when the pixel value of the other pixel a or b is the same as the pixel value pix[x],
   EdgeType=1.      When the pixel value of one of the pixels a and b is smaller than the pixel value pix[x] and when the
15 pixel value of the other pixel a or b is the same as the pixel value pix[x], EdgeType=3.      When the pixel values of both
   of the pixels a and b are smaller than the pixel value pix[x], EdgeType=4.       When the pixel value of one of the pixels a
   and b is smaller than the pixel value pix[x] and when the pixel value of the other pixel a or b is greater than the pixel
   value pix[x], or when the pixel values of both of the pixels a and b are the same as the pixel value pix[x], EdgeType=2.
   [0437]
20           In part (b) of Fig. 26, the center black circle in each graph indicates the pixel value of the subject pixel x, and
   the black circles on both sides indicate the pixel values of the pixel a and the pixel b.   The vertical direction in the
   graphs shown in part (b) of Fig. 26 indicates the magnitude of the pixel value.
   [0438]
             Then, on the basis of the determined EdgeType, the classifying section 624 determines the class index
25 (classidx) of the class to which the subject pixel x belongs in the following manner.
                                                                  105

   [0439]
              class_idx = EoTbl[EdgeType]
   where EoTbl[EdgeType] is a transform table used for determining class_idx from EdgeType.              Specific examples of the
   transform table EoTbl are shown in parts (d) through (f) of Fig. 26.
 5 [0440]
             A transform table 100 IX shown in part (d) of Fig. 26 is a transform table which is not switched in accordance
   with the hierarchical depth of a subject QAOU.
   [0441]
             A transform table 1001A shown in part (e) of Fig. 26 and a transform table 1001B shown in part (f) of Fig. 26
10 are transform tables which are switched in accordance with the hierarchical depth of a subject QAOU.            If the
   hierarchical depth of a subject QAOU is smaller than a threshold, the transform table 1001A is used, and if the
   hierarchical depth of the subject QAOU is equal to or greater than the threshold, the transform table 100 1B is used.
   [0442]
              When the same transform table is used regardless of the hierarchical depth of a subject QAOU, as indicated by
15 the transform table 1001X, if there is no edge in an area constituted by the subject pixel x, the pixel a, and the pixel b
   (hereinafter, such a case will also be referred to as a case in which an area is flat), that is, when EdgeType=2, the
   classifying section 624 classifies the subject pixel x as class 0 ("classidx"=0).      The classifying section 624 also
   classifies EdgeType=0, 1, 3, 4 as class idx=1, 2, 3, 4, respectively.
   [0443]
20            When the transform table is not switched in accordance with the hierarchical depth of a subject QAOU and
   when the hierarchical depth of a subject QAOU is smaller than a threshold, as indicated by the transform table 100 1A, if
   there is no edge in an area constituted by the subject pixel x, the pixel a, and the pixel b (hereinafter, such a case will also
   be referred to as a case in which an area is flat), that is, when EdgeType=2, the classifying section 624 classifies the
   subject pixel x as class 0 ("classidx"=0).     The classifying section 624 also classifies EdgeType=0, 1, 3, 4 as
25 classidx=1, 3, 4, 2, respectively.    When the hierarchical depth of the subject QAOU is equal to or greater than the
                                                                  106

   threshold, as indicated by the transform table 100 1B, if there is no edge in an area constituted by the subject pixel x, the
   pixel a, and the pixel b (hereinafter, such a case will also be referred to as a case in which an area is flat), that is, when
   EdgeType=2, the classifying section 624 classifies the subject pixel x as class 0 ("class idx"=0).      The classifying
   section 624 also classifies EdgeType=0, 1 as class idx=1 and EdgeType=3, 4 as class idx=2.           Accordingly, in the
 5 transform table 1001B, one class index (class id) is associated with a plurality of edge types (EdgeType).
   [0444]
             It appears that the transform table 1001A may be formed in the same manner as the transform table 1001X.
   However, if the transform table 1001A is the same as the transform table 1001X, different class indexes (classidx) are
   applied to the same edge type (EdgeType) in relation to the transform table 100 1B, and processing varies depending on
10 the hierarchical depth.    For example, in the case of EdgeType=3, when the hierarchical depth is smaller than the
   threshold, EdgeType=3 is classified as classid=4, and when the hierarchical depth is equal to or greater than the
   threshold, EdgeType=3 is classified as classid=2.       Thus, processing varies depending on the hierarchical depth.
   [0445]
             Accordingly, by setting the transform table 1001A to be the transform table 1001B shown in part (e) of Fig. 26,
15 the same processing is performed regardless of the hierarchical depth.
   [0446]
   (When Offset Type is 5 or 6 (Band Offset))
             When the offset type supplied from the offset-type determining section 623 is 5 or 6, the classifying section
   624 classifies the pixel value of the subject pixel x as one of a plurality of classes in accordance with the pixel value
20 pix[x] of the subject pixel x.
   [0447]
             - When offset type is 5 (saotype idx=5)
             When the pixel value pix[x] of the subject pixel x satisfies: (max x 1/4)      pix[x]  (max x 3/4), the classifying
   section 624 classifies the subject pixel as a class other than 0.   That is, when the pixel value of the subject pixel is
25 contained within a range indicated by the hatched portion in part (a) of Fig. 11, the classifying section 624 classifies the
                                                                 107

    subject pixel as a class other than 0.    In the above-described condition, max indicates the maximum value that can be
    taken by the subject pixel x, and for example, max=255. When max=255, the above-described condition may be
    represented by 8      (pix[x]/8)  23, or 4    (pix[x]/1 6)   11.
    [0448]
 5-              When offset type is 6 (saotype idx=6)
               When the pixel value pix[x] of the subject pixel x satisfies: pix[x]     (max x 1/4) or (max x 3/4)    pix[x], the
    classifying section 624 classifies the subject pixel as a class other than 0.    That is, when the pixel value of the subject
    pixel is contained within a range indicated by the hatched portion in part (b) of Fig. 11, the classifying section 624
    classifies the subject pixel as a class other than 0.    In the above-described condition, max indicates the maximum value
10  that can be taken by the subject pixel x, and for example, max=255.          When max=255, the above-described condition
    may be represented by (pix[x]/8)       7 or 24    (pix[x]/8), or (pix[x]/16)  3 or 12    (pix[x]/16).
    [0449]
               The classifying processing performed by the classifying section 624 will be more specifically described below.
    [0450]
15             When the offset type is 5 or 6, the classifying section 624 determines the class index (classidx) of the class to
    which the subject pixel x belongs in accordance with the hierarchical depth of a subject QAOU in the following manner.
               - In a case in which the hierarchical depth of the subject QAOU is smaller than a threshold,
               class_idx =EoTbl[saotypeidx][pix[x]/>>BoRefBit32]; and
               - In a case in which the hierarchical depth of the subject QAOU is equal to or greater than the threshold
20             class_idx =EoTbl[saotypeidx][pix[x]/>>BoRefBitl6]
    where BoTbl[saotypeidx][pix[x]/>>BoRefBit32] and BoTbl[saotype idx][pix[x]/>>BoRefBitl6] are transform
    tables used for determining classidx from the pixel value pix[x] of the subject pixel x and saotypeidx.           When the
    image bit depth is indicated by PIC_DEPTH, BoRefBit32 and BoRefBit16 are values determined by PICDEPTH-5 and
    PIC_DEPTH-4, respectively, and are values obtained by quantizing a pixel value in 32 or 16 steps.          The quantized pixel
25  value is also indicated by pixquant.     Performing bitwise right shift by BoRefBit32 and BoRefBitl6 corresponds to
                                                                     108

   performing division by 1BoRefBit32 and 1BoRefBitl6, respectively.              The quantization widths of 1<BoRefBit32
   and 1<BoRefBitl6 are 1(8-5)=8 and 1(8-4)=16, respectively, when the bit depth of the pixel values is eight bits.
   Classifying processing when the quantization width is eight will be discussed below.        The quantization width is referred
   to as a "class width".
 5 [0451]
              The classifying section 624 performs classifying by changing the class width in accordance with the
   hierarchical depth of a subject QAOU.       For example, as shown in part (a) of Fig. 28, when the hierarchical depth of a
   subject QAOU is smaller than a threshold, the classifying section 624 divides the pixel values into 32 levels by setting
   the class width to be "8" and then performs classifying.    As shown in part (b) of Fig. 28, when the hierarchical depth of
10 a subject QAOU is equal to or greater than the threshold, the classifying section 624 divides the pixel values into 16
   levels by setting the class width to be "16" and then performs classifying.
   [0452]
              Specific examples of the transform table EoTbl are shown in Fig. 29.      In parts (a) and (b) of Fig. 29, "BO 1"
   indicates that "saotype index"=5, and "BO_2" indicates that "saotypeindex"=6.             A transform table 1301 shown in
15 part (a) of Fig. 29 is a transform table when the hierarchical depth of a subject QAOU is smaller than a threshold, and a
   transform table 1302 shown in part (b) of Fig. 29 is a transform table when the hierarchical depth of a subject QAOU is
   equal to or greater than the threshold.
   [0453]
              When the hierarchical depth of a subject QAOU is smaller than the threshold and when "sao type index"=5, as
20 shown in part (a) of Fig. 29, the classifying section 624 classifies the subject pixel x as one of class indexes I to 16 in
   accordance with the magnitude of pix[x] if the pixel value pix[x] of the subject pixel x satisfies the condition 8    <
   (pix[x]/8)    23.
   [0454]
              When "saotypeidx"=6, the classifying section 624 classifies the subject pixel x as one of class indexes I to
25 16 in accordance with the magnitude of pix[x] if the pixel value pix[x] of the subject pixel x satisfies the condition
                                                                109

   (pix[x]/8)    7 or 24    (pix[x]/8).
   [0455]
              When the hierarchical depth of a subject QAOU is equal to or greater than the threshold and when
   "saotypeindex"=5, as shown in part (b) of Fig. 29, the classifying section 624 classifies the subject pixel x as one of
 5 class indexes I to 8 in accordance with the magnitude of pix[x] if the pixel value pix[x] of the subject pixel x satisfies
   the condition 4     (pix[x]/16)    11.
   [0456]
              When "saotypeidx"=6, the classifying section 624 classifies the subject pixel x as one of class indexes 1 to 8
   in accordance with the magnitude of pix[x] if the pixel value pix[x] of the subject pixel x satisfies the condition
10 (pix[x]/16)     3 or 12   (pix[x]/16).
   (Configuration of Offset Information Decoding Section 611)
              Fig. 41 is a block diagram of the offset information decoding section 611 which changes the offset type to be
   used in accordance with a parameter type (parameter condition) of an adaptive offset and/or which changes the number
   of classes in accordance with a parameter condition of an adaptive offset.     The offset information decoding section 611
15 includes an adaptive offset type decoder 6111, an offset type selector 6112, an offset type decoder 6113, an adaptive
   offset decoder 6114, a number-of-offsets selector 6115, and an offset decoder 6116.       The parameter conditions are
   parameters other than values calculated from pixel values.     Examples of the parameter conditions are a block size
   (QAOU size), a color component (component), and QP, which will be discussed in appendixes later, in addition to the
   hierarchical depth and the offset type described above.
20 [0457]
              The adaptive offset type decoder 6111 is means for adaptively decoding offset types from QAOU information
   included in coded data in accordance with a parameter condition, and includes the offset type selector 6112 and the
   offset type decoder 6113.
   [0458]
25            The offset type selector 6112 is means for selecting the offset types to be used in accordance with the parameter
                                                                110

   condition.    One of the parameter conditions is the above-described hierarchical depth.     The number of offset types to
   be used when the hierarchical depth is not deep is smaller than that when the hierarchical depth is deep.     The offset type
   selector 6112 inputs a transform table, such as that discussed with reference to Fig. 28, to the offset type decoder 6113.
   The offset type selector 6112 also inputs the maximum number of usable offset types to the offset type decoder 6113.
 5 [0459]
              The offset type decoder 6113 decodes offset types from the input transform table and the maximum number.
   If the maximum number of offset types is N, the range of codes that can be taken is restricted to N from 0 to N-1,
   thereby reducing the number of bits required for coding codes.     For example, when the maximum value is greater than
   2"   but not greater than 2'", m-bit fixed-length coding can be used.   Truncated unary coding or Truncated Rice coding
10 having N-I as the maximum value may be used.
   [0460]
              The adaptive offset decoder 6114 is means for adaptively decoding offsets from QAOU information included in
   the coded data in accordance with a parameter condition, and includes the number-of-offsets selector 6115 and the offset
   decoder 6116.     The number-of-offsets selector 6115 is means for selecting the maximum number of offsets to be used
15 and the offset precision in accordance with a parameter condition.    One of the parameter conditions is the hierarchical
   depth.    The number of offsets when the hierarchical depth is not deep is greater than that when the hierarchical depth is
   deep.    For example, when the hierarchical depth is not deep, sixteen offsets can be used, as shown in part (a) of Fig. 29,
   and when the hierarchical depth is deep, eight offsets can be used, as shown in part (b) of Fig. 29.    Additionally, as
   stated above, the offset precision (bit depth) can be changed in accordance with the hierarchical depth.     The number-of
20 offsets selector 6115 inputs the maximum number of offsets and the offset precision to the offset decoder 6116.       The
   offset decoder 6116 decodes offsets in accordance with the maximum number of offsets and the offset precision.          If the
   number of offsets is decreased, the amount of data to code offsets is reduced.   Additionally, as in the offset type, if the
   precision of each offset is determined, the range of a code for coding each offset that can be taken is also restricted,
   thereby reducing the number of bits required for coding a code.
25 [0461]
                                                               111

             Part (a) of Fig. 42 is a block diagram of a configuration of the offset type selector 6112.
   [0462]
             The offset type selector 6112 includes an offset-type transform table selector 6117, a first offset-type transform
   table storage section 6118, and a second offset-type transform table storage section 6119.
 5 [0463]
             The offset-type transform table selector 6117 selects a transform table stored in the first offset-type transform
   table storage section 6118 or a transform table stored in the second offset-type transform table storage section 6119 in
   accordance with a parameter condition.        In the above-described example, the transform table 801A is the transform
   table stored in the first offset-type transform table storage section 6118, and the transform table 801B is the transform
10 table stored in the second offset-type transform table storage section 6119.
   [0464]
             Part (b) of Fig. 42 is a block diagram of another configuration of the offset type selector.
   [0465]
             An offset type selector 6112' includes an offset type transform table selector 6117', an edge-offset-type-and
15 band-offset-type transform table storage section 6118', and a horizontal-edge-offset-type-and-band-offset-type transform
   table storage section 6119'.
   [0466]
   (Configuration of Classifying Section 624)
             Fig. 43 is a block diagram of the configuration of the classifying section 624.
20 [0467]
             The classifying section 624 includes an adaptive edge-offset classifying section 6241, an edge-offset class
   selector 6242, an edge-offset classifying section 6243, an adaptive band-offset classifying section 6244, a band-offset
   class-and-class-width selector 6245, and a band-offset classifying section 6246.
   [0468]
25           The classifying section 624 classifies each pixel as a class in accordance with a parameter condition and an
                                                                 112

   offset type.   When the offset type indicates an edge offset, the classifying section 624 classifies each pixel by using the
   adaptive edge-offset classifying section 6241.     When the offset type indicates a band offset, the classifying section 624
   classifies each pixel by using the adaptive band-offset classifying section 6244.
   [0469]
 5            The adaptive edge-offset classifying section 6241 is means for adaptively classifying each pixel as a class in
   accordance with a parameter condition, and includes the edge-offset class selector 6242 and the edge-offset classifying
   section 6243.     The edge-offset class selector 6242 selects the type of class to be used.    The edge-offset class selector
   6242 inputs a classifying method for classifying pixel values into the edge-offset classifying section 6243.        More
   specifically, the edge-offset class selector 6242 inputs a method for determining an intermediate value EdgeType which
10 is temporarily determined when a pixel value is classified and a transform table used for determining classidx from
   EdgeType.      An example of the method for determining the intermediate value EdgeType has been discussed with
   reference to parts (a) and (b) of Fig. 26, and this is referred to as a "basic edge classifying method".    Alternatively, an
   edge classifying method, which will be discussed later with reference to Fig. 33, may be used.        One of the parameter
   conditions is the hierarchical depth, as stated above, and the number of classes to be used when the hierarchical depth is
15 not deep is set to be greater than that when the hierarchical depth is deep.     Examples of the transform table used for
   determining class_idx from EdgeType are the transform tables 1001A and 1001B.              A method for switching the edge
   determining method in accordance with the color component (component) may be suitably used.              In this case, for the
   luminance components, the basic edge classifying method for classifying edges in the horizontal, vertical, and oblique
   directions are suitably used, and, for the chrominance components, a horizontal edge classifying method is suitably used
20 in order to reduce the number of line memories.       In the horizontal edge classifying method, the range of reference
   pixels used for classifying edges is restricted to the horizontal direction with respect to a subject pixel.   The edge-offset
   classifying section 6243 classifies pixels on the basis of the supplied classifying method and the transform table used for
   determining classidx from EdgeType.
   [0470]
25            The adaptive band-offset classifying section 6244 is means for adaptively classifying each pixel as a class in
                                                                  113

   accordance with a parameter condition, and includes the band-offset-class-and-class-width selector 6245 and the band
   offset classifying section 6246.    The band-offset-class-and-class-width selector 6245 inputs a classifying method for
   classifying pixel values into the band-offset classifying section 6246.    More specifically, the band-offset-class-and
   class-width selector 6245 inputs a class width, which is a quantization width used for classifying a pixel value as an
 5 intermediate value, and a transform table used for determining classidx from the intermediate value.       One of the
   parameter conditions is the hierarchical depth, as stated above, and the class width to be used when the hierarchical
   depth is not deep is smaller than that when the hierarchical depth is deep.    The band-offset classifying section 6246
   classifies a pixel value as a class in accordance with the received class width and the transform table used for
   determining class_idx from the intermediate value.      The class width, which is an input, does not have to be a class
10 width itself, and may be an integer corresponding to the class width.     For example, if lBoRefBit32 is used as the
   class width, the number of bits BoRefBit32, which is the logarithm to the base 2, used for quantizing a pixel may be
   used instead of the class width, or the value for determining the number of bits used for quantizing a pixel from the pixel
   bit depth, for example, if BoRefBit32=PIC DEPTH-5, 5 may be used instead of the class width.
   [0471]
15            In this manner, coded data is adaptively decoded in accordance with a parameter condition, and classifying of
   pixels is performed.
   [0472]
             As described above, in this embodiment, the number of classes used for classifying pixels is changed in
   accordance with the hierarchical depth of a subject QAOU.        More specifically, when the hierarchical depth of a subject
20 QAOU is deep, the number of classes is smaller than that when the hierarchical depth of the subject QAOU is not deep.
   [0473]
              Generally, the required memory space is represented by: the memory space = the data length of an offset     x the
   number of classes x the number of QAOUs.         Accordingly, by decreasing the number of classes, the memory space to be
   used can also be reduced.
25 [0474]
                                                                114

             In a deeper hierarchical level, the area of a QAOU becomes smaller, and also, the characteristics of pixel values
   within such a QAOU are almost uniform.        Accordingly, even if the number of classes is decreased, the effect of offsets
   is not considerably impaired.
   [0475]
 5           Additionally, in this embodiment, the offset precision is changed in accordance with the hierarchical depth of a
   subject QAOU.      More specifically, when the hierarchical depth of a subject QAOU is deep, the offset precision is lower
   than that when the hierarchical depth of a subject QAOU is not deep.        With this arrangement, the coding amount
   required when the hierarchical depth of a subject QAOU is deep can be reduced.
   [0476]
10           In a deeper hierarchical level of a QAOU, the number of pixels included in such a hierarchical level is smaller,
   and thus, quantization errors in the entire QAOU are also smaller than those in a higher hierarchical level.       Accordingly,
   even if the offset precision is decreased in a deeper hierarchical level, the effect of offsets is not considerably impaired.
   [0477]
   (Video Coding Device 2')
15           A video coding device 2' for generating coded data #1 by coding a subject image will be described below with
   reference to Figs. 30 and 31.    As in the video coding device 2, the video coding device 2' partially includes a method
   defined in H. 264/MPEG-4. AVC, a method used in KTA software, which is a joint development codec in VCEG (Video
   Coding Expert Group), a method used in TMuC (Test Model under Consideration) software, which is a successor codec
   to the codec used in KTA software, and a technology used in HM (HEVC TestModel) software.
20 [0478]
             The video coding device 2' includes an adaptive offset filter 80' instead of the adaptive offset filter 80 of the
   video coding device 2.     The other elements of the configuration of the video coding device 2' are similar to those of the
   video coding device 2.
   [0479]
25 (Adaptive Offset Filter 80')
                                                                115

              An adaptive offset filter 80' will be discussed below with reference to Fig. 30.       Fig. 30 is a block diagram
   illustrating the configuration of the adaptive offset filter 80'.    The adaptive offset filter 80' includes, as shown in Fig. 30,
   an adaptive offset filter information setting unit 81' and an adaptive offset filter processor 82'.
   [0480]
 5            The adaptive offset filter information setting unit 81' includes, as shown in Fig. 30, an offset calculator 811, an
   offset shifter 816, an offset clipping section 812, an offset information selector 813 and an offset attribute setting section
   815.
   [0481]
   (Offset Calculator 811)
10            The offset calculator 811 calculates offsets concerning all the offset types and all the classes for all QAOUs up
   to a predetermined split depth included in the unit of processing (for example, an LCU) in accordance with the
   hierarchical depth of a subject QAOU.        In this case, the offset types and the classes are the same as those discussed in a
   description of the video decoding device 1.
   [0482]
15            The offset calculator 811 supplies offset information indicating offsets calculated by the above-described
   processing, offset types, classes, and QAOU structure information indicating the QAOU split structure to the offset
   clipping section 812.
   [0483]
   (Offset Clipping Section 812)
20            The offset clipping section 812 performs clip processing by using one of the following first clip processing and
   second clip processing on offsets supplied from the offset calculator 811.
   [0484]
   (First Clip Processing)
              The offset clipping section 812 clips each offset supplied from the offset calculator 811 to, for example, values
25 from -8 to 7, thereby expressing each offset by four bits.      The clipped offsets are supplied to the offset information
                                                                   116

   selector 813.     The bit width used for clipping is set in accordance with the image bit depth and the offset bit depth, as in
   the video decoding device 1.
   [0485]
              By clipping each offset in this manner, the memory size of a memory (not shown) to store each offset can be
 5 reduced.     The amount of data required to code offsets included in the coded data #1 can also be decreased, thereby
   making it possible to improve the coding efficiency.       Additionally, since excessive offsets are not added, a suitable level
   of image quality can be guaranteed.
   [0486]
   (Second Clip Processing)
10            The offset clipping section 812 may differently set the clipping range of each offset supplied from the offset
   calculator 811 in accordance with the offset type.
   [0487]
              For example, if the offset type is an edge offset, the number of bits of an offset is set to be eight bits, and if the
   offset type is a band offset, the number of bits of an offset is set to be four bits. More generally, when the number of
15 bits of an edge-offset offset is N bits and the number of bits of a band-offset offset is M bits, the numbers of bits of
   offsets are determined so that the condition N>M can be satisfied.
   [0488]
              In this manner, by varying the number of bits of an offset in accordance with the offset type, the coding
   efficiency can be improved without requiring an excessive memory size of a memory for storing each offset.
20 [0489]
              When the threshold th for restricting values that can be taken as an offset is greater than 2 '" but not greater
   than 2'", m-bit fixed-length coding method can be used as the coding method for coding the offset.          More specifically,
   Truncated unary coding or Truncated Rice coding having th as the maximum value may be used.
   [0490]
25            Clip processing performed by a combination of the above-described first clip processing and second clip
                                                                  117

   processing is also included in this embodiment.     The adaptive offset filter 80 may not include the offset clipping section
   812.
   [0491]
              The offset clipping section 812 changes the clipping range in accordance with the hierarchical depth of a
 5 subject QAOU so as to match the offset precision.      More specifically, a case in which the offset precision is   nA bits
   when the hierarchical depth of a subject QAOU is smaller than a threshold and the offset precision is np bits when the
   hierarchical depth of a subject QAOU is equal to or greater than the threshold will be considered.       In this case, when
   the hierarchical depth of a subject QAOU is smaller than the threshold, the offset clipping section 812 sets the clipping
   range to be -2n^2 to 2 "^-1. When the hierarchical depth of a subject QAOU is equal to or greater than the threshold,
10 the offset clipping section 812 sets the clipping range to be -2nA2 to 2 n^2 1 , and then sets the lower (nA-nB) bits to be "0".
   This enables the offset information selector 813 to perform processing by using offsets subjected to clipping processing
   performed by the offset clipping section 812 without considering the offset precision.
   [0492]
   (Offset Information Selector 813)
15            The offset information selector 813 determines a combination of an offset type, classes, and offsets which
   minimize the RD cost (Rate-Distortion cost) and the associated QAOU split structure, and supplies QAOU information
   indicating the determined offset type, classes, and offsets, and the associated QAOU split structure to the variable-length
   code coder 22.     The offset information selector 813 also supplies the determined offsets for each QAOU to the adaptive
   offset filter processor 82.
20 [0493]
              Processing performed by the offset information selector 813 will be discussed more specifically with reference
   to Fig. 31.
   [0494]
              Fig. 31 shows an overview of a case in which a squared error concerning a QAOU having a QAOU index "x"
25 is calculated with respect to each offset type.   As shown in Fig. 31, the offset information selector 813 calculates a
                                                                118

   squared error concerning each QAOMU with respect to each offset type.             Then, the offset information selector 813 sets
   the offset type which minimizes the squared error to be the offset type of the QAOU.            As a result, offset types for all the
   QAOMUs (QAOMU numbers 0 to 340) are determined.
   [0495]
 5            Then, the offset information selector 813 calculates the RD cost when the split depth is 0 and the RD cost when
   the split depth is 1.   A specific calculation method is the same as that discussed with reference to Fig. 18.
   [0496]
   (Adaptive Offset Filter Processor 82')
              The adaptive offset filter processor 82' adds an offset supplied from the offset information selector 813 to each
10 pixel of a subject QAOU in the deblocked decoded image P_DB.              The adaptive offset filter processor 82' outputs, as an
   offset-filtered decoded image POF, an image obtained by performing processing on all the QAOUs included in the
   deblocked decoded image PDB.           The configuration of the adaptive offset filter processor 82' is the same as that of the
   adaptive offset filter processor 62', and thus, an explanation thereof is omitted here.
   [0497]
15            Alternatively, an adaptive clip type may be utilized.      That is, as one of offset types, an adaptive clip (AC) type
   may be used.     In this case, there are three offset types, EO, BO, and AC.       In the adaptive clip type (AC), a pixel value
   is corrected by using clips, such as a lower limit value cI and an upper limit value c2, without using offsets.          Examples
   of the lower limit value cI and the upper limit value c2 are c1=16 and c2=235, respectively.
   [0498]
20            The use of the adaptive clip type eliminates the need to perform offset coding processing and to provide a
   memory for storing many offsets.       If the lower limit value and the upper limit value used for clipping are adaptively
   provided instead of using fixed values, they are coded.       In this case, differences from suitable fixed values, for example,
   an offset from 16 in the case of the lower limit value and an offset from 235 in the case of the upper limit value may
   preferably be coded.     The upper limit value, particularly, may become a large value.         However, the coding efficiency
25 is not decreased by coding in this manner.
                                                                  119

   [0499]
   (Appendix 2)
              As stated above, "in accordance with the hierarchical depth of a QAOU" may be read as "in accordance with
   the size of a QAOU".       That is, the number of SAO types, the number of SAO classes, and the offset precision may be
 5 changed in accordance with the size of a QAOU.
   [0500]
              For example, if the size of a QAOU is smaller than NxN pixels, one or a plurality of factors, such as the
   number of SAO types, the number of SAO classes, and the offset precision, may be restricted more heavily than that
   when the size of a QAOU is equal to or greater than NxN pixels.        A specific value of N is, for example, N=64 (size of
10 an LCU).
   [0501]
              The number of SAO types may be restricted, for example, as follows: EO is restricted to a horizontal or vertical
   type, BO is restricted to one type (transform table 802C), only EO is used and BO is not used (transform table 802A),
   and only horizontal EO and only some modes of BO is used (transform table 801B or 802D).             The transform tables
15 indicated in the parentheses are used in the offset information decoding section 611.      The restriction of the number of
   SAO classes and the offset precision may be imposed in a manner similar to the above-described embodiments.
   [0502]
              By restricting the modes of SAO types, the number of SAO types, the number of SAO classes, and the offset
   precision in this manner, a memory space to be used can be decreased and the processing load can be reduced.           In
20 particular, if the modes of SAO types are restricted to BO or if EO is restricted to a horizontal or vertical type, the
   processing load in the coding device and the classifying section of the decoding device can be reduced.        If the number
   of types is restricted, cost calculations for selecting an optimal type can be reduced, thereby reducing the processing load
   in the coding device.     If the modes of SAO types are restricted to EO or if the number of classes and the offset precision
   are restricted, a memory space to be used for storing offsets can be reduced.     Moreover, by restricting the modes of the
25 SAO types to horizontal EO and BO, the number of temporary memories, such as line memories for storing reference
                                                                  120

   pixels used for classifying, can be reduced, and also, a delay occurring while waiting for the decoding of reference pixels
   can be decreased.     In order to reduce a memory space to be used, it is more effective if the number of BO classes is
   restricted than the number of EO classes is restricted.
   [0503]
 5            The above-described restrictions can be implemented by the configuration of the means shown in Figs. 41
   through 43 if the operations of the offset information decoding section 611 and the classifying section 624 are adaptively
   changed in accordance with the size of a QAOUN used as a parameter condition.
   [0504]
   (Appendix 3)
10           Additionally, the size of a QAOU to be used when adding an offset to a luminance value (hereinafter also
   referred to as a "luminance unit") may be made different from the size of a QAOU to be used when adding an offset to a
   chrominance value (hereinafter also referred to as a "chrominance unit").      Then, the modes of SAO types, the number
   of SAO types, the number of SAO classes, the offset precision, and the maximum split hierarchical level may be
   restricted in a chrominance unit more heavily than in a luminance unit.
15 [0505]
              For example, in a data format in which the resolution of the luminance is different from that of the
   chrominance and the resolution of the chrominance is lower than that of the luminance, such as in YUV 4:2:0 image
   format, it is not necessary to split a chrominance unit to a smaller level than a luminance unit.  Accordingly, by
   restricting the modes of SAO types, the number of SAO types, the number of SAO classes, and offset precision, and the
20 maximum split hierarchical level of a chrominance unit more heavily than those of a luminance unit, a memory space to
   be used can be decreased and the processing load can be reduced.       The maximum split hierarchical level of a
   chrominance unit can be restricted by making the maximum hierarchical depth of a SAO tree structure smaller than that
   of a luminance unit.
   [0506]
25            The above-described restrictions can be implemented by the configuration of the means shown in Figs. 41
                                                                121

   through 43 if the operations of the offset information decoding section 611 and the classifying section 624 are adaptively
   changed in accordance with the size of a QAOUN used as a parameter condition.
   [0507]
              In particular, when SAO processing is performed for both of the luminance and the chrominance, a temporary
 5 memory, such as a line memory for storing reference pixels for classifying, is required for each color component.           Since
   the luminance component is less important than the chrominance components, the chrominance components are used as
   a parameter condition, and the modes of SAO types is restricted to horizontal EO and BO, thereby making it possible to
   reduce the number of line memories for the chrominance components.             In this case, in the offset information decoding
   section 611, a transform table, such as the transform table 801B or 802D, is utilized.
10 [0508]
              It is also particularly effective if the chrominance components are used as a parameter condition and the offset
   precision for the chrominance components is set to be smaller than that for the luminance component.             For example, if
   the offset precision for the luminance component is set to be:
              SAODEPTH = MIN(PICDEPTH, AY)
15            shift value = PIC_DEPTH - MIN(PICDEPTH, AY); and
   the offset precision and the shift value for the chrominance components are set to be:
              SAODEPTH = MIN(PICDEPTH, THC)
              shift value = PIC_DEPTH - MIN(PICDEPTH, AC),
   the variables AY and AC for controlling the precision are suitably determined such that they may satisfy AY>AC.             For
20 example, AY is set to be 10 or 9, and AC is set to be 8.     In this case, the offset precision for the luminance component is
   higher than that for the chrominance components.         The shift value for the luminance component is lower than that for
   the chrominance components.
   [0509]
   (Appendix 4)
25            Moreover, when the value of a quantization parameter QP (QP value) for a CU is equal to or greater than a
                                                                  122

   threshold, the modes of SAO types, the number of SAO types, the number of SAO classes, the offset precision, and the
   maximum split hierarchical level may be restricted.        As the QP value, the QP value of the leading portion of a picture
   may be used, or if a QAOU to be subjected to SAO processing faces a boundary of an LCU or a CU, the QP value
   corresponding to the top left coordinates or the central coordinates of the QAOU may be used.
 5 [0510]
              As the QP value becomes greater, the image quality of a prediction image becomes lower, and it is more
   difficult to perform detailed classifying processing and correction by using offsets.    Accordingly, even if the modes of
   SAO types, the number of SAO types, the number of SAO classes, the offset precision, and the maximum split
   hierarchical level are restricted, the influence on the image quality is small.   Thus, when the QP value is equal to or
10 greater than a threshold, the number of SAO types, the number of SAO classes, the offset precision, and the maximum
   split hierarchical level are restricted.   Then, it is possible to decrease a memory space to be used and to reduce the
   processing load without influencing the image quality.
   [0511]
              The above-described restrictions can be implemented by the configuration of the means shown in Figs. 41
15 through 43 if the operations of the offset information decoding section 611 and the classifying section 624 are adaptively
   changed in accordance with QP used as a parameter condition.
   [0512]
   (Appendix 5)
              Moreover, for a specific picture type, for example, B pictures and non-reference pictures (IDR pictures), the
20 modes of SAO types, the number of SAO types, the number of SAO classes, the offset precision, and the maximum split
   hierarchical level may be restricted.
   [0513]
              Concerning I pictures and P pictures, the image quality of these pictures greatly influence the subsequent
   pictures, and thus, it is necessary to maintain the SAO precision at a high level.    On the other hand, pictures other than I
25 pictures and P pictures do not greatly influence the subsequent pictures, and thus, concerning these pictures (B pictures
                                                                   123

   and IDR pictures), the modes of SAO types, the number of SAO types, the number of SAO classes, the offset precision,
   and the maximum split hierarchical level may be restricted.     Then, it is possible to decrease a memory space to be used
   and to reduce the processing load.
   [0514]
 5           The above-described restrictions can be implemented by the configuration of the means shown in Figs. 41
   through 43 if the operations of the offset information decoding section 611 and the classifying section 624 are adaptively
   changed in accordance with a picture type used as a parameter condition.
   [0515]
   (Appendix 6)
10           Additionally, in accordance with the position of a QAOU on a screen, the number of SAO types, the number of
   SAO classes, the offset precision, and the maximum split hierarchical level may be restricted.
   [0516]
             For example, at the periphery of a screen, the number of SAO types, the number of SAO classes, the offset
   precision, and the maximum split hierarchical level may be restricted.
15 [0517]
             A user is likely to focus on and around the center of the screen, and thus, a decrease in the image quality at this
   portion directly reflects the subjective image quality.  However, even if the image quality around the periphery of the
   screen is decreased, the subject image quality is not decreased to the same level as that when it is decreased on and
   around the center of the screen.
20 [0518]
             Additionally, if a SAO type which requires sample points beyond the edge of a screen is not used, the load in
   boundary determining processing can be reduced.
   [0519]
             The above-described restrictions can be implemented by the configuration of the means shown in Figs. 41
25 through 43 if the operations of the offset information decoding section 611 and the classifying section 624 are adaptively
                                                               124

   changed in accordance with the position of a QAOU used as a parameter condition.
   [0520]
   [Fourth Embodiment]
             Another embodiment of the present invention will be described below with reference to Fig. 32.           A fourth
 5 embodiment is different from the above-described embodiments in that EO and BO are switched in accordance with a
   pixel value.
   [0521]
              The present inventors have found that there are characteristics in which EO is more effective than BO in an
   intermediate tone area of pixels and BO is more effective than EO in the other areas (a lower pixel value area and a
10 higher pixel value area).   Accordingly, in this embodiment, BO is used in the lower pixel value area and the higher
   pixel value area and EO is used in the intermediate tone area.
   [0522]
              This will be described below more specifically with reference to Fig. 32.      Fig. 32 illustrates the configuration
   in which EO and BO are switched in accordance with the pixel value: part (a) shows an overview of the configuration in
15 which EO and BO are switched in accordance with the pixel value; part (b) shows specific values to be switched; and
   part (c) shows the content of a list memory stored in the offset information storage section 621.
   [0523]
             As shown in part (a) of Fig. 32, in this embodiment, BO is used when the pixel value (luminance value) is
   around 0 or 255, and EO is used when the pixel value is in the other areas.      That is, the SAO type is changed depending
20 on the pixel value, and when the pixel values are around 0 or 255, an offset is added in accordance with the pixel value
   of a pixel, and when the pixel value is in the other areas, an offset is added in accordance with the type of edge.
   [0524]
              It has been found through the inventors' experiments that there are particular ranges of pixel values to be used
   both for EO and BO.      That is, EO is more frequently used in the intermediate tone area.      This means that, in the lower
25 pixel value area and in the higher pixel value area, pixel values influence errors more greatly than the type of edge.
                                                                125

   [0525]
             Accordingly, in this embodiment, EO and BO are switched in accordance with the pixel value range, and in one
   SAO type, EO or BO having a higher error correcting effect is utilized, thereby making it possible to improve the coding
   efficiency.    Additionally, since it is possible to decrease the number of SAO types and the total number of classes, a
 5 memory space to be used can be decreased and the processing load can also be reduced.
   [0526]
              This embodiment will be described below in a greater detail.      Unlike the above-described embodiments, in
   this embodiment, the following four SAO types, "saotypeidx"=1 to "saotype idx"=4, are used and are defined as
   follows.
10              "saotypeidx"=1: (BO_EO_0) corresponding to EO_0+BO in the first embodiment
                "saotypeidx"=2: (BO_EO_1) corresponding to EO_1+BO in the first embodiment
              - "saotypeidx"=3: (BO EO_2) corresponding to EO_2+BO in the first embodiment
              - "saotypeidx"=4: (BO_EO_3) corresponding to EO_3+BO in the first embodiment
              EO_0 to 3 and BO 0, 1 in the first embodiment are not used.
15 [0527]
              Then, as shown in part (b) of Fig. 32, the classifying section 624 performs classifying by using BoTbl[BO_1]
   in the transform table 1301 shown in Fig. 29.       When the pixel value is 1/4 of max or smaller or is 3/4 of max or greater,
   the classifying section 624 performs classifying by using the transform table 1301.
   [0528]
20            When the pixel value is between 1/4 and 3/4 of max, the classifying section 624 determines the edge type in
   accordance with EO_0 to 3 discussed in the above-described embodiments, and performs classifying.
   [0529]
              With this operation, the classifying section 624 is able to perform classifying by switching between BO and EO
   in accordance with the pixel value.
25 [0530]
                                                                  126

              The offset calculator 811 calculates offsets in a manner similar to the offset calculating method discussed in the
   above-described embodiments.        However, in this embodiment, the number of modes of offset types is four, and the
   offset calculator 811 calculates offsets by changing the number of modes of offset types to four.
   [0531]
 5            In a list memory 2101 stored in the offset information storage section 621, as shown in part (c) of Fig. 32, the
   QAOU index, the offset type, and specific values of offsets with respect to classes that can be selected for this offset type
   are stored in association with each other.
   [0532]
              When an offset type having characteristics of EO and BO is used, classifying may be first performed by using
10 EO, and then, for some classes (classes having a flat edge), classifying may be further performed by using BO, or both
   of EO and BO may be used, as discussed in this embodiment.          Alternatively, it may be determined depending on the
   condition discussed in the above-described embodiments (depending on whether the hierarchical depth of a QAOU is
   smaller than a threshold) whether an offset type having characteristics of EO and BO will be provided.        In this case, the
   number of types and the number of classes can be reduced.
15 [0533]
   [Fifth Embodiment]
             Another embodiment of the present invention will be described below with reference to Figs. 33, 34, and 42.
   A fifth embodiment is different from the above-described embodiments in that when performing classifying using EO,
   pixels used for determining the type of edge are restricted to pixels positioned in the horizontal direction of a subject
20 pixel.
   [0534]
              In the above-described embodiments, a pixel positioned in the upward or downward direction of a subject pixel
   is also used for determining the type of edge.    Accordingly, for a pixel (pixel subjected to offset processing) positioned
   in the upward direction of the subject pixel, a line buffer for storing the pixel value of this pixel which has not been
25 subjected to offset processing is necessary.
                                                                 127

   [0535]
             Accordingly, by restricting pixels used for determining the type of edge to pixels positioned in the horizontal
   direction of a subject pixel, no reference is made to pixels in the upward direction, thereby making it possible to reduce a
   memory space by an amount equal to a line buffer.       Moreover, processing for determining a boundary in the upward
 5 direction (screen edge) is not necessary, thereby making it possible to reduce the amount of processing.
   [0536]
             Additionally, if pixels positioned only in the horizontal direction are used, the processing results which have
   been obtained so far can be utilized, thereby making it possible to further reduce the amount of processing.     This will
   be discussed below with reference to part (a) of Fig. 33.    Part (a) of Fig. 33 shows pixels positioned in the horizontal
10 direction, and xO through x3 indicate pixel values of pixels.
   [0537]
              Concerning the pixel having the pixel value xl, the difference between this pixel and the two adjacent pixels
   are as follows.
              sI    sign(x1-xO) - sign(x2-x1)
15            Then, the difference between the pixel having the pixel value x2 and the two adjacent pixels are as follows.
              s2 = sign(x2-xl) - sign(x3-x2)
              In the above-described expressions, s I and s2 are values used for classifying an edge.
   [0538]
              In this manner, when calculating s2, sign(x2-x1) used for calculating s I is reused.  Accordingly, the amount of
20 processing can be reduced by the amount of this reuse.
   [0539]
              Details of this embodiment will be discussed below.     In this embodiment, the EO type is restricted to two
   modes.     That is, "saotype idx"=1, 2 are used as edge offsets (EO).      When "saotypeidx"=1 (EO_0), the type of
   edge is determined by comparing a subject pixel with pixels positioned immediately on the right and left sides of the
25 subject pixel (part (b) of Fig. 33).   When "saotypeidx"=2 (EO'_0), the type of edge is determined by comparing a
                                                                128

   subject pixel with pixels positioned two pixels away from the subject pixel on the right and left sides (part (c) of Fig. 33).
   [0540]
              When "saotypeidx"=3, 4, a band offset (BO) is used.           In this case, "saotypeidx"=5, 6 in the first
   embodiment may be read as "saotypeidx"=3, 4, respectively.
 5 [0541]
              The purpose of determining the type of edge by using pixels positioned two pixels away from a subject pixel
   when "sao type idx"=2 is to facilitate the detection of an edge having a small angle with respect to the horizontal
   direction.   As shown in part (d) of Fig. 33, if two pixels immediately next to a subject pixel are used for comparison,
   the difference between the pixel value of the subject pixel and those of such adjacent pixels is small, and may not be
10 judged as an edge.     Even in this case, if the subject pixel is also compared with pixels positioned two pixels away from
   the subject pixel, an edge having a small angle with respect to the horizontal direction can be detected.
   [0542]
              Instead of using two pixels immediately next to a subject pixel, a pixel positioned immediately on the left side
   of a subject pixel and a pixel positioned two pixels away from the subject pixel on the right side (part (a) of Fig. 34) may
15 be used for determining the difference.      Conversely, a pixel positioned immediately on the right side of the subject pixel
   and a pixel positioned two pixels away from the subject pixel on the left side (part (b) of Fig. 34) may be used for
   determining the difference.    These cases are particularly effective when reference pixels are positioned at an edge of a
   screen.
   [0543]
20            The use of pixels positioned only in the horizontal direction may be restricted to some cases.      For example, in
   the case of pictures which influence other pictures less, such as B pictures and non-reference pictures, reference may be
   made only to pixels in the horizontal direction.     In the case of other pictures, reference may be made to pixels in a
   manner similar to the first embodiment.       Alternatively, at an edge of a screen or near a boundary of a slice, reference
   may be made only to pixels in the horizontal direction, and at the other areas, reference may be made to pixels in a
25 manner similar to the first embodiment.       With this arrangement, a memory space to be used can be decreased, and the
                                                                  129

   amount of processing can also be reduced.        Additionally, since no reference is made in the upward direction, the
   amount of boundary determining processing for determining boundaries, such as an edge of a screen or a boundary of a
   slice, can also be reduced.
   [0544]
 5            Pixels to be used may be explicitly specified by using a flag for each picture or each block.
              For example, if there are many horizontal edges (part (c) of Fig. 34), processing is performed by using the
   method discussed in the above-described embodiments, thereby suppressing a decrease in the performance.
   [0545]
             Alternatively, it may be determined, depending on the condition discussed in the above-described embodiments
10 (depending on whether the hierarchical depth of a QAOU is smaller than a threshold), whether pixels positioned only in
   the horizontal direction will be used.    Moreover, only when BO and EO are both used, as discussed in the above
   described embodiment, may pixels positioned only in the horizontal direction be used for EO.
   [0546]
              The type of edge may be determined depending on whether the difference between a subject pixel and a
15 reference pixel positioned in the horizontal direction of the subject pixel is greater (or smaller) than a threshold.  More
   specifically, classifying may be performed according to the following equations.
   [0547]
              Sign(z) - +1        (when z > th)
              Sign(z) = 0         (when -th    z   th)
20            Sign(z) = -1        (when z < -th)
   In the these equations, th denotes a threshold having a predetermined value.
   [0548]
              In the case of a pixel value indicating a chrominance component, pixels positioned only in the horizontal
   direction may be used for an edge offset.
25 [0549]
                                                                 130

              The effect obtained by restricting pixels to those in the horizontal direction may be achieved even when only
   one horizontal-edge classifying method is provided, for example, even when the method shown in part (b) of Fig. 33 is
   used.    If, as described above, a plurality of different horizontal-edge classifying methods are combined, more precise
   edge classifying can be implemented than that when the method shown in part (b) of Fig. 33 is singly used.         A
 5 combination of a plurality of horizontal-edge classifying methods is not restricted to the use of two examples by
   changing the distance between a subject pixel and reference pixels, as in parts (b) and (c) of Fig. 33, but may be the use
   of two thresholds for determining the type of edge.       For example, two horizontal-edge classifying methods, that is, one
   method when the threshold th is 0 and another method when the threshold th is 1, may be used.
   [0550]
10            The configuration in which the use of pixels positioned only in the horizontal direction is restricted to some
   cases can be implemented by the configuration of the means shown in Figs. 41 through 43 of the first embodiment if the
   operations of the offset information decoding section 611 and the classifying section 624 are adaptively changed in
   accordance with a parameter condition.       For example, in the offset information decoding section 611 shown in Fig. 41,
   the modes of offset types to be selected in the offset type selector 6112 in accordance with a parameter condition are
15 restricted only to the horizontal-edge classifying methods, and the restricted modes of the offset types are decoded in the
   offset type decoder 6113.
   [0551]
              As shown in part (b) of Fig. 42, the offset type transform table selector 6117 selects the transform table stored
   in the edge-offset-type-and-band-offset-type transform table storage section 6118' or the transform table stored in the
20 horizontal-edge-offset-type-and-band-offset-type transform table storage section 6119' in accordance with the parameter
   condition.     In one example, if the color component is a luminance component, the offset type transform table selector
   6117 selects the transform table stored in the edge-offset-type-and-band-offset-type transform table storage section 6118'.
   If the color component is a chrominance component, the offset type transform table selector 6117 selects the transform
   table stored in the horizontal-edge-offset-type-and-band-offset-type transform table storage section 6119'.
25 [0552]
                                                                  131

             The offset type selector 6112 selects an edge classifying method which is restricted to a horizontal edge
   classifying method in accordance with the parameter condition, and, in the classifying section 624, the edge offset
   classifying section 6243 classifies pixels.   In one example, if the color component is a luminance component, the basic
   edge classifying method is selected, and if the color component is a chrominance component, a horizontal edge
 5 classifying method is selected.
   [0553]
   [Sixth Embodiment]
             Another embodiment of the present invention will be described below with reference to Figs. 35 through 37.
   A sixth embodiment is different from the above-described embodiments in that the offset precision is improved or more
10 precise classifying is performed around the center of the pixel values of chrominance components.
   [0554]
             An image has characteristics in which errors around an achromatic color are noticeable in terms of the
   subjective image quality.    In the pixel values of each chrominance component, an achromatic color is positioned at the
   center of the value range, and if the bit depth is eight bits, the achromatic color is a color having a pixel value of 128.
15 Accordingly, in this embodiment, in the case of BO, around the pixel value of 128 of a chrominance component, the
   offset precision is improved or more precise classifying is performed by using a smaller class width.       Concerning the
   pixel values of a chrominance component, the value range around an achromatic color is referred to as the "achromatic
   color value range".
   [0555]
20           With this arrangement, since the offset precision can be improved in the achromatic color value range, the
   performance in making corrections by using offsets in the achromatic color value range can be enhanced.           Accordingly,
   errors caused by the deviation from the original image in the achromatic color value range are decreased, thereby
   making it possible to improve the subjective image quality.        Moreover, by performing more precise classifying using a
   smaller class width in the achromatic color value range than in other value ranges, corrections using offsets in the
25 achromatic color value range can be performed precisely.         Accordingly, errors caused by the deviation from the original
                                                                  132

   image in the achromatic color value range are decreased, thereby making it possible to improve the subjective image
   quality.
   [0556]
              In this embodiment, the offset precision is improved or more precise classifying is performed for the
 5 achromatic color value range.      However, the value range subjected to this processing is not restricted to the achromatic
   color value range.    More precise classifying may be performed or the offset precision may be improved for a value
   range which is likely to influence the subjective image quality, thereby making it possible to improve the subjective
   image quality.     Additionally, for the pixel value of a luminance component, as well as that of a chrominance component,
   for a value range in which degradation of the image quality is likely to be noticeable in terms of the subjective image
10 quality, the offset precision may be similarly improved or more precise classifying may be similarly performed, thereby
   achieving advantages similar to those obtained for a chrominance component.
   [0557]
              A case in which the offset precision is improved will first be discussed below with reference to Fig. 35.     Fig.
   35 illustrates an overview of a case in which the offset precision is improved.      In order to improve the offset precision,
15 as shown in Fig. 35, the offset precision around the pixel value of 128 of a chrominance component is set to be high, and
   the offset precision in the other value ranges is set to be low.
   [0558]
              The offset information storage section 621 secures a storage area of np bits (for example, nB=8) for offsets used
   for a value range having a higher offset precision, and secures a storage area of nA bits (for example, nA= 6 ) for offsets
20 used for a value range having a lower offset precision.      If there is an enough storage area, the storage area may be set to
   be unconditionally nB bits, in which case, the implementation is facilitated.
   [0559]
              The offset clipping section 812 utilizes different clipping ranges for a value range having a high offset
   precision and a value range having a low offset precision.       For example, for a value range having a low offset precision,
25 the clipping range is set to be -2     to 2 "^-1, and for a value range having a high offset precision, the clipping range is
                                                                 133

   set to be - 2 nB 2 t0 2 nB /2 1
   [0560]
             A case in which more precise classifying is performed will now be discussed below with reference to Fig. 36.
   Fig. 36 illustrates an overview of a case in which more precise classifying is performed: part (a) shows a transform table;
 5 and parts (b) through (d) show that pixel values are divided.
   [0561]
              When more precise classifying is performed, in a list memory for a chrominance component in the offset
   information storage section 621, an offset storage area corresponding to "saotype idx"=5 is secured in accordance with
   the number of classifying levels.
10 [0562]
              In the case of BO, the classifying section 624 considers whether or not the pixel value of a subject pixel is
   contained within the achromatic color value range when determining a class from the pixel value.       More specifically,
   the classifying section 624 performs classifying by using one of the patterns in a transform table 2501 shown in part (a)
   of Fig. 36.
15 [0563]
              In the transform table 2501, classifying is performed by using "class_idx=BoTbl[saotype idx][pix[x]/4]", and
   classes are assigned to smaller widths of pixel values than those in the transform tables 1301 and 1302 shown in Fig. 29.
   [0564]
              "BoTbl[BO0][pix/4](a)" in the transform table 2501 shows an example (part (b) of Fig. 36) in which there are
20 sixteen classes, and the achromatic color value range is split into smaller widths and the intermediate tone areas
   (pix/4=16 to 21, 42 to 47) are split into larger widths.   "BoTbl[BO_0][pix/4](b)" shows an example (part (c) of Fig. 36)
   in which there are sixteen classes, and the range in which offsets are added is narrowed, and the achromatic color value
   range is split into smaller widths while the maximum number of classes and the maximum class width are maintained.
   "BoTbl[BO0][pix/4](c)" shows an example (part (d) of Fig. 36) in which there are eighteen classes, and the achromatic
25 color value range is split into smaller widths while the maximum class width is maintained.      Additionally, in another
                                                                134

   example, the class width of the intermediate tone areas is set to be relatively larger than that of the achromatic color
   value range, thereby decreasing the number of entire classes.       In this example, it is possible to reduce the space of the
   offset storage area while maintaining the correction precision in the achromatic color value range.          Such an
   arrangement is possible because the frequency with which offsets are used for correction in the intermediate tone areas is
 5 lower than that in the achromatic color value range.
   [0565]
             Alternatively, both of an improvement in the offset precision and more precise classifying in the achromatic
   color value range may be performed.
   [0566]
10           Alternatively, the offset precision may be changed according to whether the pixel value indicates a luminance
   component or a chrominance component.         That is, the offset precision for the pixel value indicating a chrominance
   component may be set to be lower than that for the pixel value indicating a luminance component.
   [0567]
   (Appendix 7)
15            The addition of an offset may be performed such that the pixel value approximates an achromatic color (=pixel
   value of 128 of a chrominance component).        That is, the offset adder 626 may handle offsets such that the sign of an
   offset to be added to the pixel value of 128 or greater is opposite to the sign of an offset to be added to the pixel value
   smaller than 128.     For example, when an offset is indicated by a, the sign of the offset may be determined such that
   x'=x+a (x<128) and x'=x-a(x>128).
20 [0568]
              If 128 is positioned between the pixel value to which an offset has not been added and the pixel value to which
   an offset has been added, the pixel value to which an offset has been added may be clipped by 128.            For example, if the
   pixel value is "126 (blue)" and the offset is "3", instead of directly adding the offset to the pixel value such that
   x=126+3=129, x may be set to x=clip(126+3, 0, 128)=128.           In this equation, clip(x, y, z) indicates processing for
25 restricting the value of x to ysxsz.
                                                                 135

   [0569]
             If the pixel value is "129 (red)" and the offset is "-4", instead of directly adding the offset to the pixel value
   such that x=129+(-4)=125, x may be set to x=clip(129+(-4), 128, 255)=128.
   [0570]
 5 (Appendix 8)
             For a chrominance component, offsets may be added in one of the intermediate tone areas and the other areas.
   For example, as shown in part (a) of Fig. 37, classifying may be performed by focusing on the pixel values of an
   achromatic color.    In the example shown in part (a) of Fig. 37, in the case of "saotypeidx"=5(BO_0), classes are
   assigned to the intermediate tone areas, and in the case of "saotypeidx"=6(BO_1), classes are assigned to value ranges
10 other than the intermediate tone areas.
   [0571]
             Alternatively, as shown in part (b) of Fig. 37, classifying may be performed asymmetrically in two value ranges
   with respect to the pixel values of an achromatic color.     In the example shown in part (b) of Fig. 37, the class width is
   the same as that of part (a) of Fig. 37, and partitioning positions of the classes are different from those of part (a) of Fig.
15 37.
   [0572]
             Alternatively, as shown in parts (c) and (d) of Fig. 37, classifying may be performed differently in accordance
   with the chrominance channel (Cr or Cb).       In the examples shown in parts (c) and (d) of Fig. 37, the class width is
   different between the chrominance component (Cr) and the chrominance component (Cb).               The class width can be
20 changed by setting more precise classifying levels, as in the transform table 2501.
   [0573]
             As shown in part (e) of Fig. 37, for the chrominance components, classifying may be performed by using only
   one BO type.
   [0574]
25 [Seventh Embodiment]
                                                                 136

              Another embodiment of the present invention will be described below with reference to Figs. 38 through 40.
   A seventh embodiment is different from the above-described embodiments in that, instead of using the value of an offset
   itself, the value of an offset is subjected to predictive coding, that is, an offset residual calculated by using the value of
   an offset and a prediction value of the offset value is coded and in that "0" may be provided as the prediction value of an
 5 offset.
   [0575]
               Generally, in SAO, an offset is coded in accordance with a class indicating a classified area.      If there is no
   pixel classified under a certain class (class is empty), "0" is coded.     Generally, offsets in adjacent classes or adjacent
   areas have similar values.     Accordingly, if an offset value which has already been decoded is used as a prediction value
10 of another offset and the difference from this prediction value is coded, the amount of data to code offsets can be reduced.
   However, if the class is empty, the amount of data to code offsets is increased.
   [0576]
               In this embodiment, therefore, "0" is included as prediction values of offsets, and one prediction value is
   selected from a plurality of candidates of prediction values including "0", and then, the prediction difference is coded,
15 thereby reducing the amount of data to code offsets.
   [0577]
               With this arrangement, regardless of whether a class is empty or there is a pixel classified under this class and
   an offset is coded, a suitable prediction value can be assigned, thereby making it possible to reduce the amount of data to
   code offsets by performing predictive coding.
20 [0578]
               The configuration of an offset information decoding section 611' of this embodiment will be described below
   with reference to Fig. 38.     The offset information decoding section 611' is provided instead of the offset information
   decoding section 611 shown in Fig. 22, and includes an offset residual decoder 651, an offset reconstructing section 652,
   and a prediction value determining section 653.        The prediction value determining section 653 includes a prediction
25 candidate flag decoder 661, a fixed prediction value calculator 662, a coded prediction value calculator 663, and a
                                                                 137

   prediction value candidate selector 664.
   [0579]
             The offset residual decoder 651 decodes an offset residual from QAOU information included in coded data #1
   and supplies the decoded offset residual to the offset reconstructing section 652.
 5 [0580]
             The prediction value determining section 653 determines a prediction value of an offset.      The prediction
   candidate flag decoder 661 decodes a prediction candidate flag from the QAOU information and supplies the decoded
   prediction candidate flag to the prediction value candidate selector 664.    The prediction candidate flag decoder 661 may
   be configured such that it does not decode a candidate section flag if a previously decoded offset value is "0".
10           The fixed prediction value calculator 662 calculates a fixed value (in this case, "0"), which is an offset to be
   coded when there is no pixel classified under a certain class, as a prediction value, and supplies the calculated fixed
   value to the prediction value candidate selector 664.
   [0581]
             The coded prediction value calculator 663 reads a decoded offset from the offset information storage section
15 621 and calculates a prediction value, and supplies it to the prediction value candidate selector 664.
   [0582]
             The prediction value candidate selector 664 selects, in accordance with the prediction candidate flag, a
   prediction value to be supplied to the offset reconstructing section 652 from among the prediction value supplied from
   the fixed prediction value calculator 662 and the prediction value supplied from the coded prediction value calculator
20 663.
   [0583]
             The offset reconstructing section 652 reconstructs an offset (Offset) from the prediction value (pred) supplied
   from the prediction value candidate selector 664 and from the offset residual (saooffset) supplied from the offset
   residual decoder 651 according to the following equation.
25           "Offset"   "pred" + '"sao offset"
                                                               138

              The reason why the amount of data required for coding is increased if there is a class (empty class) under
   which no pixel is classified will be discussed below with reference to Fig. 39.     Fig. 39 illustrates an overview of a case
   in which there are empty classes.     As shown in Fig. 39, in a class which is not empty, an offset used for this class is
   coded.    Then, for each class, by using the previous offset as a prediction value, the difference between the prediction
 5 value and the current offset is coded.    However, if there is an empty class, the prediction value becomes "0", and the
   difference value is increased.    As a result, the amount of data required for coding is increased.
   [0584]
              In this embodiment, therefore, by using a prediction candidate flag, the prediction value may be selected from
   "0" and "previous offset (other than 0)".
10 [0585]
              Syntax to be used when the prediction candidate flag is used will be described below with reference to Fig. 40.
   Fig. 40 illustrates syntax 2901 to be used when a prediction candidate flag is used.      As shown in Fig. 40, a prediction
   candidate flag "saopred flag" is added, and if "saopred flag"=0, the previously decoded offset (other than 0) is set as
   a prediction value offsetp.    If "saopred flag"=1 or the current offset is the first offset, the prediction value offsetp is
15 set to be 0.    In the syntax 2901, "saooffset delta" represents a difference d from the prediction value, and
   "saooffset[saocurr depth] [ys] [xs] "="offsetp+d".
   [0586]
              The prediction candidate flag may not be used depending on the offset type.        For example, in the case of EO,
   the prediction candidate flag is not used, and in the case of BO, the prediction candidate flag is used.     This is because,
20 in BO, empty classes may be frequently generated depending on the distribution of pixel values.
   [0587]
             Additionally, a determination may be made whether to perform predictive coding itself depending on the offset
   type.   For example, in the case of EO, predictive coding may be performed, and in the case of BO, predictive coding
   may not be performed.
25 [0588]
                                                                139

   (Appendix 9)
             In the above-described third through seventh embodiments, by using the intra-prediction mode, edge offset
   classes may be estimated, restricted, rearranged for each LCU.
   [0589]
 5           An edge direction selected as an EO type of SAO may have correlation with an intra-prediction mode.
   Accordingly, reference to the intra-prediction mode of a CU located at a position corresponding to a QAOU of SAO can
   be utilized for selecting an EO class.
   [0590]
             For example, by using the intra-prediction mode, an EO class may be estimated and determined, EO class
10 candidates may be restricted, or the order (index) of EO classes may be rearranged in order in which they are more likely
   to be selected.
   [0591]
             This is particularly easy to implement if an image is divided such that the size of a QAOU of each hierarchical
   level of SAO is the same as that of a CU, for example, such that the maximum size of a QAOU of SAO is equal to the
15 size of an LCU.
   [0592]
   (Appendix 10)
             In a known adaptive offset filter, since many modes of offsets (types and classes) are provided, a large memory
   size may be required.     Accordingly, in this embodiment, for example, an image filtering device which is capable of
20 reducing block distortion while suppressing an increase in the memory size is implemented.
   [0593]
             An image filtering device according to the present invention is an image filtering device for adding an offset
   selected from among a plurality of offsets to a pixel value of each pixel forming an input image which is constituted by a
   plurality of unit areas.  The image filtering device includes: offset determining means for determining, for each unit
25 area, an offset to be added to the pixel value of each pixel included in the unit area; and filtering means for adding an
                                                                140

   offset determined by the offset determining means to the pixel value of each pixel included in the unit area.      In a case in
   which the size of a unit area for which an offset is determined is smaller than a predetermined size, the offset
   determining means determines an offset to be added to the pixel value of each pixel included in the unit area from among
   a more restricted number of selectable offsets than that in a case in which the size of the unit area is equal to or greater
 5 than the predetermined size.
   [0594]
              With the above-described configuration, in a case in which the size of a unit area for which an offset is
   determined is smaller than the predetermined size, an offset to be added is determined from among a more restricted
   number of offsets than that in a case in which the size of the unit area is equal to or greater than the predetermined size.
10 [0595]
              When the size of a unit area is small, the number of pixels included in this unit area is also small, and it is more
   likely that pixels in this unit area have similar values.  Accordingly, when the size of a unit area is small, even if the
   number of selectable offsets is restricted, the influence on an image to which an offset is applied is small.     Additionally,
   by restricting the number of selectable offsets, a required memory space can be reduced.
15 [0596]
              Thus, with the above-described configuration, it is possible to reduce a memory space to be used while the
   influence on an image to which an offset is applied is decreased.     Additionally, since the number of selectable offsets is
   decreased, the amount of data to code offsets can be reduced, thereby improving the coding efficiency.
   [0597]
20            In order to solve the above-described problem, an image filtering device according to the present invention is
   an image filtering device for adding an offset to a pixel value of each pixel forming an input image which is constituted
   by a plurality of unit areas.    The image filtering device includes: offset determining means for determining, for each
   unit area, an offset to be added to the pixel value of each pixel included in the unit area; and filtering means for adding
   an offset determined by the offset determining means to the pixel value of each pixel included in the unit area.       In a
25 case in which the size of a unit area for which an offset is determined is smaller than a predetermined size, the filtering
                                                                 141

   means adds an offset having a lower precision than that in a case in which the size of the unit area is equal to or greater
   than the predetermined size.
   [0598]
              With the above-described configuration, in a case in which the size of a unit area for which an offset is
 5 determined is smaller than the predetermined size, an offset having a lower precision than that in a case in which the size
   of the unit area is equal to or greater than the predetermined size is added.
   [0599]
              When the size of a unit area is small, the number of pixels included in this unit area is also small, and it is more
   likely that pixels in this unit area have similar values.  Accordingly, when the size of a unit area is small, even if the
10 precision of an offset is decreased, the influence on quantization errors is small, and thus, the influence on an image to
   which an offset is applied is also small.    Additionally, by decreasing the precision of an offset, a required memory space
   can be reduced.
   [0600]
              Thus, with the above-described configuration, it is possible to reduce a memory space to be used while the
15 influence on an image to which an offset is applied is decreased.      Additionally, since the precision of an offset is
   decreased, the amount of data to code offsets can be reduced, thereby improving the coding efficiency.
   [0601]
             An image filtering device according to the present invention is an image filtering device for performing
   adaptive offset (SAO: Sample Adaptive Offset) on a pixel value of each pixel forming an input image which is
20 constituted by a plurality of unit areas.   The image filtering device includes: offset determining means for determining,
   for each unit area, an offset type of offset to be added to the pixel value of each pixel included in the unit area; and
   filtering means for adding an offset of an offset type determined by the offset determining means to the pixel value of
   each pixel included in the unit area.     The offset determining means determines the offset type to be a band offset (BO)
   in a case in which the pixel value of a subject pixel to which an offset is added is positioned in a value range around a
25 maximum value or a minimum value, and determines the offset type to be an edge offset (EO) in a case in which the
                                                                 142

   pixel value of the subject pixel is positioned in a value range other than the value ranges around the maximum value and
   the minimum value.
   [0602]
              With the above-described configuration, when adaptive offsetting is performed, a band offset is applied to
 5 pixels having pixel values positioned around the maximum value or the minimum value, and an edge offset is applied to
   pixels having pixel values positioned in value ranges other than the value ranges of the maximum value and the
   minimum value.
   [0603]
              In a higher pixel value range or a lower pixel value range, it is more likely that pixel values influence errors
10 than edges do.     Accordingly, with the above-described configuration, the efficiency in correcting errors can be
   enhanced, thereby improving the coding efficiency.
   [0604]
              If one offset type includes both of a band offset and an edge offset, the number of types of adaptive offsets can
   be decreased, thereby decreasing a memory space to be used and also reducing the amount of processing.
15 [0605]
              In the above-described configuration, band offset processing is offset processing for adding one of a plurality
   of offsets to the pixel value of a subject pixel in accordance with the magnitude of the pixel value of the subject pixel
   (this also applies to the subsequent description).    Edge offset processing is offset processing for adding one of a
   plurality of offsets to the pixel value of a subject pixel in accordance with the difference between the pixel value of the
20 subject pixel and the pixel value of a pixel positioned around the subject pixel (this also applies to the subsequent
   description).
   [0606]
              In the image filtering device according to the present invention, the offset determining means may determine
   the offset type to be a band offset for pixels positioned in a value range from the minimum value to 1/4 of the maximum
25 value or a value range from 3/4 of the maximum value to the maximum value, and may determine the offset type to be
                                                                  143

   an edge offset for pixels positioned in value ranges other than these value ranges.
   [0607]
             With the above-described configuration, it is possible to clearly separate pixels to which a band offset is
   applied from pixels to which an edge offset is applied.
 5 [0608]
             An image filtering device according to the present invention is an image filtering device for performing
   adaptive offset (SAO: Sample Adaptive Offset) on an input image.         The image filtering device includes: classifying
   means for determining an edge in order to determine a class to be used in an edge offset (EO) by referring to pixels
   positioned only in the horizontal direction of a subject pixel; and filtering means for adding an offset associated with a
10 class determined by the classifying means in a case in which an edge offset (EO) is applied.
   [0609]
             With the above-described configuration, when determining an edge, reference is made to pixels positioned only
   in the horizontal direction, thereby making it possible to reduce a required memory space, compared with a case in
   which reference is also made to pixels in the upward direction.     Additionally, boundary determining processing for
15 determining boundaries in the upward direction is not necessary, thereby making it possible to reduce the amount of
   processing.
   [0610]
             In the image filtering device according to the present invention, the classifying means may determine an edge
   by referring to a pixel positioned two pixels away from the subject pixel in the horizontal direction.
20 [0611]
             With the above-described configuration, since reference is made to a pixel positioned two pixels away from the
   subject pixel, it is even possible to detect an edge having a small angle.
   [0612]
             An image filtering device according to the present invention is an image filtering device for performing
25 adaptive offset (SAO: Sample Adaptive Offset) on an input image.         The image filtering device includes: classifying
                                                                144

   means for determining a class to be used in a band offset (BO) by setting the division width of classes to be used in a
   range of pixel values indicating the chrominance around a center value, which is a value positioned at the center between
   the maximum value and the minimum value, to be smaller than that in the other ranges of the pixel values; and filtering
   means for adding an offset associated with a class determined by the classifying means in a case in which a band offset
 5 (BO) is applied.
   [0613]
             With the above-described configuration, the division width of classes to be used in a range of pixel values
   indicating the chrominance around the center value, which is a value positioned at the center between the maximum
   value and the minimum value, is set to be smaller than that in the other ranges of the pixel values, and then, the classes
10 to be used when a band offset (BO) is applied are determined.
   [0614]
             When the pixel value indicating the chrominance is the center value, this pixel is an achromatic color. Errors
   of an achromatic color are noticeable to the human eye, thereby decreasing the subjective image quality.       Then, as in
   the above-described configuration, if the class width in a range around the center value is set to be small, offsets can be
15 set precisely for pixels around the center value. As a result, the subjective image quality can be increased.
   [0615]
             In the image filtering device according to the present invention, if the center value is positioned between the
   pixel value to which an offset has not been added and the pixel value to which an offset has been added, the filtering
   means may set the pixel value to which an offset has been added to be the center value.
20 [0616]
             With the above-described configuration, an offset is not added if the pixel value to which an offset has been
   added is beyond the center value.     When the pixel value indicating the chrominance is the center value, this pixel is an
   achromatic color, and in value ranges with the center value therebetween, the color perceived by the human eye is
   changed from the achromatic color. Then, with the above-described configuration, it is possible to prevent the color
25 perceived by the human eye from changing, which would otherwise be caused by adding an offset.
                                                               145

   [0617]
             An image filtering device according to the present invention is an image filtering device for performing
   adaptive offset (SAO: Sample Adaptive Offset) on an input image.         The image filtering device includes: filtering means
   for adding an offset having a higher precision to a pixel having a pixel value indicating the chrominance around a center
 5 value, which is a value positioned at the center between the maximum value and the minimum value, than the precision
   of an offset to be added to a pixel in the other value ranges.
   [0618]
              With the above-described configuration, an offset having a higher precision is added to a pixel having a pixel
   value indicating the chrominance around a center value, which is a value positioned at the center between the maximum
10 value and the minimum value, than the precision of an offset to be added to a pixel in the other value ranges.
   [0619]
              When the pixel value of a pixel indicating the chrominance is the center value, this pixel is an achromatic color.
   Errors of an achromatic color are noticeable to the human eye, thereby decreasing the subjective image quality.        Then,
   as in the above-described configuration, if the precision of an offset to be added to pixels around a center value is
15 improved, offsets can be precisely added to pixels around the center value.      As a result, the subjective image quality can
   be increased.
   [0620]
             An offset decoding device according to the present invention is an offset decoding device for decoding each
   offset which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.
20 The offset decoding device includes: offset residual decoding means for decoding each offset residual from coded data;
   prediction value determining means for determining a prediction value of each offset from a decoded offset or a
   predetermined value; and offset calculating means for calculating each offset from a prediction value determined by the
   prediction value determining means and an offset residual decoded by the offset residual decoding means.
   [0621]
25            With the above-described configuration, since an offset is decoded from a residual, the amount of data to code
                                                                146

   an offset can be decreased, compared with a case in which each offset itself is coded.     Additionally, a prediction value
   for determining a residual is determined from a decoded offset or a predetermined value.       Thus, it is possible to prevent
   the amount of data to code difference data from becoming greater than that when an offset itself is coded, which would
   otherwise be caused by the use of decoded offsets only.
 5 [0622]
               The predetermined value may be, for example, "0".
   [0623]
              An offset coding device according to the present invention is an offset coding device for coding each offset
   which is referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image.      The
10 offset coding device includes: prediction value determining means for determining a prediction value of each offset from
   a coded offset or a predetermined value; offset residual calculating means for calculating an offset residual from each
   offset and a prediction value determined by the prediction value determining means; and offset residual coding means
   for coding an offset residual calculated by the offset residual calculating means.
   [0624]
15             With the above-described configuration, since an offset is decoded from a residual, the amount of data to code
   an offset can be decreased, compared with a case in which each offset itself is coded.     Additionally, a prediction value
   for determining a residual is determined from a decoded offset or a predetermined value.       Thus, it is possible to prevent
   the amount of data to code difference data from becoming greater than that when an offset itself is coded, which would
   otherwise be caused by the use of decoded offsets only.
20 [0625]
              A data structure of coded data according to the present invention is a data structure of coded data which is
   referred to by an image filter for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas.   The data structure includes: prediction value determining information
   indicating whether a prediction value will be determined from a decoded offset or a predetermined value.         The image
25 filter refers to the prediction value determining information included in the coded data, and then determines a prediction
                                                                147

   value and decodes an offset.
   [0626]
             With the above-described configuration, it is possible to determine, by using the prediction value determining
   information, whether a prediction value will be set to be a decoded offset or a predetermined value.
 5 [0627]
   (Application Examples)
             The above-described video decoding device 1 (1') and video coding device 2 (2') may be mounted on and used
   for various apparatuses for transmitting, receiving, recording, and playing back video images.       Video images may be
   non-artificial video images captured by, for example, a camera, or artificial video images (including CG and GUI)
10 generated by, for example, a computer.
   [0628]
             The use of the above-described video decoding device 1 and video coding device 2 for transmitting and
   receiving video images will be described below with reference to Fig. 45.
   [0629]
15           Part (a) of Fig. 45 is a block diagram illustrating the configuration of a transmitting apparatus A on which the
   video coding device 2 is mounted.       As shown in part (a) of Fig. 45, the transmitting apparatus A includes a coder Al
   which codes video images so as to obtain coded data, a modulator A2 which modulates a carrier wave by using the
   coded data obtained by the coder Al so as to obtain a modulation signal, and a transmitter A3 which transmits the
   modulation signal obtained by the modulator A2.        The above-described video coding device 2 is used as this coder Al.
20 [0630]
             As supply sources for inputting video images into the coder Al, the transmitting apparatus A may also include
   a camera A4 which captures video images, a recording medium A5 on which video images are recorded, an input
   terminal A6 for receiving video images from an external source, and an image processor A7 which generates or
   processes images.     Although the configuration of the transmitting apparatus A including all these elements is shown in
25 part (a) of Fig. 45, some elements may be omitted.
                                                                 148

   [06311
              The recording medium A5 may be a recording medium on which video images which are not coded or video
   images which are coded by a recording coding method, which is different from a transmitting coding method, are
   recorded.    If video images which are coded by a recording coding method are recorded on the recording medium A5, a
 5 decoder (not shown) for decoding coded data read from the recording medium A5 by using a recording coding method
   may be interposed between the recording medium A5 and the coder Al.
   [0632]
              Part (b) of Fig. 45 is a block diagram illustrating the configuration of a receiving apparatus B on which the
   video decoder 1 is mounted.       As shown in part (b) of Fig. 45, the receiving apparatus B includes a receiver BI which
10 receives a modulation signal, a demodulator B2 which demodulates the modulation signal received by the receiver BI so
   as to obtain coded data, and a decoder B3 which decodes the coded data obtained by the demodulator B2 so as to obtain
   video images.     The above-described video decoding device 1 is used as this decoder B3.
   [0633]
             As destinations to which video images output from the decoder B3 are supplied, the receiving apparatus B may
15 also include a display B4 which displays video images, a recording medium B5 for recording video images thereon, and
   an output terminal B6 for outputting video images to an external source.       Although the configuration of the receiving
   apparatus B including all these elements is shown in part (b) of Fig. 45, some elements may be omitted.
   [0634]
              The recording medium B5 may be a recording medium for recording thereon video images which are not coded
20 or video images which are coded by a recording coding method, which is different from a transmitting coding method.
   If the recording medium B5 is a recording medium for recording video images which are coded by a recording coding
   method, a coder (not shown) for coding video images obtained from the decoder B3 by using a recording coding method
   may be interposed between the decoder B3 and the recording medium B5.
   [0635]
25           A transmission medium for transmitting a modulation signal may be a wireless medium or a wired medium.
                                                                  149

   A transmission mode in which a modulation signal is transmitted may be broadcasting (in this case, a transmission mode
   in which a transmission destination is not specified in advance) or communication (in this case, a transmission
   destination is specified in advance).     That is, transmission of a modulation signal may be implemented by any one of
   radio broadcasting, cable broadcasting, radio communication, and wired communication.
 5 [0636]
             For example, a broadcasting station (such as broadcasting equipment) and a receiving station (such as a
   television receiver) of terrestrial digital broadcasting are respectively an example of the transmitting apparatus A which
   transmits a modulation signal via radio broadcasting and an example of the receiving apparatus B which receives a
   modulation signal via radio broadcasting.       A broadcasting station (such as broadcasting equipment) and a receiving
10 station (such as a television receiver) of cable television broadcasting are respectively an example of the transmitting
   apparatus A which transmits a modulation signal via cable broadcasting and an example of the receiving apparatus B
   which receives a modulation signal via cable broadcasting.
   [0637]
             A server (such as a workstation) and a client (such as a television receiver, a personal computer, or a
15 smartphone) using VOD (Video On Demand) services or video hosting services on the Internet are respectively an
   example of the transmitting apparatus A which transmits a modulation signal via communication and the receiving
   apparatus B which receives a modulation signal via communication (generally, in the case of a LAN, a wired or wireless
   transmission medium is used, and in the case of a WAN, a wired transmission medium is used).             Examples of the
   personal computer are a desk-top PC, a laptop PC, and a tablet PC.           An example of the smartphone is a multifunction
20 mobile phone terminal.
   [0638]
             A client using video hosting services has a function of coding video images captured by a camera and
   uploading the coded video images to a server, as well as a function of decoding coded data downloaded from a server
   and displaying the decoded data on a display.        That is, a client using video hosting services serves as both of the
25 transmitting apparatus A and the receiving apparatus B.
                                                                    150

   [0639]
             The use of the above-described video decoding device 1 and video coding device 2 for recording and playing
   back video images will be described below with reference to Fig. 46.
   [0640]
 5           Part (a) of Fig. 46 is a block diagram illustrating the configuration of a recording apparatus C on which the
   video decoding device 1 is mounted.       As shown in part (a) of Fig. 46, the recording apparatus C includes a coder C1
   which codes video images so as to obtain coded data, and a writer C2 which writes the coded data obtained by the coder
   C1 into a recording medium M.        The above-described video coding device 2 is used as this coder C1.
   [0641]
10           The recording medium M may be (1) a type integrated in the recording apparatus C, such as an HDD (Hard
   Disk Drive) or an SSD (Solid State Drive), (2) a type connected to the recording apparatus C, such as a SD memory card
   or a USB (Universal Serial Bus) flash memory, or (3) a type loaded in a drive (not shown) integrated in the recording
   apparatus C, such as a DVD (Digital Versatile Disc) or a BD (Blu-ray Disc: registered trademark).
   [0642]
15          As supply sources for inputting video images into the coder C1, the recording apparatus C may also include a
   camera C3 which captures video images, an input terminal C4 for receiving video images from an external source, a
   receiver C5 for receiving video images, and an image processor C6 which generates or processes images.           Although the
   configuration of the recording apparatus C including all these elements is shown in part (a) of Fig. 46, some elements
   may be omitted.
20 [0643]
             The receiver C5 may be a receiver which receives video images which are not coded or video images which are
   coded by a transmitting coding method, which is different from a recording coding method.         If the receiver C5 is a
   receiver which receives video images coded by a transmitting coding method, a transmitting decoder (not shown) for
   decoding coded data coded by a transmitting coding method may be interposed between the receiver C5 and the coder
25 ci.
                                                                 151

   [0644]
              Examples of the recording apparatus C are a DVD recorder, a BD recorder, and an HD (Hard Disk) recorder (in
   this case, the input terminal C4 or the receiver C5 is a main supply source for video images).      Other examples of the
   recording apparatus C are a camcorder (in this case, the camera C3 is a main supply source for video images), a personal
 5 computer (in this case, the receiver C5 or the image processor C6 is a main supply source for video images), and a
   smartphone (in this case, the camera C3 or the receiver C5 is a main supply source for video images).
   [0645]
              Part (b) of Fig. 46 is a block diagram illustrating the configuration of a playback apparatus D on which the
   video decoding device 1 is mounted.        As shown in part (b) of Fig. 46, the playback apparatus D includes a reader D1
10 which reads coded data written into a recording medium M, and a decoder D2 which decodes the coded data read by the
   reader D1 so as to obtain video images.       The above-described video decoding device 1 is used as this decoder D2.
   [0646]
             The recording medium M may be (1) a type integrated in the playback apparatus D, such as an HDD or an SSD,
   (2) a type connected to the playback apparatus D, such as a SD memory card or a USB flash memory, or (3) a type
15 loaded in a drive (not shown) integrated in the playback apparatus D, such as a DVD or a BD.
   [0647]
             As destinations to which video images output from the decoder D2 are supplied, the playback apparatus D may
   also include a display D3 which displays video images, an output terminal D4 for outputting video images to an external
   source, and a transmitter D5 which transmits video images.         Although the configuration of the playback apparatus D
20 including all these elements is shown in part (b) of Fig. 46, some elements may be omitted.
   [0648]
              The transmitter D5 may be a transmitter which transmits video images which are not coded or video images
   which are coded by a transmitting coding method, which is different from a recording coding method.          If the transmitter
   D5 is a transmitter which transmits video images coded by a transmitting coding method, a coder (not shown) for coding
25 video images by a transmitting coding method may be interposed between the decoder D2 and the transmitter D5.
                                                                  152

   [0649]
              Examples of the playback apparatus D are a DVD player, a BD player, and an HDD player (in this case, the
   output terminal D4 to which a television receiver, for example, is connected is a main destination to which video images
   are supplied).     Other examples of the playback apparatus D are a television receiver (in this case, the display D3 is a
 5 main destination to which video images are supplied), a desk-top PC (in this case, the output terminal D4 or the
   transmitter D5 is a main destination to which video images are supplied), a laptop or tablet PC (in this case, the display
   D3 or the transmitter D5 is a main destination to which video images are supplied), a smartphone (in this case, the
   display D3 or the transmitter D5 is a main destination to which video images are supplied), and digital signage (also
   called an electronic bulletin board system, and in this case, the display D3 or the transmitter D5 is a main destination to
10 which video images are supplied).
   [0650]
   (Configuration Implemented by Software)
              The individual blocks of the video decoding device 1(1') and the video coding device 2(2'), in particular, the
   variable-length code decoder 13, the motion-vector reconstructing unit 14, the inter-prediction image generator 16, the
15 intra-prediction image generator 17, the prediction-method determining unit 18, the inverse-quantize-and-inverse
   transform unit 19, the deblocking filter 41, the adaptive filter 50, the adaptive offset filter 60 (60'), the transform-and
   quantize unit 21, the variable-length code coder 22, the inverse-quantize-and-inverse-transform unit 23, the intra
   prediction image generator 25, the inter-prediction image generator 26, the motion-vector detector 27, the prediction
   method controller 28, the motion-vector redundancy eliminating unit 29, the deblocking filter 33, the adaptive filter 70,
20 and the adaptive offset filter 80 (80') may be implemented by hardware by using a logical circuit formed on an integrated
   circuit (IC chip), or may be implemented by software by using a CPU (central processing unit).
   [0651]
              If the above-described elements are implemented by software, the video decoding device 1 and the video
   coding device 2 each include a CPU which executes commands of a control program which implements the individual
25 functions, a ROM (read only memory) storing this program therein, a RAM (random access memory) loading this
                                                                153

   program, a storage device (recording medium), such as a memory, storing this program and various items of data therein,
   and so on.    The object of the present invention may also be implemented by supplying a recording medium on which
   program code (an execution form program, an intermediate code program, and a source program) of the control program
   for the video decoding device 1 and the video coding device 2, which is software implementing the above-described
 5 functions, is recorded in a computer readable manner, to the video decoding device 1 and the video coding device 2, and
   by reading and executing the program code recorded on the recording medium by a computer (or a CPU or an MPU
   (micro processing unit)) of each of the video decoding device 1 and the video coding device 2.
   [0652]
             As the above-described recording medium, for example, a tape type, such as magnetic tape or cassette tape, a
10 disk type including a magnetic disk, such as a floppy (registered trademark) disk or a hard disk, and an optical disc, such
   as a CD-ROM (compact disc read-only memory), an MO disc (magneto-optical disc), an MD (Mini Disc), a DVD
   (digital versatile disc), or a CD-R (CD recordable), a card type, such as an IC card (including a memory card) or an
   optical card, a semiconductor memory type, such as a mask ROM, an EPROM (erasable programmable read-only
   memory), an EEPROM (electrically erasable and programmable read-only memory) (registered trademark), or a flash
15 ROM, or a logical circuit type, such as a PLD (Programmable logic device) or an FPGA (Field Programmable Gate
   Array), may be used.
   [0653]
              The video decoding device 1 and the video coding device 2 may be connected to a communication network and
   the above-described program code may be supplied via the communication network.           This communication network is
20 not particularly restricted as long as it is capable of transmitting the program code.  For example, the Internet, an
   intranet, an extranet, a LAN (local area network), ISDN (integrated services digital network), a VAN (value-added
   network), a CATV (community antenna television/cable television) communication network, a VPN (virtual private
   network), a public switched telephone network, a mobile communication network, a satellite communication work, etc.
   may be used.      Additionally, a transmission medium forming this communication network is not restricted to a specific
25 configuration or a specific type as long as it is capable of transmitting the program code.   For example, a wired
                                                                 154

   transmission medium, such as IEEE (institute of electrical and electronic engineers)1394, USB, power line
   communication, a cable TV line, a telephone line, or an ADSL (asymmetric digital subscriber loop) circuit, or a wireless
   transmission medium, such as infrared, for example, IrDA (infrared data association) or a remote controller, Bluetooth
   (registered trademark), IEEE802. 11 radio, HDR (high data rate), NFC (Near Field Communication), DLNA (Digital
 5 Living Network Alliance), a mobile phone network, a satellite circuit, or a terrestrial digital network, may be used.   In
   the present invention, the above-described program code may also be implemented in the form of a computer data signal
   embedded in a carrier wave through digital transmission.
   [0654]
             The present invention is not restricted to the above-described embodiments, and various modifications may be
10 made within the scope of the claims.      Embodiments obtained by combining technical means disclosed in the different
   embodiments in an appropriate manner are also encompassed in the technical scope of the present invention.
   Industrial Applicability
   [0655]
             The present invention is suitably applicable to an image filter which performs offset filtering on image data.
15 The invention is also suitably applicable to a decoding device which decodes coded data and a coding device which
   codes coded data.
   Reference Signs List
   [0656]
             1   video decoding device (decoding device)
20           41    deblocking filter
             50    adaptive filter
             60, 60'   adaptive offset filter (image filtering device)
             611, 611'   offset information decoding section (offset decoding device, determining means, offset attribute
   setting means, offset decoding means, offset residual decoding means, prediction value determining means)
25           612    QAOU structure decoding section
                                                                 155

             613    offset attribute setting section (offset attribute setting means)
             621    offset information storage section
             622    QAOU controller
             623    offset-type determining section
 5           624    classifying section (calculating means, bit shift means, classifying means)
             625    offset determining section (offset reverse shift means)
             626    offset adder (offset means)
             2   video coding device (coding device)
             33   deblocking filter
10           70   adaptive filter
             80, 80'    adaptive offset filter (image filtering device)
             811    offset calculator
             812    offset clipping section
             813    offset information selector
15           814    offset residual determining section
             815    offset attribute setting section (offset attribute setting means)
             816    offset shifter (offset shift means)
             Further embodiments of the present invention are provided in the following. It should be noted that the
   numbering used in the following section does not necessarily need to comply with the numbering used in the previous
20 sections.
   [First embodiment]
             An image filtering device for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas, comprising:
25           offset attribute setting means for setting an offset value range by referring to coded data;
                                                                   156

              offset decoding means for decoding an offset which is restricted to the set offset value range; and
              filtering means for adding the offset to the pixel value of each pixel forming the input image.
   [Second embodiment]
              The image filtering device according to thefirst embodiment, wherein:
 5            the coded data includes information concerning a bit depth of the pixel values of the pixels forming the input
   image; and
              the offset attribute setting means sets the offset value range in accordance with the bit depth of the pixel values.
   [Third embodiment]
              The image filtering device according to the second embodiment, wherein the offset attribute setting means sets
10 an offset bit depth, which is a bit width of the offset, to be equal to the bit depth of the pixel values in a case in which the
   bit depth of the pixel values is ten bits or smaller, and the offset attribute setting means sets the offset bit depth to be ten
   in a case in which the bit depth of the pixel values is eleven bits or greater, and the offset attribute setting means sets the
   offset value range to be a range of 2 (offset bit depth - 5) or smaller.
   [Fourth embodiment]
15            The image filtering device according to one of the first to third embodiments, wherein:
              the decoded offset is a quantized value; and
              the filtering means adds a value obtained by inverse-quantizing the offset by using a parameter included in the
   offset attributes to the pixel value of each pixel.
   [fifth embodiment]
20            The image filtering device according to the third embodiment, wherein the filtering means inverse-quantizes
   the decoded offset by performing bitwise left shift on the decoded offset by an amount equal to a shift value obtained by
   subtracting the offset bit depth from the bit depth of the pixel values.
   [Sixth embodiment]
              An offset decoding device for decoding each offset which is referred to by an image filter for adding an offset
25 to a pixel value of each pixel forming an input image, comprising:
                                                                        157

             offset residual decoding means for decoding each offset residual from coded data;
             prediction value determining means for determining a prediction value of each offset from a decoded offset;
   and
             offset calculating means for calculating each offset from a prediction value determined by the prediction value
 5 determining means and an offset residual decoded by the offset residual decoding means.
   [Seventh embodiment]
             The offset decoding device according to the sixth embodiment, wherein:
             the input image is constituted by a plurality of unit areas;
             the offset residual decoding means decodes each offset residual in association with an offset type, which is
10 determined for each unit area, and an offset class, which is determined for each pixel; and
             the prediction value determining means determines a prediction value of each offset from a decoded offset
   associated with the same offset type and the same offset class as an offset type and an offset class of the offset for which
   a prediction value will be determined.
   [Eight embodiment]
15           The offset decoding device according to the sixth embodiment, wherein:
             the input image is constituted by a plurality of unit areas;
             the offset residual decoding means decodes each offset residual in association with an offset type, which is
   determined for each unit area, and an offset class, which is determined for each pixel; and
             the prediction value determining means determines a prediction value of each offset from a decoded offset
20 associated with the same offset type as an offset type of the offset for which a prediction value will be determined in a
   case in which the offset type associated with the offset belongs to a first offset type group, and the prediction value
   determining means determines a prediction value of each offset from a decoded offset associated with the same offset
   type and the same offset class as an offset type and an offset class of the offset for which a prediction value will be
   determined in a case in which the offset type associated with the offset belongs to a second offset type group.
25 [Ninth embodiment]
                                                                158

             The offset decoding device according to the eight embodiment, wherein:
             the offset calculating means calculates each offset as a linear function of a prediction value determined by the
   prediction value determining means and an offset residual decoded by the offset residual decoding means; and
             a coefficient to be multiplied by the prediction value differs depending on whether the offset type associated
 5 with the offset belongs to the first offset type group or the second offset type group.
   [Tenth embodiment]
             The offset decoding device according to one of the sixth to ninth embodiments, wherein the prediction value
   determining means determines a prediction value of each offset by calculating a weighted average of a decoded offset
   and a prediction value of the decoded offset.
10 [Eleventh embodiment]
             The offset decoding device according to one of the sixth to tenth embodiments, wherein the prediction value
   determining means includes clipping means for clipping each of the determined prediction values by using an upper
   limit value and a lower limit value which correspond to a bit depth of the pixel value of each pixel forming the input
   image.
15 [Twelfth embodiment]
             An image filtering device which operates on an input image, comprising:
             calculating means for calculating a difference value between a pixel value of a subject pixel forming an input
   image and a pixel value of a pixel around the subject pixel;
             bit shift means for performing bitwise right shift on a pixel value referred to by the calculating means or the
20 difference value calculated by the calculating means by an amount equal to a predetermined shift value;
             classifying means for classifying the subject pixel as one of a plurality of offset classes in accordance with a
   magnitude relation between the difference value subjected to bitwise right shift by the bit shift means and 0; and
             offset means for adding an offset associated with the offset class of the subject pixel classified by the
   classifying means to the pixel value of the subject pixel.
25 [Thirteenth embodiment]
                                                                 159

              The image filtering device according to the twelfth embodiment, wherein the predetermined shift value has a
   positive correlation with a bit depth of the pixel value of the subject pixel.
   [Fourteenth embodiment]
             An image filtering device which operates on an input image, comprising:
 5            calculating means for calculating a difference value between a pixel value of a subject pixel forming an input
   image and a pixel value of a pixel around the subject pixel;
              classifying means for classifying the subject pixel as one of a plurality of offset classes in accordance with a
   magnitude relation between the difference value calculated by the calculating means and each of predetermined first and
   second thresholds; and
10            offset means for adding an offset associated with the offset class of the subject pixel classified by the
   classifying means to the pixel value of the subject pixel.
   [Fifteenth embodiment]
              The image filtering device according to the fourteenth embodiment, wherein absolute values of the first and
   second thresholds have a positive correlation with a bit depth of the pixel value of the subject pixel.
15 [Sixteenth embodiment]
             An image filtering device which operates on an input image constituted by a plurality of unit areas, comprising:
              determining means for determining, among first and second offset types, an offset type to which a subject unit
   area including a subject pixel forming the input image belongs;
              classifying means for classifying the subject pixel as one of an offset class in which an offset is not added and a
20 plurality of offset classes in which an offset is added in accordance with the offset type to which the subject unit area
   belongs and a pixel value of the subject pixel; and
              offset means for adding an offset associated with the offset type to which the subject unit area belongs and the
   offset class of the subject pixel classified by the classifying means to the pixel value of the subject pixel,
             wherein, in a case in which the pixel value of the subject pixel is within a predetermined range, the classifying
25 means classifies the subject pixel as an offset class in which an offset is added, regardless of whether the offset type to
                                                                  160

   which the unit area including the subject pixel belongs is the first offset type or the second offset type.
   [Seventeenth embodiment]
             An image filtering device for adding an offset to a pixel value of each pixel forming an input image which is
   constituted by a plurality of unit areas, comprising:
 5           determining means for determining an offset type to which a subject unit area belongs among a plurality of
   offset types;
             offset coding means for determining an offset having a bit width which differs depending on the offset type and
   for coding the offset; and
             filtering means for adding the determined offset to the pixel value of each pixel forming the input image.
10 [Eighteenth embodiment]
             An offset coding device for coding each offset which is referred to by an image filter for adding an offset to a
   pixel value of each pixel forming an input image, comprising:
             prediction value determining means for determining a prediction value of each offset from a coded offset;
             offset residual calculating means for calculating an offset residual from each offset and a prediction value
15 determined by the prediction value determining means; and
             offset residual coding means for coding an offset residual calculated by the offset residual calculating means.
   [Nineteenth embodiment]
             A data structure of coded data which is referred to by an image filter for adding an offset to a pixel value of
   each pixel forming an input image which is constituted by a plurality of unit areas, comprising:
20           an offset having a bit width which differs depending on an offset type to which each unit area belongs,
             wherein the image filter refers to the coded data, and determines an offset type to which a subject unit area
   belongs and also decodes an offset having a bit width which differs depending on the determined offset type.
   [0657]
             Where any or all of the terms "comprise", "comprises", "comprised" or "comprising" are used in this
25 specification (including the claims) they are to be interpreted as specifying the presence of the stated features, integers,
                                                                161

steps or components, but not precluding the presence of one or more other features, integers, steps or components.
                                                          162

   The claims defining the invention are as follows:
       1.  An image filtering device for receiving a deblocked decoded image and adding an offset
   to a pixel value of each pixel of a unit area of the deblocked decoded image which is constituted
 5 by a plurality of unit areas, comprising:
           an offset attribute setting unit configured to set an offset bit depth and an offset value
   range by referring to coded data and a shift value, wherein a maximum bit length representing
   the offset value range is equal to the offset bit depth-K, and the value of K is equal to 4;
           an offset decoding unit configured to decode an offset which is restricted to the offset
10 value range;
           a bit shift unit configured to perform bitwise left shift on the decoded offset by an
   amount equal to the shift value; and
           a filtering unit configured to add the left-shifted offset to the pixel value of each pixel of
   a unit area of the deblocked decoded image.
15
      2.   The image filtering device according to Claim 1, wherein:
           the offset attribute setting unit is configured to set the offset bit depth and the offset
   value range in accordance with a bit depth of the pixel values.
20    3.   The image filtering device according to Claim 2, wherein the offset attribute setting unit
   is configured to set the offset bit depth to be equal to the bit depth of the pixel values in a case
   that the bit depth of the pixel values is ten or smaller, and the offset attribute setting unit is
   configured to set the offset bit depth to be ten in a case that the bit depth of the pixel values is
   eleven or greater, wherein
                                                     163

              the offset attribute setting unit is configured to set a maximum offset value range to be
   2(the offset bit depth-K) -1-1 or smaller, wherein the value of K is equal to an integer.
       4.     The image filtering device according to any one of Claims 1 to 3, wherein: the shift
   value equals to PIC_DEPTH - SAODEPTH, wherein the PIC_DEPTH indicates the pixel bit
 5 depth of the image, and the SAODEPTH indicates the offset bit depth.
       5.       A filtering method for receiving a deblocked decoded image and adding an offset to a
   pixel value of each pixel of a unit area of the deblocked decoded image which is constituted by a
   plurality of unit areas, comprising:
10            setting an offset bit depth and an offset value range by referring to coded data and a shift
   value, wherein a maximum bit length representing the offset value range is equal to the offset bit
   depth-K, and the value of K is equal to 4;
              perform bitwise left shift on the decoded offset by an amount equal to the shift value
              adding the offset to the pixel value of each pixel of a unit area of the deblocked decoded
15 image.
       6. The image filtering method according to Claim 5, wherein:
              the offset bit depth and the offset value range are set in accordance with a bit depth of
   the pixel values.
20
       7.     The image filtering method according to Claim 5, wherein the offset bit depth is set to be
   equal to the bit depth of the pixel values in a case that the bit depth of the pixel values is ten or
   smaller, and the offset attribute setting unit is configured to set the offset bit depth to be ten in a
   case that the bit depth of the pixel values is eleven or greater, wherein
                                                         164

         a maximum offset value range to be           2 (the offset bit depth-K) -1-1 or smaller, wherein the
  value of K is equal to an integer.
     8.  The image filtering device according to any one of Claims 5 to 7, wherein: the shift value equals to
5 PIC_DEPTH - SAODEPTH, wherein the PIC_DEPTH indicates the pixel bit depth of the image, and the
  SAODEPTH indicates the offset bit depth.
                                                    165

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
