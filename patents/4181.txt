                                             Abstract
   There is disclosed a method and mobile device for displaying points of interest in a view
 5 of a real environment displayed on a screen of the mobile device with a functionality for
   interaction with a user, which comprises the steps of: capturing an image of the real envi
   ronment or a part of the real environment using a camera, determining at least one point of
   interest related to the real environment, determining an image position of the at least one
   point of interest in the image, displaying at least part of the image on at least part of the
10 screen, overlaying a computer-generated indicator with the at least part of the image on the
   screen at a screen position according to the image position of the at least one point of in
   terest, displaying a computer-generated virtual object related to the at least one point of
   interest on the screen at a screen position determined according to the screen position of
   the computer-generated indicator and which is adjacent to a bottom edge of the screen,
15 displaying a visually perceivable relation indication indicative of a relation between the
   computer-generated virtual object and the computer-generated indicator. The mobile de
   vice may perform an action related to the at least one point of interest if at least part of the
   computer-generated virtual object displayed on the screen is touched.
20 Fig. la

                    1/13
Fig. la
                          1a08
           la~l            1'O
        la09    -a>
                               1aa02
                                  1a06
 la 101
                         G          1a07

                                                   1
                  Method for representing points of interest in a view of a real
                   environment on a mobile device and mobile device therefor
 5 This application is a divisional of Australian patent application no. 2013401486, the entire
   disclosure of which is incorporated herein by reference.
   The present disclosure is related to a method for representing points of interest in a view of
   a real environment on a screen of a mobile device with a functionality for interaction with
10 a user. The present disclosure is further related to a mobile device and a computer program
   product comprising software code sections which are adapted to perform such method.
   It is known that Augmented Reality (AR) systems could enhance information of a real en
   vironment by providing a visualization of overlaying computer-generated virtual infor
15 mation with a view of the real environment or a part of the real environment. The virtual
   information can be any type of visually perceivable data such as objects, texts, drawings,
   videos, or their combination. The view of the real environment or a part of the real envi
   ronment could be perceived as visual impressions by user's eyes and/or be acquired as one
   or more images captured by a camera held by a user or attached on a device held by a user.
20
   A point of interest (commonly referred to as "POI") is known in the art to represent a loca
   tion or a real object (e.g., a building, a landmark, a moving car) in a real environment. A
   POI often has associated digital content that is related to the location or the real object.
   The digital content could be, for instance, audio information, video information, pictures,
25 textual information, 3D representations or their combinations.
   Representing points of interest (POIs) in a view of a real environment is particularly useful
   and popular in location-based (mobile) augmented reality applications, such as tour guid
   ance for exploring the urban environment, see references [1, 2]. For example, users may
30 use augmented reality to see visual information related to POIs overlaid with the view of
   the real environment when they are in unknown places or want to obtain information about
   things they see.
   Augmented reality systems face significant challenges to more usefully display POIs to
35 users. It is especially challenging to overlay the POIs with an image of a real environment
   on a screen of a mobile device, e.g. a hand-held device or a wear-able device, as the screen
   commonly has a very limited size and the POIs may occlude part of imagery information

                                                   2
   on the screen. Further, enabling the users to intuitively interact with the POIs displayed on
   the screen is another challenge, particularly if the device has limited input capabilities.
   A touchscreen, such as of a mobile device, could provide a direct interface between the
 5 users and the POIs displayed on the touchscreen. For example, the user could touch a POI
   on the screen in order to show more information related to the POI on the screen. Howev
   er, there exist easily and difficult accessible areas on a touchscreen of a hand-held device,
   e.g. mobile phone, or tablet, see references [1,2,7]. It is typically inconvenient for a user's
   finger to reach the difficult accessible areas, while the user holds the device. While the
10 user touches the POIs on the screen, the fingers might also occlude some imagery infor
   mation on the touchscreen. Thus, representing POIs on a touchscreen of a hand-held de
   vice is desirable to be optimized for comfortable user interaction and information visibil
   ity.
15 Yovcheva et al. in reference [5] show one AR application of representing a POI in an im
   age of a real world on a screen of a mobile device. A computer-generated icon is overlaid
   at the image position of the POI on the screen. The icon is connected by a line to a sidebar
   of the screen where an annotation shows additional information about the POI.
20 Grasset et al. in reference [6] investigated an optimal placement of virtual objects in an
   image of a real world and they propose a method to overlay a computer-generated annota
   tion of a POI in an image position, such that important imagery information in the image is
   not occluded.
25 Rose et al. in reference [11] present a system and method to annotate a real object or parts
   of the real object in an image captured by a camera. They first compute a camera pose of
   the image relative to the real object, e.g. based on a 3D model of the real object, and then
   overlay computer-generated texts with a computer-generated line drawn to the annotated
   part of the real object in the image.
30
   Uusitalo et al. in reference [8] disclose a method for displaying POI information based on
   partitioning of the real environment. The method determines to overlay one or more POIs
   based on the one or more partitions of the view of the real environment. They teach utiliz
   ing the knowledge of floor plan or height of a building to separate the building into parti
35 tions and overlaying the POI information to corresponding partitions in an image of the
   buildings.

                                                  3
   When POIs have the same view direction, the POIs may be arranged behind each other for
   displaying in augmented reality applications. In this regard, some of the POIs may not be
   visible, as they may be hidden behind other POIs. For this, Sandberg in reference [9] moti
   vates grouping POIs that have the same view direction and then displaying the grouped
 5 POIs in a visible manner.
   Meier et al. in reference [10] disclose a method to achieve an ergonomic representation of
   POIs in augmented reality systems by subdividing the view of the real environment into a
   plurality of regions based on the distances of the regions to the view point.
10
   For representing POIs in an image of a real environment displayed on a screen of a mobile
   device, none of these prior arts proposes any solution for the problem of an inconvenient
   user interaction with POIs displayed on a screen caused by difficult accessible areas and
   the problem of imagery information occluded by user's fingers during user interactions,
15 e.g. when touching a POI on the screen.
   It is an object of the invention to provide a method for representing points of interest in a
   view of a real environment displayed on a screen of a mobile device with a functionality
   for interaction with a user, which is capable to provide an improved way of comfortable
20 user interaction and information visibility.
   According to a first aspect, there is provided a method for representing points of interest in
   a view of a real environment displayed on a screen of a mobile device with a functionality
   for interaction with a user, which comprises the steps of: capturing an image of the real
25 environment or a part of the real environment using a camera, determining at least one
   point of interest related to the real environment, determining an image position of the at
   least one point of interest in the image, displaying at least part of the image on at least part
   of the screen, overlaying a computer-generated indicator with the at least part of the image
   on the screen at a screen position according to the image position of the at least one point
30 of interest, displaying a computer-generated virtual object related to the at least one point
   of interest on the screen at a screen position determined according to the screen position of
   the computer-generated indicator and which is adjacent to a bottom edge of the screen,
   and displaying a visually perceivable relation indication indicative of a relation between
   the computer-generated virtual object and the computer-generated indicator. Preferably,
35 the mobile device may perform an action related to the at least one point of interest if at
   least part of the computer-generated virtual object displayed on the screen is touched.

                                                    4
   According to the first aspect, there is also provided a mobile device comprising a camera
    adapted for capturing an image of a real environment or a part of a real environment, a
    screen adapted for displaying at least part of the image on at least part of the screen, and a
   processing device configured to display points of interest in the image of the real environ
 5  ment when displayed on the screen. The processing device is further configured to deter
    mine at least one point of interest related to the real environment, to determine an image
   position of the at least one point of interest in the image, to overlay a computer-generated
    indicator with the at least part of the image on the screen at a screen position according to
   the image position of the at least one point of interest, to display a computer-generated vir
10 tual object related to the at least one point of interest on the screen at a screen position de
   termined according to the screen position of the computer-generated indicator and which is
    adjacent to a bottom edge of the screen, and to display a visually perceivable relation indi
    cation indicative of a relation between the computer-generated virtual object and the com
   puter-generated indicator. Preferably, the processing device may further be configured to
15 perform an action related to the at least one point of interest, such as performing a function
    or service of an application running on the mobile device related to the at least one point of
    interest, if at least part of the computer-generated virtual object displayed on the screen is
   touched, e.g. by a user's finger or device held by the user.
20 According to an embodiment, the screen may be a touchscreen or a screen that has no
   touchable capability. According to an embodiment, the screen is a touchscreen and the
    screen is touched by a user's finger or a device held by the user.
   According to another embodiment, the at least part of the computer-generated virtual ob
25 ject displayed on the screen is determined to be touched by detecting an image position of
    a user s finger or a device held by the user in the image.
    Particularly, the camera of the mobile device can be used to detect a user's finger or a de
    vice held by the user. Accordingly, an image position of the user's finger or device held by
30 the user in the image of the real environment or the part of the real environment can be
    detected based on the image captured by the camera. A position of the computer-generated
    virtual object displayed on the screen relative to the image can be determined according to
    a screen position of at least part of the image displayed on the screen.
35  The mobile device could be triggered to perform an action related to the at least one point
    of interest, such as performing a function or service of an application running on the mo
   bile device related to the at least one point of interest, according to the image position of

                                                  5
   the user's finger or device held by the user and the position of the computer-generated vir
   tual object relative to the image. For example, if at least part of the computer-generated
   virtual object displayed on the screen overlaps with the user's finger, the mobile device is
   triggered to perform the action. The at least part of the computer-generated virtual object
 5 overlapping with the user's finger could be equivalent to that the at least part of the com
   puter-generated virtual object displayed on the screen is touched by the user's finger.
   This embodiment is particularly useful when using a head-mounted display comprising the
   camera and the screen. For example, the head-mounted display is a video-see-through
10 head-mounted display (HMD). It is typically not possible for the user to touch the head
   mounted screen in a manner like a touchscreen. However, the camera that captures an im
   age of the real environment may also be used to detect image positions of the user's finger
   in the image. The image positions of the user's finger could be equivalent to touching
   points touched by the user's finger on the touchscreen.
15
   According to a second aspect, there is provided a method for representing points of interest
   in a view of a real environment on a semi-transparent screen of a mobile device with a
   functionality for interaction with a user, comprising the steps of: determining at least one
   point of interest related to the real environment, determining a position of the at least one
20 point of interest relative to the view, blending in a computer-generated indicator in at least
   part of the view on the semi-transparent screen at a screen position according to the posi
   tion of the at least one point of interest relative to the view, blending in a computer
   generated virtual object related to the at least one point of interest on the semi-transparent
   screen at a screen position determined according to the screen position of the computer
25 generated indicator and which is adjacent to a bottom edge of the semi-transparent screen,
   and blending in a visually perceivable relation indication indicative of a relation between
   the computer-generated virtual object and the computer-generated indicator. Preferably,
   the mobile device may perform an action related to the at least one point of interest if at
   least part of the computer-generated virtual object blended in on the semi-transparent
30 screen is overlapped by a user's finger or device held by the user.
   According to the second aspect, there is also provided a mobile device comprising a semi
   transparent screen adapted for providing a view of a real environment and for representing
   points of interest in the view, and a processing device configured to display points of in
35 terest in the view of the real environment provided on the semi-transparent screen. The
   processing device is further configured to determine at least one point of interest related to
   the real environment, to determine a position of the at least one point of interest relative to

                                                     6
   the view, to blend in a computer-generated indicator in at least part of the view on the
   semi-transparent screen at a screen position according to the position of the at least one
   point of interest relative to the view, to blend in a computer-generated virtual object relat
   ed to the at least one point of interest on the semi-transparent screen at a screen position
 5 determined according to the screen position of the computer-generated indicator and which
   is adjacent to a bottom edge of the semi-transparent screen, and to blend in a visually per
   ceivable relation indication indicative of a relation between the computer-generated virtual
   object and the computer-generated indicator. Preferably, the processing device may further
   be configured to perform an action related to the at least one point of interest, such as per
10 forming a function or service of an application running on the mobile device related to the
   at least one point of interest, if at least part of the computer-generated virtual object blend
   ed in on the semi-transparent screen is overlapped by a user's finger or device held by the
   user.
15 For example, the semi-transparent screen is part of a head-mounted display, particularly
   part of an optical-see-through head-mounted display (HMD).
   Aspects and embodiments of the invention described in the following with respect to the
   first aspect related to points of interest represented in a view of a real environment dis
20 played on a screen, such as a touchscreen, of a mobile device can equivalently also be ap
   plied in combination with the second aspect with points of interest represented in a view of
   a real environment on a semi-transparent screen of a mobile device. Therefore, all the em
   bodiments and aspects described herein can be combined and applied with the features of,
   both, the first aspect and the second aspect described above.
25
   According to a further aspect, there is provided a computer program product comprising
   software code sections which are adapted to perform a method according to the invention
   as described herein when loaded into the internal memory of the mobile device. Particular
   ly, the computer program product is non-transitory.
30
   According to the invention, there is provided an improved way of comfortable user interac
   tion and information visibility, wherein improved interaction and visualization design gen
   erally facilitates comparison, exploration and problem solving. The present invention con
   siderably improves user experience in retrieving, visualizing and interacting with infor
35 mation about real objects (e.g. landmarks or buildings) that surround a user in a particular
   real environment. The present invention contributes to develop a solution with design
   principles feedback, visibility and affordance in mind:

                                                  7
   The mobile device may be a hand-held device, such as a mobile phone or tablet computer.
   The mobile device may also be a wearable device, such as a watch or glass.
 5 The camera may be any capturing device providing an image. It is not restricted to cameras
   providing color images in the RGB format. It can also be applied to any other color format
   and also to monochrome images, for example, to cameras providing images in grayscale
   format. The camera may further provide an image with depth data. The depth data does not
   need to be provided in the same resolution as the (color/grayscale) image. A camera
10 providing an image with depth data is often called RGB-D camera. A RGB-D camera sys
   tem may be a time of flight (TOF) camera system. Kolb et al. in reference [4] give an
   overview on state of the art of time-of-flight camera sensors and applications.
   The screen of the mobile device may have a planar polygon shape (such as a screen as
15 shown in the following Figures) and may be a touchscreen. The screen may be a LCD or
   LED screen.
   It is not necessary to have a known spatial relationship between the optical axis and a nor
   mal direction of the screen. However, the spatial relationship between the optical axis and
20 the normal direction of the screen may be provided or determined.. For example, the opti
   cal axis of the camera has a fixed same or opposite direction as the normal direction of the
   screen. The camera may be rotatable or translatable with respect the screen. A spatial rela
   tionship between the camera and the screen could be measured by an encoded motor that
   connects the camera and/or the screen. It is also possible to attach positioning and/or orien
25 tation sensors to each of the camera and the screen for measuring their spatial relationship.
   The at least one point of interest (POI) represents a location or a real object (e.g. a build
   ing, a landmark, a moving car) in a real environment and may include digital content that
   is related to the location or the real object. Any digital content related to the at least one
30 point of interest may be provided, such as a name, description, image, video, or web ad
   dress and their combinations. The real environment could be any real scene in real world,
   such as a nature scene, an indoor environment scene, or a city scene. A real environment
   includes one or more real objects and/or locations.
35 The at least one point of interest may have a known location in the real environment. For
   example, a building or a landmark commonly has a known location in a global coordinate
   system, such as a coordinate system having geo-location coordinates (e.g. the geo-location

                                                    8
   coordinates may comprise a 2D coordinate of longitude and latitude, or a 3D coordinate of
   longitude, latitude and altitude) and/or an address (e.g. floor number, street, postcode,
   country). The address and the global location may be converted to each other. Further, the
   at least one point of interest may have a known orientation in the real environment.
 5
   A pose as described herein describes an object's position and orientation relative to a coor
   dinate system or an object.
   A camera pose relative to the real environment may be determined using one or more loca
10 tion or orientation sensors. For example, a global positioning system (GPS) may be em
   ployed to determine the global location of the capture device (camera), e.g. a geo
   coordinate such as a 2D coordinate of longitude and latitude, or a 3D coordinate of longi
   tude, latitude and altitude. Various orientation sensors, such as compass and/or gravity
   sensors, can measure the orientation with respect to the global coordinate system. An in
15 door positioning system based on known locations of anchors, with systems making use of
   various optical, radio, and/or acoustic technologies, can also be used to determine the cam
   era pose.
   The camera pose of the image (i.e., the camera pose when capturing the image) relative to
20 the at least one point of interest may be computed based on the position of the at least one
   POI and the camera relative to the real environment, e.g. a global coordinate system.
   The camera pose of the image relative to the at least one point of interest can also be com
   puted by using a computer vision method. For example, if a real object associated with the
25 at least one point of interest is visible in the image and has a known geometrical size, then
   the camera pose relative to the at least one point of interest could be determined according
   to correspondences between 3D points on the real object and 2D image points of the real
   object in the image.
30 The image position of the at least one POI in the image may be computed based on the
   camera pose of the image relative to the at least one POI. For example, the image position
   is a projection of the at least one POI or a part of the at least one POI to the image plane of
   the camera.
35 The image position of the at least one POI may also be determined by analyzing image fea
   tures in the image, e.g. according to image pixel values or template matching. For example,
   when at least part of a real object is visible in the image, an image area of the at least part

                                                   9
   of the real object may be determined by matching an image template of the real object with
   the image or a part of the image. Then, any one or multiple image points within the image
   area of the at least part of the real object may be chosen as the image position of the at
   least one point of interest.
 5
   An image captured by the camera may be completely displayed and occupy the whole
   screen. It is also possible to display only a part of the image. The displayed image or part
   of the image may also occupy a part of the screen instead of the whole screen.
10 The computer-generated indicator (e.g., a virtual balloon or a virtual circle as shown in the
   Figures) may be any computer generated visual information, such as an icon, a point, a
   letter or their combinations. The computer-generated indicator may be overlaid with the at
   least part of the image at or near to (within a predetermined distance) the image position of
   the at least one point of interest on the screen.
15
   The computer-generated virtual object (e.g., rectangle boxes as shown in the Figures) may
   be any computer generated visual object, such as an icon, text, a figure, a video or their
   combinations. The computer-generated virtual object is related to the at least one point of
   interest. For example, the computer-generated virtual object may be an annotation, a name
20 and/or a description of the at least one point of interest.
   According to an embodiment, the computer-generated virtual object is displayed at a
   screen position which is lower than the displayed computer-generated indicator, particular
   ly along a vertical direction on the screen.
25
   The vertical direction may be a direction orthogonal to the bottom edge of the screen. The
   vertical direction may also be defined by projecting a gravity direction to the screen plane.
   According to an embodiment, the visually perceivable relation indication is a computer
30 generated line segment connecting the computer-generated virtual object and the comput
   er-generated indicator. For example, the line segment runs along a direction which is or
   thogonal to the bottom edge of the screen. According to another embodiment, the line
   segment may run along a direction which is defined by projecting a gravity direction to the
   screen plane.
35
   According to an embodiment, when multiple computer-generated virtual objects related to
   respective ones of multiple points of interest are displayed on the screen, the computer-

                                                    10
    generated virtual objects are each placed at the bottom of the screen or are placed in a
    stacked manner one above the other.
   According to an embodiment, the at least one point of interest is determined according to a
 5  location of the at least one point of interest in the real environment and a location of the
    camera in the real environment when capturing the image.
   According to a further embodiment, the image position of the at least one point of interest
    in the image is determined based on an orientation of the camera relative to the at least one
10 point of interest when capturing the image.
   According to another embodiment, the at least one point of interest and/or the image posi
   tion of the at least one point of interest is determined from analyzing the at least part of the
    image.
15
   According to an embodiment, the method may further comprise the steps of determining a
    gravity angle as an angle between gravity direction and a normal direction of the screen,
    and if the determined gravity angle exceeds a predetermined threshold, displaying virtual
    visual information related to the at least one point of interest on the touchscreen, and any
20  image captured by the camera is not displayed.
    The examples disclosed above describe how to represent points of interest in a view of a
   real environment using a video-see-through device. For example, the video-see-through
    device comprises the screen and the camera. The real environment or the part of the real
25  environment is captured as the image by the camera. The computer-generated virtual ob
   ject, the computer-generated indicator and the at least part of the image are shown on the
    screen to the user.
   As described above, the present invention could also be applied to representing points of
30  interest in a view of a real environment using an optical-see-through device. For example,
   the optical-see-through device has a semi-transparent screen, such as a semi-transparent
    spectacle or glasses. A human eye is a capture device (equivalent to the camera) for cap
   turing a view of the real environment or the part of the real environment. The view cap
   tured by the eye is equivalent to the image captured by the camera. The user then sees
35 through the semi-transparent screen the real environment and the computer-generated vir
   tual object and the computer-generated indicator blended in in the view on the screen.

                                                11
   Aspects and embodiments of the invention will now be described with respect to the draw
   ings, in which:
   Fig. l a   shows an embodiment of the present invention when viewing an outdoor scene,
 5
   Fig. lb    shows another embodiment of the present invention when viewing an outdoor
              scene,
   Fig. 2     shows an embodiment of the present invention when viewing an internal real
10            environment of a vehicle,
   Fig. 3     shows a visualization for an exemplary determination of a bottom edge of a
              display device, such as a touchscreen, having the shape of a rectangle,
15 Fig. 4     shows an embodiment of the present invention when a screen of a mobile de
              vice is placed such that the normal of the screen is parallel to gravity direction,
   Fig. 5     shows a visualization for an exemplary determination of a heading direction of
              a device,
20
   Fig. 6     shows a visualization for an exemplary determination of a heading angle of a
              device,
   Fig. 7a    shows a depiction (left) according to which a mobile device, such as a mobile
25            phone, is held at its vertical position, and a depiction (right) of an exemplary
              view displayed on the touchscreen of the mobile device,
   Fig. 7b    shows a depiction (left) according to which a mobile device, such as a mobile
              phone, is held more naturally and tilted from its vertical position, and a depic
30            tion (right) of an exemplary view displayed on the touchscreen of the mobile
              device,
   Fig. 8     shows a flowchart of an embodiment of the invention presenting one or more
              POIs on a touchscreen of a mobile device, such as a mobile phone, equipped
35            with a camera,

                                                   12
   Fig. 9      shows an embodiment of the invention for illustrating an exemplary triggering
               of an action performed by the mobile device when touching a displayed com
               puter-generated virtual object, such as an annotation,
 5 Fig. 10a    shows an example of state of the art for representing POIs in the view of a real
               environment on a semi-transparent screen of a head-mounted display,
   Fig. 1Ob    shows an embodiment of the present invention for representing POIs in the view
               of a real environment on a semi-transparent screen of a head-mounted display.
10
   In the following description, it is referred to the depictions of Figures 1 to 1Ob as it appears
   appropriate for describing aspects and embodiments of the present invention. All examples
   and embodiments described herein can equivalently be applied with an apparatus as shown
   in Figures 1-9 or an apparatus as shown in Fig. 10b. Accordingly, when referring to an im
15 age displayed to the user in the embodiments of Figs. 1-9, this is to be replaced by view
   (which is captured by the user through the semi-transparent screen) when applied to an
   embodiment as shown in Fig. lOb.
   The invention is most suitable to be used with a mobile device, which may be, for exam
20 ple, a hand-held device, such as a smartphone or a tablet computer.
   According a first embodiment, the mobile device, such as mobile device laO1 of Fig. la,
   comprises, for the purposes of the invention, a camera which is adapted for capturing an
   image of a real environment or a part of a real environment. The camera is not explicitly
25 shown in the Figures, but may be provided on the backside of the mobile device, such as
   known in the art for mobile phones, and indicated with reference number la08 in Fig. la
   and Fig. 7a designating a camera. The mobile device 1aO 1 further comprises a touchscreen
    la02 which is adapted for displaying at least part of the image on at least part of the
   touchscreen. The mobile device 1aO 1 also comprises a processing device, such as a micro
30 processor and associated circuitry which are commonly used in the art and not shown in
   the Figures, since they are internal to the mobile device. The internal processing device is
   indicated with reference number 1a09 in Fig. 1a.
   Among other tasks as commonly used and applied in the art, with regard to the present in
35 vention the processing device 1a09 is configured to display images and points of interest in
   an image of a real environment when such view of the real environment is displayed on the
   touchscreen la02. The processing device la09 is further configured to perform tasks and

                                                  13
   steps as described herein in connection with the invention, such as the steps as described
   with reference to Fig. 8.
   Fig. la and Fig. lb show an embodiment of presenting one or more POIs on a screen of a
 5 mobile device, such as a mobile phone, equipped with a camera. The screen is a
   touchscreen and has a rectangular shape. The optical axis of the camera has a direction
   opposite and parallel or roughly parallel to a normal direction of the screen. As commonly
   known in the art, the normal direction of the screen is directed upwards when the screen is
   facing upwards. The present mobile phone is also equipped with a GPS sensor, a gravity
10 sensor and a compass sensor.
   Fig. 8 shows a flowchart of an embodiment of a method for displaying points of interest in
   a view of a real environment displayed on a touchscreen of a mobile device with a func
   tionality for interaction with a user.
15
   In a first step 8001, an image of a real environment is captured by the camera of the mobile
   device and at least part of the image is displayed on the touchscreen. A next step 8002
   comprises obtaining the location of the mobile device in the real environment, e.g. from
   the GPS sensor. Step 8003 determines the heading direction of the mobile device in the
20 real environment, e.g. according to data from the compass sensor (as described in more
   detail below). Step 8004 provides POIs with their locations in the real environment. The
   POIs may be stored on a remote server or the mobile device. Step 8005 selects relevant
   POIs from the provided POIs. Relevant POIs are POIs which may be relevant to be dis
   played in the view of the real environment. Step 8006 selects augmented POIs from the
25 relevant POIs. Augmented POIs are POIs which are to be augmented with additional in
   formation, such as graphical object, name, etc. Step 8007 determines image positions of
   the relevant POIs in the image.
   In step 8008 screen positions of computer-generated indicators (Fig. 1a: 1a04; here: circle)
30 for relevant POIs on the touchscreen are determined according to the image positions of
   the relevant POIs. The computer-generated indicators are intended to indicate an associat
   ed POI in the area within or next to the indicator and may have, for this purpose, any suit
   able shape of a graphical element. For example, the POIs may be in (or behind) the center
   of the respective displayed circle 1a04 shown in the view of the real environment.
35
   Step 8009 displays a computer-generated virtual object (Fig. la: laO5; here: rectangle) for
   at least one augmented POI at a screen position adjacent to the bottom of the touchscreen.

                                                  14
    The computer-generated virtual object 1a05 is intended to provide a user interaction func
   tionality as described in more detail below. As further described in more detail below, the
    screen position of the computer-generated virtual object la05 is determined according to
   the screen position of a corresponding computer-generated indicator la04. In step 8010,
 5 the computer-generated indicators 1a04 are overlaid to the at least part of the image on the
   touchscreen. Step 8011 displays a visually perceivable relation indication (Fig. la: la06;
    here: a line segment) connecting the computer-generated virtual object 1a05 and the corre
    sponding computer-generated indicator 1a04 on the touchscreen. In this way, it is indicated
   to the user that with pressing or touching the computer-generated virtual object la05 an
10  action may be performed by the mobile device related to the corresponding point of inter
    est. A user may touch at least part of the computer-generated virtual object la05 on the
   touchscreen to trigger an action of the mobile device (step 8012), such as described in an
    example with respect to Fig. 9.
15  The computer-generated virtual object 1a05 displayed on the touchscreen is preferred to be
    displayed on a screen position adjacent to a bottom edge of the screen (see screen border
    1a07 which is a bottom edge of screen 1a02 in Fig. la). A screen position of the computer
    generated virtual object la05 is determined according to the screen position of the com
   puter-generated indicator la04. For example, if the screen position of the computer
20  generated indicator la04 moves to the left according to a movement of the mobile device
    and the displayed view, the computer-generated virtual object la05 is also shifted to the
    left accordingly. This may be done in a way that, e.g., the computer-generated virtual ob
   ject 1a05 is displayed vertically below the computer-generated indicator 1a04, as described
    herein, also in a case where the computer-generated indicator 1a04 moves.
25
    To determine a bottom edge or bottom of the screen, lower edges of the screen can be de
   termined at first. An edge of the screen is determined to be a lower edge of the screen, if at
    least part of the edge has the shortest distance to the ground plane among the edges of the
    screen. In such case, a lower edge of the screen is determined to be a bottom edge or bot
30 tom of the screen, if the angle of the lower edge with respect to the gravity direction is the
    smallest angle among the angles of all the lower edges with respect to the gravity direc
   tion.
   According to Fig. 3, an exemplary screen 3001 has a rectangular shape and has four edges
35  3002, 3003, 3004, and 3005. Reference number 3010 shows a gravity direction G with re
    spect to screen 3001 in Fig.3. Both edges 3004 and 3005 have a part (here: corner 3020)
   that has the shortest distance to the ground plane among the edges of screen 3001. The an-

                                                  15
   gle of edge 3005 with respect to gravity direction 3010 is angle a and the angle of edge
   3004 with respect to gravity direction 3010 is angle 6. In this example, angle a is smaller
   than angle 6, and thus edge 3005 is determined to be the bottom edge of screen 3001.
 5 If there are more than one edge candidates which satisfy the criteria of the bottom edge, an
   edge candidate that is the bottom edge in the last previous screen position may be deter
   mined to be the bottom edge. If none of the edge candidates is the bottom edge in the last
   previous screen position, any edge candidate may be chosen as the bottom edge. If the
   screen plane is orthogonal to the gravity direction, the bottom edge of the screen is indefi
10 nite and there is no bottom edge.
   According to an embodiment, the computer-generated virtual object (such as la05) is dis
   played at a screen position such that a line segment (such as 1a06) passing the computer
   generated indicator (such as 1a04) along the vertical direction intersects with the comput
15 er-generated virtual object on the screen.
   The computer-generated virtual object and the computer-generated indicator are related by
   a visually perceivable relation indication on the screen. For example, a (e.g., dash or solid)
   line, such as line segments la06, 1b06 in Fig. la, b and 2006 in Fig. 2, may be drawn to
20 pass the respective computer-generated virtual object and the computer-generated indicator
   on the screen.
   It is possible to trigger an action performed by the mobile device by touching the comput
   er-generated virtual object (such as la05, 1b05, 2005) on the screen. This may be realized,
25 for example, by a finger touching the touchscreen in the respective region of display. The
   triggered action may display a web page, a video, and/or any detailed information about
   the related at least one point of interest on the screen. The action may also generate some
   sound, e.g. music.
30 The normal direction of the touchscreen is defined as a direction perpendicular to the
   screen plane and toward the front of the screen. The gravity direction is defined as a direc
   tion along gravity and toward earth. A gravity angle of the screen is an angle between the
   gravity direction and the normal direction of the screen. When the screen is vertically held,
   the gravity angle is about 90 degrees for example. When the screen is horizontally held,
35 the gravity angle is about 180 degrees for example.

                                                   16
    The gravity angle may be determined by a gravity sensor associated with the mobile de
    vice. The gravity angle may also be determined by using the camera to capture a real ob
   ject having a known orientation with respect to gravity.
 5  If the gravity angle exceeds a pre-determined threshold, a virtual visual information related
   to the at least one point of interest may be displayed on the screen, and any image captured
   by the camera is not displayed. In this regard, Fig. 4 shows an embodiment where comput
    er-generated virtual objects (e.g. 4001) and computer-generated indicators (e.g. 4002),
    such as direction indicators, related to the computer-generated virtual objects are displayed
10  on screen 4010. The virtual visual information includes the computer-generated virtual
    object 4001 or similar and may further include an information indicating a direction from
   the mobile device to the at least one point of interest, as shown in Fig. 4 by the respective
    direction indicator. The direction may be computed based on a compass device equipped
   to the mobile device or based on an image captured by the camera.
15
   According to an embodiment, POIs are stored on a server that is remote and separate from
   the mobile device. The mobile device may communicate with the server through a wireless
   network. The server further stores information associated with the POIs, such as locations,
   names, descriptions, images and supplemental information. The locations of the POIs may
20  have 2D coordinates with latitude and longitude, which determine their respective posi
   tions on the surface of the earth, i.e. positions relative to a real environment. It is reasona
   ble to use latitude and longitude, as most real objects represented by the POIs are standing
    on the surface of the earth. However, it is also possible to use 3D coordinates with longi
   tude, latitude and altitude for the present invention. This is particularly useful for repre
25  senting POIs located above the ground, e.g. on a mountain.
    However, it should be noticed that all the information regarding the POIs mentioned above
    may also be stored on the mobile device. In this case, it is not necessary for the mobile de
    vice to communicate with the server and obtain the POIs and related information from the
30  server.
   An image of the real environment or a part of the real environment is captured by the cam
    era. For example, a geo-location of the mobile device is obtained from an associated GPS
    sensor. The geo-location may be used as the camera position of the image (i.e. the position
35  of the camera when capturing the image) in the real environment. It is reasonable to use
    latitude and longitude coordinates, when the mobile device is positioned by a user standing
    on the surface of the earth.

                                                 17
   A heading direction of the mobile device defines a direction within the ground plane and
   may be read from the compass sensor. For example, with reference to Fig. 5, the heading
   direction 5003 of the mobile device 5001 may be determined by projecting the opposite
 5 direction of the normal direction of the screen 5002 onto the ground plane 5004 along a
   plane defined by the normal direction and Y axis of the screen. A heading angle of the
   mobile device may be defined to be an angle between the heading direction and a direction
   from the mobile device to a frame of reference, such as the north. For example, according
   to Fig. 6 the heading angle  P 6004 of the mobile device 6001 is an angle between heading
10 direction 6002 and direction 6003 to north.
   According to an embodiment, POIs relevant to the image displayed on the touchscreen of
   the mobile device are selected from the POIs, such as stored on the server (see step 8005
   of Fig. 8). For this, the mobile device may send its geo-location to the server. The sever
15 computes distances of the POIs or a part of the POIs to the mobile device. A direction of a
   POI is determined by a direction pointing from the geo-location of the mobile device to the
   location of the respective POI. A direction angle of a POI is an angle of a direction of the
   POI with respect to a frame of reference, such as the north.
20 For example, POts are selected as relevant POts from a plurality of POts based on their
   distances (in the real world) to the mobile device and/or angle differences between the
   heading angle of the mobile device and their direction angles. For example, only POts
   whose distances to the mobile device are below a certain threshold are selected. Balloons
   or circles 1b08 and 1b04 indicate selected relevant POIs (see Fig. lb). According to an
25 embodiment, the relevant POIs will be represented by computer-generated indicators on
   the screen.
   For selecting relevant POIs, it is also possible to send the POIs or part of the POIs and the
   associated information from the server to the mobile device, and perform any selection
30 operations on the mobile device.
   In the further process, image positions of the relevant POts in the image are calculated (see
   step 8007 of Fig. 8). In one embodiment, the image positions could be obtained from inter
   sections between the image plane of the camera and a line from the POts to the camera.
35 Then, according to an embodiment, computer-generated indicators, e.g. balloons or circles
    1b04 and 1b08, are overlaid with the image at the image positions of the relevant POts on
   the screen (see Fig. lb). The computer-generated indicators could be 2D or 3D models.

                                                  18
   In order to avoid overloaded visual information on the screen, it is preferred to display ad
   ditional information for a limited number of the relevant POIs if there are a large number
   of the relevant POIs.
 5
   Further, one or more POIs are determined as augmented POIs among the relevant POIs
   (see step 8006) and computer-generated virtual objects respectively related to the aug
   mented POIs are displayed on the screen (see step 8009). For example, augmented POIs
   may be selected if they have the smallest angles between their directions to the mobile de
10 vice and the heading direction of the mobile device among the relevant POIs. The angle
   could be computed based on the heading angle of the mobile device and direction angles of
   the relevant POIs. In Fig. lb, balloons or circles 1b04 indicate such augmented POIs.
   Further, the computer-generated virtual objects for the augmented POIs are generated and
15 displayed. An augmented POI may have one or more computer-generated virtual objects.
   In one example shown in Fig. lb, the computer-generated virtual objects are annotations
    1b05 that include names of the augmented POIs 1b04 and distances between the mobile
   device and the augmented POIs in real world. The computer-generated virtual objects
   could also contain imagery information about POI.
20
   In the present embodiment, each annotation is represented with a rectangular box. For a
   screen size between about 10 and 15 cm (diagonal), a maximum number of augmented
   POIs is preferred to be three with motivation to reduce augmented reality (AR) scene clut
   ter and focus user attention to the AR experience itself. Therefore, with the computer
25 generated virtual objects additional information for POIs is provided, e.g. via annotations,
   in non-obstructive fashion.
   The annotations, i.e. generally the computer-generated virtual objects, should preferably be
   placed at positions on the touchscreen such that the annotations will not introduce any oc
30 clusion on imagery information on the screen. Modem touchscreens allow users to touch
   the annotation on the screen in order to trigger one or more actions related to the respec
   tive POI. Thus, the placement of the annotations should further enable them to be easily
   reached by user's fingers when a user holds the mobile device.
35 The prior art as referred to herein suggests placing annotations at image positions of the
   POIs on the screen. The image positions of the POIs depend on the respective camera pose
   relative to the POIs. This means that the image positions of the POIs depend on the orien-

                                                  19
   tation of the mobile device held by a user. In most cases, the POIs and the mobile device
   are standing on the surface of the earth. Thus, when the mobile device is held vertically,
   i.e. the normal 7a02 of the screen of the mobile device 7a01 is perpendicular to the gravity
   direction 7010, the image positions of the POIs are typically at a horizontal line roughly in
 5 the middle of the image (see right depiction of Fig. 7a). In this example, the balloons 7a05
   (having annotations 7a06) are displayed around a horizontal line roughly in the middle of
   the screen of the mobile device 7a01.
   On the other hand, according to an embodiment of the invention, when the mobile device
10 is held more naturally and tilted from its vertical position, i.e. at an obtuse angle between
   the normal 7b02 of the screen of the mobile device 7b0 1 and the gravity direction 7010,
   the image positions of the POIs are at a horizontal line above the middle of the image (see
   right depiction of Fig. 7b). In this example, the balloons (generally the indicators) 7b05 are
   displayed in an upper area of the screen of the mobile device 7b01, while the annotations
15 7b06 (generally the virtual objects) are still shown at the bottom of the screen. From this
   example, the improvement in comfortable user interaction and information visibility
   achieved by the invention is clearly evident, since the annotations 7b06 to trigger an inter
   action with the mobile device can still be touched comfortably by the thumb of the user's
   hand.
20
   Users typically prefer to hold mobile devices tilted rather than vertically in most cases.
   Holding the device tilted introduces problems for a user to touch the annotations of the
   POIs displayed in the middle or upper areas on the screen. For example, when a hand holds
   the mobile device and the thumb of the hand touches the POIs, the thumb may occlude im
25 agery information displayed on the screen. Further, as disclosed in references [1,2,7], for
   mobile hand-held devices, such as phones and tablets, upper areas of their screens are un
   comfortable to be reached by the thumb.
   An improvement according to the present invention, as described above, results from plac
30 ing an indicator at or close to the image position of a POI to indicate the POI on the dis
   play, and placing a related computer-generated virtual object for POI related user interac
   tion, such as an annotation, below the POI, e.g. at the bottom of the screen, apart from the
   image position of the PO. This optimizes visibility and accessibility of the POI and its
   associated functions. It is particularly beneficial for cases of users using one hand to hold
35 the mobile device and the thumb of the same hand to touch the computer-generated virtual
   object related to the POI on the screen for any user interaction. Usually, the bottom area of
   the screen is a comfortably or easily accessible area, as disclosed in references [1,2,7], and

                                                 20
   further the thumb will not occlude major imagery information when it touches the bottom
   area.
   As shown in the embodiments of the Figures, the annotations are displayed lower than the
 5 respective displayed computer-generated indicator for the POIs on the screen. Particularly,
   the annotations are preferred to be displayed adjacent to the bottom of the screen. It might
   be that the annotations displayed on the screen may not directly contact the bottom of the
   screen, but have a small gap from the bottom of the screen. The area of the small gap may
   be defined according to the display area of the screen. For example, the height of the small
10 gap (i.e. a distance between the bottom of the screen and a lower border of an annotation)
   is defined as being smaller than about 1/10 of the height between the bottom of the screen
   and the top of the screen.
   Furthermore, when there are more than one annotation to be displayed, the annotations
15 may each be placed on the bottom of the screen, or placed in a stack mode, i.e. one annota
   tion being placed on the top of another annotation (see Figs. 7a and 7b). The screen posi
   tions of the annotations are further determined according to the screen positions of the in
   dicators of the corresponding POIs.
20 According to an embodiment, an annotation is displayed at a screen position such that a
   line passing the corresponding indicator along a direction orthogonal to the bottom edge of
   the screen intersects with the annotation on the screen.
   In order to enable the user to visually perceive relationships between the displayed annota
25 tions and the augmented POIs, a line segment may be drawn on the screen to connect an
   annotation and an indicator related to the same augmented POI.
   The user may touch an annotation or a part of an annotation displayed on the screen in or
   der to trigger an action, e.g. displaying a web page, a video, or a complete description
30 about the corresponding POI related to the touched annotation. Fig. 9 shows an embodi
   ment of triggering an action by touching an annotation. Reference number 9001 shows an
   AR view of annotations and indicators overlaid with an image of a real environment ac
   cording to an embodiment of the present invention. 9004 indicates the touch-point on the
   annotation 9005 of POI "Marienplatz", which is associated with indicator 9006, touched
35 by a thumb of a user. This touching triggers an action that displays a description 9007 of
   POI "Marienplatz" on the top of the screen as shown in view 9002. The user could further
   touch the screen at the touch-point 9008. Then, this touching may further trigger an action

                                                    21
   to display a route to the physical location of POI "Marienplatz", shown here as dashed
   curve 9009 overlaid with a camera image in the view 9003. The dashed curve ends at the
   location of the POI.
 5 As the augmented POIs, according to an embodiment, are selected based on the heading
   direction of the mobile device, the user may rotate the mobile device to different heading
   directions in order to choose POIs to have their annotations displayed on the bottom of the
   screen. For example, referring to Fig. 1b, the mobile device may be rotated to the left, so
   that annotations for three indicators (left) lb08 are shown, and the presently displayed an
10 notations 1b05 for indicators 1b04 disappear. Therefore, no additional (touch) user input is
   required to select POIs around the user in a current context.
   If the screen, e.g. the mobile device, is only tilted without changing its heading direction,
   the image positions of the indicators will change accordingly, while the displayed annota
15 tions will stay at substantially the same positions on the screen (see Figs. 7a and 7b). This
   improves the usability of overlaying POI information with images of a real environment
   displayed on a screen of a mobile device held by a user. This accounts for the fact that a
   comfortable way of holding a mobile device by one hand or two hands is typically in most
   cases a tilted holding, in contrast to a vertical holding.
20
   When the mobile device is moved to another heading direction and capturing a second im
   age, the image positions of the indicators in the second image will change accordingly and
   the annotations will also move on the screen according to changes between the image posi
   tions of the indicators in the original (first) image and the second image.
25
   Furthermore, according to an embodiment, a radar view may be displayed on the screen.
   The relevant POIs or some of them and the mobile device may be displayed in the radar
   view according to their locations and orientations in the real environment.
30 The gravity angle of the screen, which is an angle between the gravity direction and the
   normal direction of the screen, may be determined by a gravity sensor associated with the
   mobile device. For example, if the gravity angle of the screen exceeds a pre-determined
   threshold, information related to relevant POIs or part of them is displayed on the screen,
   while any image captured by the camera is not displayed. The relevant POIs are deter
35 mined among the POIs stored on the server based on relative locations and directions be
   tween the POIs and the mobile device.

                                                  22
   The information of a POI could be a rectangular box (e.g. 4001) including imagery, or tex
   tual information or similar and may further include direction information from the mobile
   device to the POts. The direction could be computed based on a compass device which is
   part of the mobile device. When the screen is horizontally placed, a list of rectangular box
 5 es representing POIs is displayed on the screen as shown in Fig. 4, i.e. one box placed on
   top of another box. Each box (e.g. 4001) may have a direction indicator (e.g. 4002) that
   indicates the direction to the real location of the POI represented by the box.
   In the example above, real objects represented by POIs do not need to be visible in a cam
10 era image. Image positions of the POIs could be computed based on known locations of the
   POIs and the pose of the camera in a common coordinate system, e.g. a global coordinate
   system.
   For overlaying information about a path to a POI in the image, a user can choose an option
15 to show a virtual path that would be leading to a physical location of the selected POI. A
   virtual path may be drawn as a curved line overlaid on a camera-image in respective
   frames. A line starts at the current user location and ends at the location of the POI.
   In another example, image positions of POIs may be determined by analyzing image fea
20 tures in a camera image. This requires that real objects represented by the POIs are visible
   in the camera image. Any known image template matching methods may be employed for
   detecting real objects in the image with one or more image templates of the real objects.
   According to a further embodiment of the invention, Fig. 2 shows computer-generated vir
25 tual objects (here: annotations) overlaid with an image of a control panel of a car on a
   screen of the mobile device. The image is captured by a camera of the mobile device. Sev
   eral objects of the control panel may be detected in the image based on visual template
   matching by comparing at least part of the image and visual templates. For example, the
   visual templates are stored on a server that sends the templates to the mobile device via
30 wireless connection. The visual templates could also be stored on the mobile device. In the
   context of the invention, some of the objects on the control panel are points of interest.
   For example, a computer-generated indicator (here: circle) 2004 is placed at the respective
   image position to indicate a location of a POI in the image. The computer-generated virtual
35 object (here: annotation) 2005 showing the POI name is displayed below and connected
   with line 2006 to the circle. In this example, not each of the POIs is augmented by an an
   notation, so that, e.g., circles 2011 do not have associated and displayed annotations. De-

                                                    23
   termining augmented POIs among the POIs displayed in the image could be based on their
   image positions or manually. For example, POIs with image positions near to or at the
   middle of the image may be chosen as augmented POIs.
 5 The disclosed method could also be applied to representing points of interest in a view of a
   real environment using a mobile device such as an optical-see-through device. For exam
   ple, the optical-see-through device comprises a head-mounted display comprising a semi
   transparent screen. The human eye is a capture device (equivalent to the camera) for cap
   turing a view of the real environment or the part of the real environment. The view cap
10 tured by the eye is equivalent to the image captured by the camera, as described above.
   The user then sees through the semi-transparent screen the real environment and the com
   puter-generated virtual object and the computer-generated indicator blended in in the view
   on the semi-transparent screen.
15 The head-mounted display may be further equipped with a location sensor (such as GPS)
   and an orientation sensor (such as compass sensor). The location sensor could determine a
   position of a viewing point of the view relative to the real environment. The orientation
   sensor could determine a view orientation of the view relative to the real environment. A
   location of at least one point of interest relative to the real environment may be provided.
20 The position of the at least one point of interest relative to the view can be determined ac
   cording to the position of the viewing point, the view orientation and the location of the at
   least one point of interest.
   According to an embodiment, the head-mounted display may be equipped with a camera.
25 The position of the at least one point of interest relative to the view could also be deter
   mined according to one or more images of at least part of the real environment captured by
   the camera.
   A computer-generated indicator (e.g. a balloon) could be blended in in at least part of the
30 view on the semi-transparent screen at a screen position according to the position of the at
   least one point of interest relative to the view.
   A computer-generated virtual object (e.g. an annotation) related to the at least one point of
   interest is blended in on the semi-transparent screen at a screen position determined ac
35 cording to the screen position of the computer-generated indicator and which is adjacent to
   a bottom edge of the semi-transparent screen.

                                                 24
   For example, a line segment between the computer-generated virtual object and the com
   puter-generated indicator is blended in on the semi-transparent screen.
   The head-mounted display performs an action related to the at least one point of interest if
 5 at least part of the computer-generated virtual object blended in on the semi-transparent
   screen is overlapped by a user's finger or device held by the user. This requires to detect a
   position of the user's finger or device held by the user relative to the semi-transparent
   screen, i.e. determining a screen position of the user's finger or device held by the user on
   the semi-transparent screen.
10
   In one embodiment, a position sensor may be attached to the user's finger or device held
   by the user in order to determine the screen position of the user's finger or device held by
   the user. The position sensor could be such as, but not limited to, a GPS and an accelerom
   eter.
15
   In another embodiment, the camera of the head-mounted display can be used to detect the
   user's finger or device held by the user. An image position of the user's finger or device
   held by the user can be detected based on the image. The camera may have a known posi
   tion relative to the semi-transparent screen. Thus, a screen position of the user's finger or
20 device held by the user on the semi-transparent screen may be determined according to its
   image position in the image.
   Figs. 1Oa and 1Ob show a user 10a04 who sees through the semi-transparent screen 10a02
   of the head-mounted display 10a03 a view 1OaO1 of a real environment and balloons
25  1Oal 1, 10a12 or 10a13 and annotations 10a21, 10a22 or 10a23 related to POIs blended in
   in the view 1OaO1 on the semi-transparent screen 10a02.
   Fig. 1Oa shows an example of state of the art for representing three POIs in the view 1OaO 1
   of a real environment on the semi-transparent screen 10a02 of the head-mounted display
30  10a03. The head-mounted display 10a03 is worn by the user 10a04. In this embodiment,
   the three POIs have associated indicators (i.e. balloon) 10al 1, 10a12 or 10a13 and annota
   tions 10a21, 10a22 or 10a23. For each of the three POIs, the corresponding balloon and
   annotation are blended in at the same position in the view.
35 The user 10a04 has to move his hand 10a06 up such that the user's finger could overlap
   with the annotation 10a23 blended in in the view on semi-transparent screen 10a02 in
   order to interact with the corresponding POI, such as triggering a program related to the

                                                 25
   corresponding POI running on the head-mounted display 10a03. One problem is that the
   user's hand 10a06 occludes a certain part of the view. Further, it is uncomfortable for the
   user to move his hand too high.
 5 Fig. 10b shows an embodiment of the present invention for representing the three POts in
   the view 1OaO 1 of the real environment on the semi-transparent screen 10a02 of the head
   mounted display 10a03. Analogous to the other embodiment related to a touchscreen of a
   mobile device described above, the head-mounted display 10a03 also comprises a pro
   cessing device, such as a microprocessor and associated circuitry which are commonly
10 used in the art and not shown in the Figures, since they are internal to the device. The in
   ternal processing device is indicated with reference number 10a09 in Fig. lOb.
   Among other tasks as commonly used and applied in the art, with regard to the present in
   vention the processing device 10a09 is configured to display the points of interest on the
15 semi-transparent screen 10a02 of the head-mounted display 10a03, i.e. points of interest in
   the view of the real environment provided on the semi-transparent screen. The processing
   device 10a09 is further configured to perform tasks and steps as described herein in con
   nection with the invention, such as the steps as described or analogous with reference to
   Fig. 8.
20
   The POts have their corresponding annotations 10a2 1, 10a22 and 10a23 blended in on the
   semi-transparent screen and the annotation 10a23 is adjacent to the bottom edge 10a05 of
   the screen while corresponding indicators (balloons) 1Oa 11, 10a12 and 10a13 are blended
   in on the semi-transparent screen at the positions of the POts. The balloons and the anno
25 tations are connected by lines blended in on the semi-transparent screen. The user 10a04
   could move his hand 10a06 such that the user's finger could overlap with the annotation
    10a23 blended in in the view in order to interact with the corresponding POI without oc
   cluding the view 1OaO 1. This movement is different from the movement as shown in Fig.
    1Oa and more advantageous, since it is sufficient for the user to keep his hand lower in the
30 view for interacting with the corresponding POI, thus resulting in not occluding the view.
   Further, it is more comfortable for the user to move his hand more naturally, particularly
   not as high as in Fig. 1Oa.
35

                                               26
   References:
   1.   http://blog.utest.com/how-mobile-users-hold-devices/2013/03/
   2.   http://www.uxmatters.com/mt/archives/2013/02/how-do-users-really-hold-mobile
 5      devices.php
   3.   Du Preez, Victor, et al. "Human-computer interaction on touch screen tablets for
        highly interactive computational simulations." Proc. International Conference on
        Human-Computer Interaction. 2011.
   4.   Andreas Kolb, Erhardt Barth, Reinhard Koch, Rasmus Larsen: Time-of-Flight Sen
10      sors in Computer Graphics. Eurographics 2009.
   5.   Yovchevaa et al., Overview of Smartphone Augmented Reality Applications for
        Tourism
        http://ertr.tamu.edu/files/2012/11/eRTR SI V10i2 Yovcheva Buhalis Gatzidis 63
        66.pdf
15 6.   Grasset, Raphael, et al. "Image-driven view management for augmented reality
        browsers." Mixed and Augmented Reality (ISMAR), 2012 IEEE International Sym
        posium on. IEEE, 2012.
   7.   http://www.lukew.com/ff/entry.asp? 1649
   8.   US 2012/0113138 Al
20 9.   US 2012/0075341 Al
   10.  US 2012/0176410 Al
   11.  Rose, Eric, et al. "Annotating real-world objects using augmented reality." Computer
        graphics: developments in virtual environments, Academic Press Ltd., London, UK
        (1995).
25

                                                   27
                                                 Claims
    1. A method for representing points of interest in a view of a real environment displayed
 5 on a screen of a mobile device with a functionality for interaction with a user, comprising
   the steps of:
   - capturing an image of the real environment or a part of the real environment using a cam
   era,
   - determining at least one point of interest related to the real environment,
10 - determining an image position of the at least one point of interest in the image,
   - displaying at least part of the image on at least part of the screen,
   - overlaying a computer-generated indicator with the at least part of the image on the
   screen at a screen position according to the image position of the at least one point of in
   terest,
15 - displaying a computer-generated virtual object related to the at least one point of interest
   on the screen at a screen position determined according to the screen position of the com
   puter-generated indicator and which is adjacent to a bottom edge of the screen, and
   - displaying a visually perceivable relation indication indicative of a relation between the
   computer-generated virtual object and the computer-generated indicator.
20
   2. The method according to claim 1, wherein the mobile device performs an action related
   to the at least one point of interest if at least part of the computer-generated virtual object
   displayed on the screen is touched.
25 3. The method according to claim 1 or 2, wherein the screen is a touchscreen and the
   screen is touched by a user's finger or a device held by the user.
   4. The method according to one of claims 1 to 3, wherein the at least part of the computer
   generated virtual object displayed on the screen is determined to be touched by detecting
30 an image position of a user's finger or a device held by the user in the image.
   5. The method according to one of claims 1 to 4, wherein the visually perceivable relation
   indication is a computer-generated line segment connecting the computer-generated virtual
   object and the computer-generated indicator.
35
   6. The method according to claim 5, wherein the line segment runs along a direction which
   is orthogonal to the bottom edge of the screen.

                                                   28
   7. The method according to claim 5, wherein the line segment runs along a direction which
   is defined by projecting a gravity direction to a plane of the screen.
 5 8. The method according to one of claims 1 to 7, wherein when displaying multiple com
   puter-generated virtual objects related to respective ones of multiple points of interest on
   the screen, the computer-generated virtual objects are each placed at the bottom of the
   screen or are placed in a stacked manner one above the other.
10 9. The method according to one of claims 1 to 8, wherein the computer-generated indicator
   comprises at least one of an icon, a graphical object, a point, a letter or combinations
   thereof.
    10. The method according to one of claims 1 to 9, wherein the computer-generated indica
15 tor is overlaid with the at least part of the image at or within a predetermined distance to
   the image position of the at least one point of interest on the screen.
    11. The method according to one of claims 1 to 10, wherein the computer-generated virtual
   object comprises at least one of an icon, a graphical object, text, a figure, a video,
20 an annotation, a name and a description of the at least one point of interest or a combina
   tion thereof.
    12. The method according to one of claims 1 to 11, wherein the at least one point of inter
   est is determined according to a location of the at least one point of interest in the real en
25 vironment and a location of the camera in the real environment when capturing the image.
    13. The method according to one of claims 1 to 12, wherein the image position of the at
   least one point of interest in the image is determined based on an orientation of the camera
   relative to the at least one point of interest when capturing the image.
30
    14. The method according to one of claims 1 to 13, wherein at least one of the point of in
   terest and the image position of the at least one point of interest is determined from analyz
   ing the at least part of the image.
35  15. The method according to one of claims I to 14, further comprising the steps of:
   - determining a gravity angle as an angle between gravity direction and a normal direction
   of the screen,

                                                   29
   - if the determined gravity angle exceeds a predetermined threshold, displaying virtual
   visual information related to the at least one point of interest on the screen, and any image
   captured by the camera is not displayed.
 5  16. The method according to one of claims 1 to 15, wherein the mobile device is a hand
   held device, particularly a smartphone or a tablet computer.
    17. A method for representing points of interest in a view of a real environment on a semi
   transparent screen of a mobile device with a functionality for interaction with a user, com
10 prising the steps of:
   - determining at least one point of interest related to the real environment,
   - determining a position of the at least one point of interest relative to the view,
   - blending in a computer-generated indicator in at least part of the view on the semi
   transparent screen at a screen position according to the position of the at least one point of
15 interest relative to the view,
   - blending in a computer-generated virtual object related to the at least one point of interest
   on the semi-transparent screen at a screen position determined according to the screen po
   sition of the computer-generated indicator and which is adjacent to a bottom edge of the
   semi-transparent screen, and
20 - blending in a visually perceivable relation indication indicative of a relation between the
   computer-generated virtual object and the computer-generated indicator.
    18. The method according to claim 17, wherein the mobile device performs an action re
   lated to the at least one point of interest if at least part of the computer-generated virtual
25 object blended in on the semi-transparent screen is overlapped by a user's finger or device
   held by the user.
    19. A mobile device comprising:
   - a camera adapted for capturing an image of a real environment or a part of a real envi
30 ronment,
   - a screen adapted for displaying at least part of the image on at least part of the screen,
   - a processing device configured to display points of interest in the image of the real envi
   ronment when displayed on the screen,
   - wherein the processing device is further configured to:
35 - determine at least one point of interest related to the real environment,
   - determine an image position of the at least one point of interest in the image,

                                                     30
   - overlay a computer-generated indicator with the at least part of the image on the screen at
   a screen position according to the image position of the at least one point of interest,
   - display a computer-generated virtual object related to the at least one point of interest on
   the screen at a screen position determined according to the screen position of the comput
 5 er-generated indicator and which is adjacent to a bottom edge of the screen, and
   - to display a visually perceivable relation indication indicative of a relation between the
   computer-generated virtual object and the computer-generated indicator.
   20. A mobile device comprising
10 - a semi-transparent screen adapted for providing a view of a real environment and for rep
   resenting points of interest in the view,
   - a processing device configured to display points of interest in the view of the real envi
   ronment provided on the semi-transparent screen,
   - wherein the processing device is further configured to:
15 - determine   at least one point of interest related to the real environment,
   - determine   a position of the at least one point of interest relative to the view,
   - blend in    a computer-generated indicator in at least part of the view on the semi
   transparent  screen at a screen position according to the position of the at least one point of
   interest relative to the view,
20 - blend in a computer-generated virtual object related to the at least one point of interest on
   the semi-transparent screen at a screen position determined according to the screen posi
   tion of the computer-generated indicator and which is adjacent to a bottom edge of the
   semi-transparent screen, and
   - to blend in a visually perceivable relation indication indicative of a relation between the
25 computer-generated virtual object and the computer-generated indicator.
   21. A computer program product comprising software code sections which are adapted to
   perform a method according to any of the claims 1 to 18 when loaded into the internal
   memory of the mobile device.
30

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
