   [ABSTRACT]
            Provided are a method and an apparatus for coefficient scan on the base of a
  partition mode of a prediction unit. The method comprises the steps of: determining a scan
  method on the basis of a partition mode of a prediction unit; and encoding the information
5 about the scan method, wherein the scan method is determined, on the basis of RDO (Rate
  Distortion optimization), from among the extracted candidate scan methods which have
  been extracted with consideration of the shapes of the partitions of the partition mode.

     [DESCRIPTION]
     [Invention Title]
            METHOD AND          APPARATUS        FOR COEFFICIENT SCAN             BASED     ON
   PARTITION MODE OF PREDICTION UNIT
 5   [Cross Reference to Related Applications]
            [1] The present application is a divisional application of Australian Patent
   Application No. <removed-apn>, filed 8 July 2016, which is a divisional application of
   Australian Patent Application No. <removed-apn>, filed 3 February 2016 which is a
   divisional application of 2012336557 filed 8 November 2012 which is a national stage
10 entry of PCT/KR2012/009373 (Publication No. WO/2013/069975 Al), which claims the
   benefit of Korean application No. 10-2011-0116126 filed 8 November 2011 and Korean
   application No. 10-2012-0125799 filed 8 November 2012 and the content all of which
   applications are incorporated herein by reference in their entirety.
     [Technical Field]
15          [1A] The present invention relates to video encoding and decoding, and more
   particularly, to a method and an apparatus for determining a scan method based on a
   partition mode of a prediction unit and encoding/decoding information thereof.
     [Background Art]
            [2] Recently, demands for high-resolution and high-quality videos, such as high
20 definition (HD) and ultrahigh-definition (UHD) videos, are increasing in various fields of
   applications.   As video data has higher resolution and higher quality, the amount of data
   more increases relative to existing usual video data.     Accordingly, when video data is
   transferred using media such as existing wired and wireless broad band lines or is stored in
   existing storage media, transfer cost and storage cost increase.     In order to solve these
25 problems occurring with an increase in resolution and quality of video data, high
                                              -1I-

   efficiency video compression techniques may be utilized.
            [3] Video compression technology includes various techniques, such as an inter
   prediction technique of predicting pixel values included in a current picture from previous
   or subsequent pictures of the current picture, an intra prediction technique of predicting
 5 pixel values included in a current picture using pixel information in the current picture,
   and an entropy encoding technique of assigning a short code to a value with a high
   appearance frequency and assigning a long code to a value with a low appearance
   frequency.    Video data may be effectively compressed and transferred or stored using
   such video compression techniques.
10   [Disclosure]
     [Technical Problem]
             [4] An aspect of the present invention is to provide a method of determining a
   scan method based on a partition mode of a prediction unit for improving video
   encoding/decoding efficiency and encoding/decoding the scan method.
15           [5] Another aspect of the present invention is to provide an apparatus for
   determining a scan method based on a partition mode of a prediction unit for improving
   video encoding/decoding efficiency and encoding/decoding the scan method.
     [Technical Solution]
             [6] An embodiment of the present invention provides a video encoding method.
20 The method includes determining a scanning method based on a partition mode of a
   prediction unit, and encoding information on the scanning method, wherein the scanning
   method is determined based on rate-distortion optimization (RDO) among candidate
   scanning methods derived in view of a partition shape of the partition mode.
             [7] The determining of the scanning method may derive horizontal scanning and
25 zigzag scanning as the candidate scanning methods when the partition mode has a
                                              - 2 -

   vertically oriented partition shape and derive vertical scanning and zigzag scanning as the
   candidate scanning methods when the partition mode has a horizontally oriented partition
   shape.
             [8] The partition mode may include an N x 2N mode, a 2N x N mode, a 2N x 2N
 5 mode, an N x N mode, a 2N x nU mode, a 2N x nD mode, an nL x 2N mode and an nR x
   2N mode based on a size of the prediction unit for which inter prediction has been
   performed.
             [9] The partition mode having the vertically oriented partition shape may include
   the N x 2N mode, the nL x 2N mode and the nR x 2N mode, wherein the partition mode
10 having the vertically oriented partition shape is a 1/2N x 2N mode of a left partition with a
   smaller partition size for the nL x 2N mode and the partition mode having the vertically
   oriented partition shape is a 1/2N x 2N mode of a right partition with a smaller partition
   size for the nR x 2N mode.
             [10] The partition mode having the horizontally oriented partition shape may
15 include the 2N x N mode, the 2N x nU mode and the 2N x nD mode, wherein the partition
   mode having the horizontally oriented partition shape is     a 2N x 1/2N mode of an upper
   partition with a smaller partition size for the 2N x nU mode and the partition mode having
   the horizontally oriented partition shape is es a 2N x 1/2N mode of a lower partition with a
   smaller partition size for the 2N x nD mode.
20           [11] The determining of the scanning method may determine zigzag scanning as
   the scanning method when the partition mode is the 2N x 2N mode, the N x N mode, a
   3/2N x 2N mode of a right partition with a larger partition size in the nL x 2N mode, a
   3/2N x 2N mode of a left partition with a larger partition size in the nR x 2N mode, a 2N x
   3/2N mode of a lower partition with a larger partition size in the 2N x nU mode or a 2N x
25 3/2N mode of an upper partition with a larger partition size in the 2N x nD mode.
                                                 - 3 -

             [12] The information on the scanning method may be indicated using a flag, and
   the flag may indicate whether zigzag scanning is used.
             [13] Another embodiment of the present invention provides a video encoding
   method.     The method includes determining a scanning method based on a partition mode
 5 of a prediction unit for which short distance intra prediction (SDIP) has been performed,
   and encoding information on the scanning method, wherein the scanning method is
   determined based on RDO among candidate scanning methods derived in view of a
   partition shape of the partition mode.
             [14] The determining of the scanning method may derive horizontal scanning and
10 zigzag scanning as the candidate scanning methods when the partition mode has a
   vertically oriented partition shape and derive vertical scanning and zigzag scanning as the
   candidate scanning methods when the partition mode has a horizontally oriented partition
   shape.
             [15] The partition mode may include a 1/2N x 2N mode, a 2N x 1/2N mode, an N
15 x N mode and a 2N x 2N mode based on a size of the prediction unit for which SDIP has
   been performed.
             [16] The partition mode having the vertically oriented partition shape may
   include is the 1/2N x 2N mode, and the partition mode having the horizontally oriented
   partition shape may include the 2N x 1/2N mode.
20           [17] The determining of the scanning method may determine zigzag scanning as
   the scanning method when the partition mode is the N x N mode or the 2N x 2N mode.
             [18] The information on the scanning method may be indicated using a flag, and
   the flag may indicate whether zigzag scanning is used.
             [19] Still another embodiment of the present invention provides a video encoding
25 method.     The method includes determining a scanning method based on a partition mode
                                               - 4-

   of a prediction unit, and inverse-scanning a transform coefficient according to the
   scanning method, wherein the scanning method is determined based on the partition mode
   using information signaled from an encoding apparatus, and the signaled information is a
   flag indicates whether zigzag scanning is used.
 5           [20] The determining of the scanning method may decode the flag indicating
   whether zigzag scanning is used and determine the scanning method based on a value of
   the decoded flag when the partition mode has a vertically oriented partition shape or
   horizontally oriented partition shape, in which one of zigzag scanning and horizontal
   scanning may be selected based on the value of the decoded flag when the partition mode
10 is the vertically oriented partition shape, and one of zigzag scanning and vertical scanning
   may be selected based on the value of the decoded flag when the partition mode has the
   horizontally oriented partition shape.
             [21] The partition mode may include an N x 2N mode, a 2N x N mode, a 2N x
   2N mode, an N x N mode, a 2N x nU mode, a 2N x nD mode, an nL x 2N mode and an nR
15 x 2N mode based on a size of the prediction unit for which inter prediction is performed.
             [22] the partition mode having the vertically oriented partition shape comprises
   the N x 2N mode, the nL x 2N mode and the nR x 2N mode, wherein the partition mode
   having the vertically oriented partition shape is a 1/2N x 2N mode of a left partition with a
   smaller partition size for the nL x 2N mode and the partition mode having the vertically
20 oriented partition shape is a 1/2N x 2N mode of a right partition with a smaller partition
   size for the nR x 2N mode, and wherein the partition mode having the horizontally
   oriented partition shape comprises the 2N x N mode, the 2N x nU mode and the 2N x nD
   mode, wherein s the partition mode having the horizontally oriented partition shape is a
   2N x 1/2N mode of an upper partition with a smaller partition size for the 2N x nU mode
25 and the partition mode having the horizontally oriented partition shape is a 2N x 1/2N
                                                - 5 -

   mode of a lower partition with a smaller partition size for the 2N x nD mode
             [23] The determining of the scanning method may determine zigzag scanning as
   the scanning method when the partition mode is the 2N x 2N mode, the N x N mode, a
   3/2N x 2N mode of a right partition with a larger partition size in the nL x 2N mode, a
 5 3/2N x 2N mode of a left partition with a larger partition size in the nR x 2N mode, a 2N x
   3/2N mode of a lower partition with a larger partition size in the 2N x nU mode or a 2N x
   3/2N mode of an upper partition with a larger partition size in the 2N x nD mode.
             [24] The partition mode may include a 1/2N x 2N mode, a 2N x 1/2N mode, an N
   x N mode and a 2N x 2N mode based on a size of the prediction unit in SDIP, the partition
10 mode having the vertically oriented partition shape including is the 1/2N x 2N mode and
   the partition mode having the horizontally oriented partition shape including the 2N x
    1/2N mode.
             [25] The determining of the scanning method may determine zigzag scanning as
   the scanning method when the partition mode is the N x N mode or the 2N x 2N mode.
15   [Advantageous Effects]
            [26] According to the present invention, a scanning method for transform
   coefficients is determined using a partition mode of a prediction unit, that is, particular
   directivity or particular texture of the prediction unit, thereby increasing efficiency in
   encoding and decoding.
20   [Description of Drawings]
            [27] FIG. 1 is a block diagram illustrating a video encoding apparatus according
   to an exemplary embodiment of the present invention.
            [28] FIG. 2 is a block diagram illustrating a video decoding apparatus according
   to an exemplary embodiment of the present invention.
25          [29] FIG. 3 schematically illustrates a coefficient scanning method according to
                                              - 6 -

   the present invention.
             [30] FIG. 4 illustrates a method of determining and encoding a scanning method
   based on a partition mode of a prediction unit according to an exemplary embodiment of
   the present invention.
 5           [31] FIG. 5 illustrates a method of determining and encoding a scanning method
   in asymmetric motion partition (AMP) according to an exemplary embodiment of the
   present invention.
             [32] FIG. 6 illustrates a method of determining and encoding a scanning method
   in short distance intra prediction (SDIP) according to an exemplary embodiment of the
10 present invention.
             [33] FIG. 7 is a flowchart illustrating a video encoding process according to the
   present invention.
             [34] FIG. 8 is a flowchart illustrating a video decoding process according to the
   present invention.
15   [Mode for Invention]
             [35] The present invention may be changed and modified variously and be
   illustrated with reference to different exemplary embodiments, some of which will be
   described and shown in the drawings.       However, these embodiments are not intended for
   limiting the invention but are construed as including includes all modifications,
20 equivalents and replacements which belong to the spirit and technical scope of the
   invention.    Like reference numerals in the drawings refer to like elements throughout.
             [36] Although the terms first, second, etc. may be used to describe various
   elements, these elements should not be limited by these terms.     These terms are used only
   to distinguish one element from another element.       For example, a first element could be
25 termed a second element and a second element could be termed a first element likewise
                                                -7 -

   without departing from the teachings of the present invention.            The term "and/or"
   includes any and all combinations of a plurality of associated listed items.
            [37] It will be understood that when an element is referred to as being "connected
   to" or "coupled to" another element, the element can be directly connected or coupled to
 5 another element or intervening elements.     On the contrary, when an element is referred to
   as being "directly connected to" or "directly coupled to" another element, there are no
   intervening elements present.
            [38] The terminology used herein is for the purpose of describing particular
   embodiments only and is not intended to be limiting of the invention.         As used herein,
10 the singular forms "a," "an" and "the" are intended to include the plural forms as well,
   unless the context clearly indicates otherwise.   It will be further understood that the terms
   "include" and/or "have," when used in this specification, specify the presence of stated
   features, integers, steps, operations, elements, and/or components, but do not preclude the
   presence or addition of one or more other features, integers, steps, operations, elements,
15 components, and/or groups thereof.
            [39] Hereinafter, exemplary embodiments of the invention will be described in
   detail with reference to the accompanying drawings.          Like reference numerals in the
   drawings refer to like elements throughout, and redundant descriptions of like elements
   will be omitted herein.
20          [40] FIG. 1 is a block diagram illustrating a video encoding apparatus according
   to an exemplary embodiment of the present invention.
            [41] Referring to FIG. 1, the video encoding apparatus 100 includes a picture
   partition module 110, prediction modules 120 and 125, a transform module 130, a
   quantization module 135, a rearrangement module 160, an entropy encoding module 165,
25 a dequantization module 140, an inverse transform module 145, a filter module 150 and a
                                               - 8 -

   memory 155.
            [42] Although elements illustrated in FIG. 1 are independently shown so as to
   represent different distinctive functions in the video encoding apparatus, such a
   configuration does not indicate that each element is constructed by a separate hardware
 5 constituent or software constituent.     That is, the elements are independently arranged for
   convenience of description, wherein at least two elements may be combined into a single
   element, or a single element may be divided into a plurality of elements to perform
   functions.    It is to be noted that embodiments in which some elements are integrated into
   one combined element and/or an element is divided into multiple separate elements are
10 included in the scope of the present invention without departing from the essence of the
   present invention.
            [43] Some elements are not essential to the substantial functions in the invention
   and may be optional constituents for merely improving performance.         The invention may
   be embodied by including only constituents essential to embodiment of the invention,
15 except for constituents used to merely improve performance.           The structure including
   only the essential constituents except for the optical constituents used to merely improve
   performance belongs to the scope of the invention.
            [44] The picture partition module 110 may partition an input picture into at least
   one process unit.      Here, the process unit may be a prediction unit (PU), a transform unit
20 (TU) or a coding unit (CU).       The picture partition module 110 may partition one picture
   into a plurality of combinations of CUs, PUs and TUs and select one combination of CUs,
   PUs and TUs on the basis of a predetermined criterion (for example, a cost function),
   thereby encoding the picture.
            [45] For example, one picture may be partitioned into a plurality of CUs.          A
25 recursive tree structure, such as a quad tree structure, may be used to partition a picture
                                                 - 9 -

   into CUs.    A CU, for which a picture or a CU of a maximum size may be as root, may be
   partitioned into sub-coding units with as many child nodes as the partitioned CUs.    A CU
   which is not partitioned any more in accordance with a predetermined limitation is a leaf
   node.    That is, assuming that a CU may be partitioned into quadrants only, a single CU
 5 may be partitioned into at most four different CUs.
            [46] In the embodiments of the invention, a CU may be used to refer to not only a
   unit of encoding but also a unit of decoding.
            [47] A PU may be partitioned into at least one square or rectangular form with the
   same size in a CU.    For PUs partitioned from a same CU, a PU may have different shape
10 from other PUs.
            [48] When a PU for intra prediction is generated based on a CU and the CU is not
   a minimum CU, the CU may be subjected to intra prediction without being partitioned into
   plural PUs (NxN).
            [49] The prediction modules 120 and 125 may include an inter prediction module
15 120 to perform inter prediction and an intra prediction module 125 to perform intra
   prediction.    The prediction modules 120 and 125 may determine which of inter
   prediction and intra prediction should be performed on a PU, and may determine specific
   information (for example, an intra prediction mode, a motion vector, and a reference
   picture) of the determined prediction method.    Here, a process unit on which prediction is
20 performed may be different from a process unit for which a prediction method and
   specific information thereon are determined.      For example, a prediction method and a
   prediction mode may be determined for each PU, while prediction may be performed for
   each TU.     A residual value (residual block) between a generated predicted block and an
   original block may be input to the transform module 130.          Further, prediction mode
25 information, motion vector information and the like used for prediction may be encoded
                                              - 10 -

   along with the residual value by the entropy encoding module 165 and be transmitted to
   the decoding apparatus.      When a specific encoding mode is used, the original block may
   be encoded and transmitted to the decoding apparatus without generating a prediction
   block by the prediction modules 120 and 125.
 5          [50]     The inter prediction module 120 may predict a PU on the basis of
   information on at least one picture among a previous picture of a current picture and a
   subsequent picture of a current picture.        The inter prediction module 120 may include a
   reference picture interpolation module, a motion prediction module, and a motion
   compensation module.
10          [51] The reference picture interpolation module may be supplied with reference
   picture information from the memory 155 and generate pixel information less than an
   integer pixel from a reference picture.        In the case of luma pixels, a DCT-based 8-tap
   interpolation filter with a variable filter coefficient may be used to generate information on
   a pixel smaller than an integer pixel in a unit of a 1/4 pixel.     In the case of chroma pixels,
15 a DCT-based 4-tap interpolation filter with a variable filter coefficient may be used to
   generate information on a pixel smaller than an integer pixel in a unit of a 1/8 pixel.
            [52] The motion prediction module may perform motion prediction on the basis of
   the reference picture interpolated by the reference picture interpolation module.        Various
   methods, such as a full search-based block matching algorithm (FBMA), a three-step
20 search (TSS) algorithm and a new three-step search (NTS) algorithm, may be used to
   calculate a motion vector.     A motion vector has a motion vector value in the unit of a 1/2
   or 1/4 pixel on the basis of an interpolated pixel.         The motion prediction module may
   predict a current PU using different motion prediction methods.           Various methods, such
   as skip mode, merge mode, and advanced motion vector prediction (AMVP), etc. may be
25 used as the motion prediction method.
                                                 -  11  -

            [53] The intra prediction module 125 may generate a PU on the basis of
   information on a reference pixel neighboring to a current block.      The information on a
   reference pixel neighboring to the current block is pixel information in a current picture.
   When a reference pixel is a pixel for which inter prediction has been performed because a
 5 block, which includes the reference pixel, neighboring to the current PU is a block for
   which inter prediction has been performed, information on a reference pixel included in
   the block for which inter prediction has been performed may be replaced with information
   on a reference pixel in a block for which intra prediction has been performed.      That is,
   when a reference pixel is not available, information on the unavailable reference pixel may
10 be replaced with information on at least one reference pixel of the available reference
   pixels.
            [54] A prediction mode of intra prediction includes a directional prediction mode
   in which reference pixel information is used according to a prediction direction and a non
   directional prediction mode in which information on direction is not used in performing
15 prediction.   A mode for predicting luma information and a mode for predicting chroma
   information may be different         from each other.      Further, intra prediction mode
   information used to obtain luma information or predicted luma signal information may be
   used to predict chroma information.
            [55] If a PU and a TU have the same size when intra prediction is performed, intra
20 prediction on the PU may be performed based on left pixels, an upper-left pixel and upper
   pixels of the PU.     On the other hand, if a PU and a TU have different sizes when intra
   prediction is performed, intra prediction may be performed using reference pixels based
   on the TU.      Intra prediction using NxN partitioning may be performed only for a
   minimum CU.
25          [56] In the intra prediction method, a predicted block may be generated according
                                               - 12 -

   to the prediction mode after an adaptive intra smoothing (AIS) filter being applied.
   Different types of AIS filters may be applied to the reference pixels.          In the intra
   prediction method, the intra prediction mode of a current PU may be predicted from an
   intra prediction mode of a PU neighboring to the current PU.    In predicting the prediction
 5 mode of the current PU using mode information predicted from a neighboring PU, when
   the current PU and the neighboring PU have the same intra prediction mode, information
   indicating that the current PU and the neighboring PU have the same prediction mode may
   be transmitted using predetermined flag information.       When the current PU and the
   neighboring PU have different prediction modes, information on the prediction mode of
10 the current block may be encoded by entropy encoding.
            [57] A residual block including residual information which is a difference
   between the original block of the PU and the predicted block of a PU generated based on
   the PU generated by the prediction modules 120 and 125, may be generated.                The
   generated residual block may be input to the transform module 130.
15          [58] The transform module 130 may transform the residual block using a
   transform method such as Discrete Cosine Transform (DCT) or Discrete Sine Transform
   (DST).     The residual block includes information on the residual between the PU
   generated by the prediction modules 120 and 125 and the original block. A transform
   method to be used to transform the residual block may be determined among DCT and
20 DST on the basis of the information on the intra prediction mode applied to the PU which
   is used to generate the residual block.
            [59] The quantization module 135 may quantize values transformed into a
   frequency domain by the transform module 130.       A quantization coefficient may change
   depending on a block or importance of a picture.      Values output from the quantization
25 module 135 may be provided to the dequantization module 140 and the rearrangement
                                              - 13 -

   module 160.
            [60] The rearrangement module 160 may rearrange coefficients with respect to
   quantized residual values.
            [61] The rearrangement module 160 may change a two-dimensional (2D) block of
 5 coefficients into a one-dimensional (ID) vector of coefficients through coefficient
   scanning. For example, the rearrangement module 125 may change a 2D block of
   coefficients into a ID vector of coefficients by scanning from DC coefficients to
   coefficients of a high frequency domain using zigzag scanning.      Vertical scanning for
   scanning a 2D block of coefficients in a vertical direction and horizontal scanning for
10 scanning a 2D block of coefficients in a horizontal direction may be used depending on a
   size of a TU and an intra prediction mode, instead of zigzag scanning.         That is, a
   scanning method for use may be selected based on the size of the TU and the intra
   prediction mode among zigzag scanning, vertical scanning, and horizontal scanning.
            [62] The entropy encoding module 165 may perform entropy encoding on the
15 basis of the values obtained by the rearrangement module 160.          Various encoding
   methods, such as exponential Golomb coding, context-adaptive variable length coding
   (CAVLC), or context-adaptive binary arithmetic coding (CABAC), may be used for
   entropy encoding.
            [63] The entropy encoding module 165 may encode a variety of information, such
20 as residual coefficient information and block type information on a CU, prediction mode
   information, partitioning unit information, PU information, transfer unit information,
   motion vector information, reference frame information, block interpolation information
   and filtering information from the rearrangement module 160 and the prediction modules
   120 and 125.
25          [64] The entropy encoding module 165 may entropy-encode coefficients of a CU
                                             - 14 -

   input from the rearrangement module 160.
             [65] The dequantization module 140 and the inverse transform module 145
   dequantize the values which are quantized by the quantization module 135 and inversely
   transform the values which are transformed by the transform module 130.            The residual
 5 values generated by the dequantization module 140 and the inverse transform module 145
   may be added to the predicted PU.          The predicted PU may be predicted by the motion
   vector prediction module, the motion compensation module, and the intra prediction
   module of the prediction modules 120 and 125.           A reconstructed block may be generated
   by adding the residual values to the predicted PU (predicted values).
10           [66] The filter module 150 may include at least one of a deblocking filter, an
   offset module, and an adaptive loop filter (ALF).
             [67] The deblocking filter may remove block distortion generated on boundaries
   between blocks in a reconstructed picture.          Whether to apply the deblocking filter to a
   current block may be determined on the basis of pixels included in several rows or
15 columns of the block.       When the deblocking filter is applied to a block, a strong filter or
   a weak filter may be applied depending on a required deblocking filtering strength.
   When horizontal filtering and vertical filtering are performed in applying the deblocking
   filter, the horizontal filtering and vertical filtering may be performed in parallel.
             [68] The offset module may apply an offset form the original picture in the unit of
20 a pixel to the picture for which the deblocking filtering process is completed. , A region to
   which the offset may be applied may be determined after partitioning pixels of a picture
   into a predetermined number of regions.           The offset may be applied to the determined
   area in consideration of edge information on each pixel and the method of applying the
   offset to the determined area..
25           [69] The ALF may perform filtering based on a comparison result of the filtered
                                                 - 15 -

   reconstructed picture and the original picture.     Pixels included in a picture may be
   partitioned into predetermined groups, a filter to be applied to each group may be
   determined, and differential filtering may be performed for each group.    Information on
   whether to apply the ALF may be transferred by each coding unit (CU) and a size and
 5 coefficient of an ALF to be applied to each block may vary.    The ALF may have various
   types and a number of coefficients included in a corresponding filter may vary.   Further,
   an ALF filter with the same form (fixed form) may be applied to a block regardless of
   characteristics of the block.
            [70] The memory 155 may store a reconstructed block or picture output from the
10 filter module 150, and the stored reconstructed block or picture may be supplied to the
   prediction modules 120 and 125 when performing inter prediction.
            [71] FIG. 2 is a block diagram illustrating a video decoding apparatus according
   an exemplary embodiment of the present invention.
            [72] Referring to FIG. 2, the video decoding apparatus 200 may include an
15 entropy decoding module 210, a rearrangement module 215, a dequantization module 220,
   an inverse transform module 225, prediction modules 230 and 235, a filter module 240,
   and a memory 245.
            [73] When a video bitstream is input from the video encoding apparatus, the input
   bitstream may be decoded according to an inverse process of the video encoding process
20 performed in the video encoding apparatus.
            [74] The entropy decoding module 210 may perform entropy decoding according
   to an inverse process of the entropy encoding process by the entropy encoding module of
   the video encoding apparatus.        For example, various methods, such as exponential
   Golomb coding, CAVLC or CABAC, may be used for entropy encoding, corresponding to
25 the method used by the video encoding apparatus.
                                              - 16 -

            [75] The entropy decoding module 210 may decode information associated with
   intra prediction and inter prediction performed by the encoding apparatus.
            [76] The rearrangement module 215 may perform rearrangement on the bit stream
   entropy-decoded by the entropy decoding module 210 on the basis of the rearrangement
 5 method of the encoding module.        The rearrangement module 215 may reconstruct and
   rearrange coefficients in a ID vector form into coefficients in a 2D block.             The
   rearrangement module 215 may be provided with information on coefficient scanning
   performed by the encoding apparatus and may perform rearrangement using a method of
   inversely scanning the coefficients on the basis of scanning order in which scanning is
10 performed by the encoding apparatus.
            [77] The dequantization module 220 may perform dequantization on the basis of a
   quantization parameter provided from the encoding apparatus and the rearranged
   coefficients of the block.
            [78] The inverse transform module 225 may perform inverse DCT and inverse
15 DST on a result of quantization performed by the video encoding apparatus, having been
   subjected to DCT and DST performed by the transform module.           Inverse transform may
   be performed on the basis of a transfer unit determined by the video encoding apparatus.
   The transform module of the video encoding apparatus may selectively perform DCT and
   DST depending on a plurality of information elements, such as a prediction method, a size
20 of the current block and a prediction direction, etc. and the inverse transform module 225
   of the video decoding apparatus may perform inverse transform on the basis of
   information on the transform performed by the transform module of the video encoding
   apparatus.
            [79] The prediction modules 230 and 235 may generate a prediction block
25 (predicted block) on the basis of prediction block generation information supplied from
                                               - 17 -

   the entropy decoding module 210 and information on a previously-decoded block or
   picture provided from the memory 245.
            [80] Similarly to the operation of the video encoding apparatus as described above,
   if a PU and a TU have the same size when intra prediction is performed, intra prediction
 5 on the PU is performed based on left pixels, an upper-left pixel and upper pixels of the PU.
   On the other hand, if a PU and a TU have different sizes when intra prediction is
   performed, intra prediction may be performed using reference pixels based on the TU.
   Intra prediction using NxN partitioning may be used only for a minimum CU.
            [81] The prediction modules 230 and 235 may include a PU determination
10 module, an inter prediction module and an intra prediction module.                 The PU
   determination module may receive a variety of information, such as PU information,
   prediction mode information on an intra prediction method and motion prediction-related
   information on an inter prediction method, etc. from the entropy decoding module 210,
   may determine a PU for a current CU.         The PU determination module may determine
15 which of the inter prediction and the intra prediction is performed on the PU.      An inter
   prediction module 230 may perform inter prediction on a current PU on the basis of
   information on at least one picture among a previous picture and a subsequent picture of a
   current picture including the current PU.        An inter prediction module 230 may use
   information necessary for inter prediction for the current PU provided from the video
20 encoding apparatus.
            [82] In order to perform inter prediction, it may be determined on the basis of a
   CU whether a motion prediction method for a PU included in the CU is a skip mode, a
   merge mode or an AMVP mode.
            [83] An intra prediction module 235 may generate a prediction block on the basis
25 of pixel information in a current picture.   When a PU is a PU for which intra prediction is
                                               - 18 -

   performed, intra prediction may be performed based on intra prediction mode information
   on the PU provided from the video encoding apparatus.       The intra prediction module 235
   may include an AIS filter, a reference pixel interpolation module, and a DC filter.      The
   AIS filter performs filtering on reference pixels of a current block     The AIS filter may
 5 decide whether to apply the filter or not depending on a prediction mode for the current
   PU.     AIS filtering may be performed on the reference pixels of the current block using
   the prediction mode for the PU and information on the AIS filter provided from the video
   encoding apparatus.     When the prediction mode for the current block is a mode not
   performing AIS filtering, the AIS filter may not be applied.
10           [84] When the prediction mode for the PU is a prediction mode of performing
   intra prediction on the basis of pixel values obtained by interpolating the reference pixels,
   the reference pixel interpolation module may generate reference pixels in a unit of a
   fractional pixel less than an integer pixel (i.e. full pixel) by interpolating the reference
   pixels.    When the prediction mode for the current PU is a prediction mode of generating
15 a prediction block without interpolating the reference pixels, the reference pixels may not
   be interpolated.    The DC filter may generate a prediction block through filtering when
   the prediction mode for the current block is the DC mode.
             [85] The reconstructed block or picture may be provided to the filter module 240.
   The filter module 240 includes a deblocking filter, an offset module, and an ALF.
20           [86] The video encoding apparatus mat provide information on whether the
   deblocking filter is applied to a corresponding block or picture, and information on which
   of a strong filter and a weak filter is applied when the deblocking filter is used.      The
   deblocking filter of the video decoding apparatus may be provided with information on the
   deblocking filter from the video encoding apparatus and may perform deblocking filtering
25 on a corresponding block.
                                              - 19 -

            [87] The offset module may apply offset to the reconstructed picture on the basis
   of information on an offset type and offset value applied to the picture in the encoding
   process.
            [88] The ALF may be applied to a CU on the basis of information on whether the
 5 ALF is applied and ALF coefficient information, etc. provided from the encoding
   apparatus.    The ALF information may be included and provided in a specific parameter
   set.
            [89] The memory 245 may store the reconstructed picture or block for use as a
   reference picture or a reference block and may provide the reconstructed picture to an
10 output module.
            [90] As described above, in the embodiments of the invention, the term "coding
   unit" is used as an encoding unit for a convenience of descriptions.   However, the term
   "coding unit" may be also used as a unit of decoding.
            [91] Hereinafter, scanning methods based on prediction modes and partition
15 modes in prediction to be illustrated in FIGS. 3 to 8 according to exemplary embodiments
   of the present invention may be achieved in accordance with functions of the modules of
   the encoding apparatus and the decoding apparatus described above in FIGS. 1 and 2,
   which fall within the scope of the present invention.
            [92] FIG. 3 schematically illustrates coefficient scanning methods according to
20 the present invention.
            [93] Referring to FIG. 3, the scanning methods may include horizontal scanning
   310, vertical scanning 320, and zigzag scanning 330 or upright diagonal scanning 340.
   Here, one of these scanning methods shown in FIG. 3 may be used based on a partitioned
   shape of a PU, and a 2D block of quantized transform coefficients may be changed into a
25  ID vector of transform coefficients by scanning.
                                              - 20 -

            [94] Horizontal scanning 310, which scans transform coefficients in a horizontal
   direction, may be applied, for example, when a PU is a partition which is a vertically
   oriented block, such as an N x 2N block.     The vertically oriented block is highly likely to
   include a texture of a vertical component, in which the transform coefficients are highly
 5 likely to be distributed in the horizontal direction.   Thus, a scanning order shown in the
   method 310 of FIG. 3 may be applied to scanning the transform coefficients.
            [95] Vertical scanning 320, which scans transform coefficients in a vertical
   direction, may be applied, for example, when a PU is a partition which is a horizontally
   oriented block, such as a 2N x N block.     The horizontally oriented block is highly likely
10 to include texture of horizontal component, in which the transform coefficients are highly
   likely to be distributed in the vertical direction.    Thus, a scanning order shown in the
   method 320 of FIG. 3 may be applied to scanning the transform coefficients.
            [96] Zigzag scanning 330 or upright diagonal scanning 340 may be applied when
   a PU do not have particular directivity or particular component of texture.     For example,
15 zigzag scanning 330 or upright diagonal scanning 340 may be applied to 2N x 2N or N x
   N square block.
            [97] The scanning methods of FIG. 3 are provided for examples of the present
   invention, and the present invention is not limited thereto.    Alternative scanning methods
   performed in different orders may be also used as well as the scanning methods of FIG. 3.
20          [98] As described above, when a PU is a partition such as an Nx2N block or a
   2NxN block, these blocks may be highly likely to have particular component of texture or
   strong directionality.    Accordingly horizontal scanning or vertical scanning is used
   depending on a partition shape of the PU.     However, even though a PU is a partition such
   as an Nx2N block or a 2NxN block, these blocks may have insignificant directionality or
25 not include particular component of texture.      In this case, it may not be efficient to use
                                              - 21 -

   the particular scanning methods, for example, horizontal scanning for an Nx2N block and
   vertical scanning for a 2NxN block.          Thus, a method of effectively scanning and
   encoding transform coefficients is needed.
            [99] FIG. 4 illustrates a method of determining a scanning method and encoding
 5 information thereon based on a partition mode of a PU according to an exemplary
   embodiment of the present invention.
            [100] Referring to FIG. 4, a single CU of an inter prediction mode may be
   partitioned into PUs with the same size or different sizes.  For example, the CU may be
   partitioned into 2N x N block 400, N x 2N block 410, 2N x 2N block 420 or N x N block
10 430.    Partition modes PartMode of the PUs may be determined based on sizes of the
   partitioned PUs.
            [101] Partition modes PartMode of PUs may include a PART_2NxN mode in
   which a CU is partitioned into 2N x N 400 blocks, a PARTNx2N mode in which a CU is
   partitioned into N x 2N 410 blocks, a PART_2Nx2N mode in which a CU is partitioned
15 into 2N x 2N 420 blocks, and a PARTNxN mode in which a CU is partitioned into N x N
   430 blocks.
            [102] In the present embodiment, a scanning method is determined based on a
   partition mode of a PU, in which a partitioned shape of the partition mode may be
   considered.     That is, candidate scanning methods may be obtained in view of partitioned
20 shapes of PUs, among which a scanning method may be determined based on rate
   distortion optimization (RDO).
            [103] When the partition mode indicates a horizontally oriented shape, for
   example, the partition mode is the PART_2NxN mode in which a CU is partitioned into
   2N x N 400 blocks, the blocks may be likely to have particular component of texture or
25 directionality (for example, horizontal component of texture or transform coefficients
                                               - 22 -

   distributed in the vertical direction).   Vertical scanning may be derived as a candidate
   scanning method in view of such a partitioned shape.      Also, zigzag scanning (or upright
   diagonal scanning) may be derived as a candidate scanning method considering that the
   blocks are likely not to have particular component of texture or directionality.     That is,
 5 for the partition mode of the horizontally oriented shape, a scanning method having
   minimum RDO may be selected among the two candidate scanning methods, vertical
   scanning and zigzag scanning (or upright diagonal scanning).
            [104] Alternatively, when the partition mode indicates a vertically oriented shape,
   for example, the partition mode is the PARTNx2N mode in which a CU is partitioned
10 into N x 2N 410 blocks, the blocks may be likely to have particular component of texture
   or directionality (for example, vertical component of texture or transform coefficients
   distributed in the horizontal direction).      Horizontal scanning may be derived as a
   candidate scanning method in view of such a partitioned shape.        Also, zigzag scanning
   (or upright diagonal scanning) may be derived as a candidate scanning method
15 considering that the blocks are likely not to have particular component of texture or
   directionality.  That is, for the partition mode of a vertically oriented shape, a scanning
   method having minimum RDO may be selected among the two candidate scanning
   methods, horizontal scanning and zigzag scanning (or upright diagonal scanning).
            [105] Meanwhile, for the partition mode of a square shape, for example,         the
20 PART_2Nx2N mode in which a CU is partitioned into 2N x 2N 420 blocks or the
   PARTNxN mode in which a CU is partitioned in N x N 430 blocks, zigzag scanning (or
   upright diagonal scanning) may be used.
            [106] Table 1 illustrates available scanning methods according to partition modes
   of PUs according to the exemplary embodiment of the present invention.          Here, in the
25 PART_2NxN mode and the PARTNx2N mode, one scanning method may be selected in
                                               - 23 -

   view of RDO from two candidate scanning methods.
            [107] [Table 1]
      PU Partition mode                  Scan pattern
        PART_2NxN            Vertical scanning/Zigzag scanning
        PARTNx2N           Horizontal scanning/Zigzag scanning
       PART_2Nx2N                      Zigzag scanning
        PARTNxN                        Zigzag scanning
            [108] When a scanning method is determined based on a partition mode of a PU
   as described above, transform coefficients may be scanned using the determined scanning
 5 method.      Information on the determined scanning method may be encoded and
   transmitted to the decoding apparatus.     The information on the scanning method may be
   indicated using a flag, for example, a flag isZigZagScanFlag indicating whether zigzag
   scanning is used.
            [109] For instance, when the partition mode of the PU is the PART_2NxN mode,
10 information on a determined scanning method of vertical scanning and zigzag scanning
   (or upright diagonal scanning) may be encoded using a flag, and the flag information may
   be transmitted to the decoding apparatus.     In the PART_2NxN mode, isZigZagScanFlag
   may be set to 1 if zigzag scanning is determined to be performed, and isZigZagScanFlag
   may be set to 0 if vertical scanning is determined to be performed.    Alternatively, when
15 the partition mode of the PU is the PARTNx2N, information on a determined scanning
   method of horizontal scanning and zigzag scanning (or upright diagonal) scanning may be
   encoded using a flag, for example, isZigZagScanFlag, and the flag information may be
   transmitted to the decoding apparatus.
            [110] FIG. 5 illustrates a method of determining a scanning method and encoding
20 the information thereon in asymmetric motion partition (AMP) according to an exemplary
                                               - 24 -

   embodiment of the present invention.
             [111] As described above, a single CU of an inter prediction mode may be
   partitioned into PUs with the same size or different sizes.    As shown in FIG. 5, a 64 x 64
   block may be partitioned into 16 x 64 block, 48 x 64 block, 64 x 16 block or 64 x 48 block
 5 i.e. blocks of different shapes.  This partition mode is referred to as AMP.   AMP may be
   applied to partitioning a CU to enhance coding efficiency when a picture includes
   irregular image patterns.
             [112] From left of FIG. 5, AMP includes a PARTnLx2N mode in which a CU is
   partitioned into blocks with size of nL x 2N 500, a PARTnRx2N mode in which a CU is
10 partitioned into blocks with size of nR x 2N 510, a PART_2NxnU mode in which a CU is
   partitioned into blocks with size of 2N x nU 520, and a PART_2NxnD mode in which a
   CU is partitioned into blocks with size of 2N x nD 530.          Here, in the PARTnLx2N
   mode and the PARTnRx2N mode, a PU may have a size of 1/2N x 2N 501 and 512 or
   size of 3/2N x 2N 502 and 511.        In the PART_2NxnU mode and the PART_2NxnD, a
15 PU may have a size of 2N x 1/2N 521 and 532 or size of 2N x 3/2N 522 and 531.
             [113] As described in FIG. 4, according to a embodiment of the present invention,
   a scanning method may be determined based on a partition mode, that is, a size of a
   partitioned block in AMP.      That is, candidate scanning methods may be obtained in view
   of partitioned shapes of AMP, among which a scanning method may be determined based
20 on RDO.
             [114] For instance, for a     vertically oriented block (a block that its height is
   longer than its width) in the 1/2N x 2N mode, such as a left block 501 of nL x 2N block
   500 and a right block 512 of nR x 2N block 510, horizontal scanning considering
   particular component of texture or directionality that the vertically oriented block may
25 have (for example, vertical component of texture and transform coefficients distributed in
                                                - 25 -

   the horizontal direction), or zigzag scanning (or upright diagonal scanning) considering
   that the vertically oriented block does not have particular component of texture or
   directionality, may be derived as candidate scanning methods.      Here, a scanning method
   having minimum RDO may be selected among the two candidate scanning methods.
 5          [115] Alternatively, for a horizontally oriented block (a block that its width is
   longer than its height) in the 2N x 1/2N mode, such as an upper block 521 of 2N x nU
   block 520 and a lower block 532 of 2N x nD block 530, vertical scanning considering
   particular component of texture and directionality that the horizontally oriented block may
   have (for example, horizontal texture or transform coefficients distributed in the vertical
10 direction), or zigzag scanning (or upright diagonal scanning) considering that the
   horizontally oriented block does not have particular component of texture or directionality,
   may be derived as candidate scanning methods.           Here, a scanning method having
   minimum RDO may be selected among the two candidate scanning methods.
            [116] Meanwhile, zigzag scanning (or upright diagonal scanning) may be used for
15 larger partitioned portions of nL x 2N 500, nR x 2N 510, 2N x nU 520 and 2N x nD 530
   (i.e. 3/2N x 2N and 2N x 3/2N modes).       That is, zigzag scanning (or upright diagonal
   scanning) may be used for a right partition 502 of nL x 2N block 500, a left partition 512
   of nR x 2N block 510, a lower partition 522 of 2N x nU block 520 and an upper partition
   531 of 2N x nD block 530.
20          [117] When a scanning method is determined based on an AMP mode as
   described above, information on the determined scanning method may be encoded.          For
   example, as described above in FIG. 4, in the PARTnLx2N mode and PARTnRx2N
   mode and for the vertically oriented blocks 501 and 512 (i.e. the 1/2N x 2N mode),
   isZigZagScanFlag may be set to 1 if zigzag scanning is used, and isZigZagScanFlag may
25 be set to 0 if horizontal scanning is used . In the PART_2NxnU and PART_2NxnD
                                             - 26 -

   modes and for the horizontally oriented blocks 521 and 532 (i.e. the 2N x 1/2N mode),
   isZigZagScanFlag may be set to 1 if zigzag scanning is used and isZigZagScanFlag may
   be set to 0 if vertical scanning is used.      Such flag information may be encoded and
   transmitted to the decoding apparatus.
 5          [118] FIG. 6 illustrates a method of determining a scanning method and encoding
   the information thereon in short distance intra prediction (SDIP) according to an
   exemplary embodiment of the present invention.
            [119] SDIP refers to a method of partitioning a CU into 2N x 2N PU, N x N PU,
    1/2N x 2N PU or 2N x 1/2N PU and performing intra prediction on the partitioned PU.
10 When SDIP is performed, a distance between a reference pixel for intra prediction and a
   prediction target pixel may be reduced as compared with in conventional intra prediction
   performed using a square-shaped PU.        Thus a residual value that is a differential value
   between an original pixel and the prediction target pixel (predicted pixel) decreases,
   resulting in an increase in encoding efficiency.
15          [120] Referring to FIG. 6, one CU may be partitioned into PUs with different
   sizes depending on features of a picture.    For example, a 32 x 32 CU may be partitioned
   into four 16 x 16 PUs 610, 620, 630 and 640.        A 16 x 16 PU 610 may be additionally
   partitioned into four 4 x 16 PUs 611, 612, 613 and 614, among which a 4 x 16 PU 611
   may be further partitioned into four 1 x 16 PUs 611-1, 611-2, 611-3 and 611-4.
20          [121] Likewise, a 16 x 16 PU 630 may be additionally partitioned into four 8 x 8
   PUs.    A 8 x 8 PU 631 may be further partitioned into four 2 x 8 PUs 631-1, 631-2, 631-3
   and 631-4.    Also, a 8 x 8 PU 632 may be further partitioned into four 4 x 4 PUs, among
   which a 4 x 4 PU 632-1 may be further partitioned into four 1 x 4 PUs.
            [122] As described above with reference to FIGS. 4 and 5, a scanning method is
25 determined based on a partition mode of a PU in SDIP, that is, a size of the PU in the
                                               - 27 -

   present embodiment.      That is, candidate scanning methods are obtained in view of
   partitioned shapes of PUs, among which a scanning method is determined based on RDO.
            [123] For instance, when the partition mode of the PU in SDIP is a 1/2N x 2N
   mode which has a vertically oriented partition shape such as the 4 x 16 PUs 611, 612, 613
 5 and 614, the 2 x 8 PUs 631-1, 631-2, 631-3 and 631-4 and the 1 x 4 PU, horizontal
   scanning and zigzag scanning (or upright diagonal scanning) may be derived as candidate
   scanning methods in view of particular component of texture or directionality (for
   example, vertical texture and transform coefficients distributed in the horizontal direction).
   Here, a scanning method having minimum RDO may be selected among the two candidate
10 scanning methods.
            [124] Alternatively, when the partition mode of the PU in SDIP is a 2N x 1/2N
   mode which has a horizontally oriented partition shape such as 16 x 4 PU, 8 x 2 PU and 4
   x 1 PU, vertical scanning and zigzag scanning (or upright diagonal scanning) may be
   derived as candidate scanning methods in view of particular component of texture or
15 directionality (for example, horizontal texture and transform coefficients distributed in the
   vertical direction).   Here, a scanning method having minimum RDO may be selected
   among the two candidate scanning methods.
            [125] Information on the determined scanning method may be encoded using a
   flag, for example, isZigZagScanFlag, and be transmitted to the decoding apparatus, as
20 described above in FIGS. 4 and 5.
            [126] FIG. 7 is a flowchart illustrating a video encoding method according to the
   present invention.    Each step of FIG. 7 may be performed by corresponding modules of
   the video encoding apparatus of FIG. 1.
            [127] Referring to FIG. 7, a CU of a current picture is input to the encoding
25 apparatus (S700).     When the input CU is      a CU of an inter prediction mode, the CU of
                                               - 28 -

   the inter prediction mode ("inter CU") may include a plurality of PUs of the inter
   prediction mode ("inter PU") and have one of two prediction modes PreMode, a skip
   mode ("MODESKIP") and an inter mode ("MODEINTER").
            [128] A CU in MODESKIP is not partitioned into smaller PUs any longer and
 5 allocated motion information on a PU with a partition mode PartMode of PART_2Nx2N.
            [129] A CU in MODEINTER may be partitioned into four types of PUs, in
   which     information    indicating  that    a    prediction     mode     is MODEINTER
   (PredMode==MODEINTER) and information indicating which is a partition mode
   among PART_2Nx2N, PART_2NxN, PARTNx2N and PARTNxN (i.e. information
10 such           as          PartMode==PART_2Nx2N,                   PartMode==PART_2NxN,
   PartMode==PARTNx2N,           or PartMode==PARTNxN)             may be transmitted to the
   decoding apparatus through a syntax in a CU level.
            [130] The encoding apparatus performs motion prediction for a current inter PU
   (S710).    When the CU is partitioned into a plurality of PUs, a PU to be currently encoded
15 ("current PU") is input.   The encoding apparatus may perform motion prediction for the
   current PU using previous frame, subsequent frame or previous and subsequent frames of
   a current frame.    Motion information on the current PU, such as a motion vector, a
   reference picture index and a prediction direction index, may be obtained through motion
   prediction.
20          [131] The encoding apparatus may derive a motion prediction value of the current
   PU in the inter prediction mode (S720).     The motion information on the current PU is not
   transmitted to the decoding apparatus as it is but differential values from predicated values
   obtained from temporally and spatially neighboring blocks are transmitted to the decoding
   apparatus so as to enhance compression efficiency.          Motion prediction method may
25 include a merge mode and an AMVP mode, which may be used to derive a motion
                                              - 29 -

   prediction value.
            [132] In the merge mode, merging candidates are obtained from motion
   information on blocks temporally and spatially neighboring to the current PU.     When a
   candidate having the same motion information as the current PU is present among the
 5 candidates, the encoding apparatus may transmit a flag MergeFlag indicating that the
   merge mode is used and an index of the candidate having the same motion information as
   the current PU to the decoding apparatus.     In detail, the encoding apparatus derives an
   available temporal motion vector predictor (MVP) value using a reference picture index
   refldxLX indicating a reference picture obtained in motion prediction and makes a
10 merging candidate list MergeCandList.        When a candidate having the same motion
   information as the current PU is present on the merging candidate list, the encoding
   apparatus sets MergeFlag to 1 and encodes an index MergeIdx of the candidate.
            [133] In the AMVP mode, the encoding apparatus derives AMVP candidates from
   motion information on blocks temporally and spatially neighboring to the current PU.
15 That is, the encoding apparatus derives a motion vector predictor value mvpLX of a luma
   component.     In detail, the encoding apparatus derives spatial motion vector candidates
   (MVPs) from neighboring PUs to the current PU.           The encoding apparatus derives a
   temporal motion vector candidate of a collocated block using a reference picture index
   RefldxLX obtained in motion prediction.       The encoding apparatus makes an MVP list
20 mvpListLX based on the spatial motion vector candidates and the temporal motion vector
   candidate.   When a plurality of motion vectors has the same value on the MVP list, the
   encoding apparatus removes motion vectors other than a motion vector having a highest
   priority from the MVP list.     Here, motion vectors may have priories in order of motion
   vectors (mvLXA) of left neighboring blocks to the current PU, motion vectors (mvLXB)
25 of upper neighboring blocks to the current PU and a motion vector (mvLXCol) of a
                                              - 30 -

   temporal collocated block, which are available.      A motion vector of a best predictor
   among the motion vector candidates on the MVP list is selected as a motion vector
   predictive value mvpLX.     The best predictor is a candidate block minimizing a rate
   distortion (RD) cost function, for example,     JMotSAD considering bit cost and sum of
 5 absolute difference (SAD).
            [134] The encoding apparatus encodes the motion information on the current PU
   (S730).    When the merge mode is used for motion prediction of the current PU, if a
   candidate having the same motion information as the current PU is present among
   merging candidates, the encoding apparatus indicates that the merge mode is applied to
10 the current PU, and encodes and transmits a flag MergeFlag indicating that the merge
   mode is used and an index MergeIdx of the candidate having the same motion
   information as the current PU to the decoding apparatus.
            [135] When the AMVP mode is used for motion prediction of the current PU, the
   encoding apparatus determines a candidate minimizing a cost function among AMVP
15 candidates by comparing motion vector information on the AMVP candidates with motion
   vector information on the current PU.         The encoding apparatus performs motion
   compensation using the candidate minimizing the cost function and a differential value
   between the motion information on candidate minimizing the cost function and the motion
   information on current PU, thereby obtaining a residual signal.     That is, the encoding
20 apparatus may entropy-encode a motion vector difference between a motion vector of the
   current PU and the motion vector of the best predictor.
            [136] The encoding apparatus obtains the residual signal by deriving a difference
   by pixels between a pixel value of the current block and a pixel value of the prediction
   block through motion compensation (S740) and transforms the residual signal (S750).
25          [137] The residual signal is encoded via transformation, in which a transcoding
                                             - 31 -

   kernel may be used for transformation.      A transcoding kernel may have a shape of 2x2,
   4x4, 8x8, 16x16, 32x32 or 64x64, among which a kernel to be used for transformation
   may be determined in advance.             Here, transform coefficients are generated by
   transformation and form a 2D block.       For example, transform coefficients C for an n x n
 5 block may be derived by Equation 1.
            [138] [Equation 1]
             C(n, n) = T(n, n) x B(n, n) x T(n, n)T
            [139] Here, C(n, n) is an n x n matrix of transform coefficients, T(n, n) is an n x n
   transformation kernel matrix, and B(n, n) is an n x n matrix of a residual block.
10          [140] The transform coefficients calculated by Equation 1 are quantized.
            [141] The encoding apparatus determines based on RDO which to transmit among
   the residual signal and the transform coefficients (S760).     When prediction is properly
   done, the residual signal may be transmitted as it is, without transcoding.         Here, the
   encoding apparatus may compare cost functions before/after transcoding and may select a
15 method having minimum costs.
            [142] The encoding apparatus may transmit a type of a signal to transmit (e.g. the
   residual signal or the transform coefficients), with respect to the current block and
   transmit the signal to the decoding apparatus.     For example, if transmitting the residual
   signal as it is without transcoding involves minimum cost, the encoding apparatus may
20 signal the residual signal with respect to the current block.   If transmitting the transform
   coefficients involves minimum cost, the encoding apparatus may signal the transform
   coefficients with respect to the current block.
            [143] The encoding apparatus scans the transform coefficients (S770).            The
   encoding apparatus changes quantized transform coefficients of a 2D block form into
                                               - 32 -

   transform coefficients of a ID vector form by scanning.            Here, one of horizontal
   scanning, vertical scanning and zigzag scanning (or upright diagonal scanning) may be
   selected based on a size of a PU, that is, a partition mode of the PU, to scan the transform
   coefficients.
 5          [144] In detail, candidate scanning modes (methods) may be derived based on
   partition shapes of PUs, among which a scanning mode is determined based on RDO.            If
   the partition mode of the PU has a vertically oriented partition shape, horizontal scanning
   and zigzag scanning (or upright diagonal scanning) are derived as candidate scanning
   modes.     If the partition mode of the PU has a horizontally oriented partition shape,
10 vertical scanning and zigzag scanning (or upright diagonal scanning) are derived as
   candidate scanning modes.        Then, a scanning mode having minimum RDO is selected
   among the candidate scanning modes.
            [145] Here, as described above in FIGS. 4 and 6, such scanning modes may be
   applied to the partition modes of the PU in inter prediction, for example, the N x 2N block,
15 2N x N block, 2N x 2N block, N x N block, 2N x nU block, 2N x nD block, nL x 2N
   block and nR x 2N block modes, and the partition modes of the PU in an intra prediction
   mode (e.g. short distance intra prediction: SDIP), for example, the 1/2N x 2N block, 2N x
    1/2N block, N x N block and 2N x 2N block modes.        As for this, descriptions thereof are
   omitted herein since already fully described before.
20          [146] The encoding apparatus may entropy-encode the information to be
   transmitted (S780).      That is, the encoding apparatus may entropy-encode the scanned
   transform coefficients and information on the prediction mode.      The encoded information
   may form a compressed bitstream and be stored or transmitted in a network abstraction
   layer (NAL).
25          [147] FIG. 8 is a flowchart illustrating a video decoding method according to the
                                               - 33 -

   present invention.    Each step of FIG. 8 may be performed by corresponding modules of
   the video decoding apparatus of FIG. 2.
             [148] Referring to FIG. 8, the decoding apparatus may entropy-decode a received
   bitstream (S800).     The decoding apparatus may identify a block type from a variable
 5 length coding (VLC) table to recognize a prediction mode of a current block.         Further,
   the decoding apparatus may identify information on whether transmitted information on
   the current block is a residual signal or transform coefficients.   Depending on a result, the
   decoding apparatus may obtain the residual signal or transform coefficients for the current
   block.
10           [149] The decoding apparatus may determine a scanning method (S810).        That is,
   the decoding apparatus determines a scanning method based on a partition mode of a PU
   using information signaled from the encoding apparatus.        The signaled information may
   be a flag indicating whether zigzag scanning is used (for example, isZigZagScanFlag).
             [150] Specifically, when the partition mode of the PU has a vertically oriented
15 partitioned shape or horizontally oriented partitioned shape, the decoding apparatus
   decodes a flag indicating whether zigzag scanning is used and determines a scanning
   method based on a value of the decoded flag.       When the partition mode of the PU has the
   vertically oriented partition shape, the decoding apparatus selects one of zigzag scanning
   and horizontal scanning based on the value of the decoded flag.          When the partition
20 mode of the PU has the horizontally oriented partition shape, the decoding apparatus
   selects one of zigzag scanning and vertical scanning based on the value of the decoded
   flag.   For instance, zigzag scanning is used when isZigZagScanFlag is 1, while horizontal
   scanning (for the partition mode having the vertically oriented partition shape)/vertical
   scanning (for the partition mode having the horizontally oriented partition shape) may be
25 used when isZigZagScanFlag is 0.
                                                - 34 -

            [151] Here, as described above in FIGS. 4 and 6, such scanning methods may be
   applied to the partition modes of the PU in inter prediction, for example, the N x 2N block,
   2N x N block, 2N x 2N block, N x N block, 2N x nU block, 2N x nD block, nL x 2N
   block and nR x 2N block modes, and the partition modes of the PU in intra prediction (e.g.
 5 short distance intra prediction: SDIP), for example, the 1/2N x 2N block, 2N x 1/2N block,
   N x N block and 2N x 2N block modes. As for this, descriptions thereof are omitted herein
   since already fully described before.
            [152] Meanwhile, zigzag scanning may be used for a partition mode having a
   square shape, such as 2N x 2N block and N x N block modes, or for a right partition of nL
10 x 2N block mode, a left partition of nR x 2N block mode, a lower partition of 2N x nU
   block mode or an upper partition of 2N x nD block mode as larger partitioned portions in
   AMP.
            [153] The decoding apparatus may inverse-scan the entropy decoded residual
   signal or transform coefficients (S820).    The decoding apparatus may generate a residual
15 block by inverse-scanning for the case of the residual signal and may generate a 2D
   transform block by inverse-scanning for the case of the transform coefficients.    When the
   transform block is generated, the decoding apparatus may dequantize and inverse
   transform the transform block, thereby obtaining a residual block.            A process of
   obtaining a residual block by inverse-transforming a transform block is expressed by
20 Equation 2.
            [154] [Equation 2]
             Bn, 0) = T(n, n ) x C(n, n) x T(n, n1 )T
            [155] Here, B(n, n) is an n x n matrix of a residual block, T(n, n) is an n x n
   transformation kernel matrix, and C(n, n) is an n x n matrix of transform coefficients.
                                               - 35 -

            [156] The decoding apparatus may perform inter prediction (S830).           The
   decoding apparatus may decode information on the prediction mode and perform inter
   prediction according to the prediction mode.
            [157] For example, when the prediction mode PredMode is the merge mode (for
 5 example, PredMode== MODESKIP && MergeFlag==1), the decoding apparatus may
   derive a motion vector mvLX of a luma component and a reference picture index
   refldxLX for the merge mode.      To this end, the decoding apparatus may derive merging
   candidates from partitions of PUs (i.e. prediction blocks) spatially neighboring to the
   current PU.    The decoding apparatus may derive the reference picture index refldxLX so
10 as to obtain a temporal merging candidate for the current PU.     The decoding apparatus
   may derive an available temporal motion vector predictor value (MVP) using the derived
   reference picture index.    When a number NumMergeCand of candidates on a merging
   candidate list MergeCandList made based on the spatial merging candidates and the
   temporal merging candidate is 1, the decoding apparatus sets a merging candidate index
15 (MergeIdx) tol.      Otherwise, the decoding apparatus may set the merging candidate
   index to a received merge index.       The decoding apparatus derives a motion vector
   (mvLX) of a merging candidate indicated by the received merge index and the reference
   picture index (refldxLX).    The decoding apparatus may use the derived motion vector
   and the derived reference picture index for motion compensation.
20          [158] When the prediction mode PredMode is the AMVP mode, the decoding
   apparatus may derive a reference picture index (refldxLX) for the current PU and may
   derive a motion vector predictor value (mvpLX) of a luma component using the reference
   picture index.    In detail, the decoding apparatus may derive spatial motion vector
   candidates (MVPs) from neighboring PUs to the current PU and may derive a temporal
25 motion vector candidate (MVP) of a collocated block indicated by the reference picture
                                              - 36 -

   index.   The decoding apparatus may generate an MVP list mvpListLX based on the
   derived spatial motion vector candidates and the derived temporal motion vector candidate.
   When a plurality of motion vectors has the same value on the MVP list, the decoding
   apparatus may remove motion vectors other than a motion vector having a highest priority
 5 from the MVP list.     Here, as described above, motion vectors have priories in order of
   motion vector (mvLXA) of left neighboring block to the current PU, motion vector
   (mvLXB) of upper neighboring block to the current PU and a motion vector (mvLXCol)
   of   a   temporal    collocated  block,   which   are   available.   When     a   number
   NumMVPCand(LX) of MVP candidates on the MVP list is 1, the decoding apparatus may
10 set an MPV candidate index mvpldx to 0.      When the number of MVP candidates is 2 or
   more, the decoding apparatus may set the MPV candidate index mvpldx equal to a
   received index value.    The decoding apparatus may determine a motion vector indicated
   by mvpldx among the MVP candidates on the MVP list mvpListLX as a motion vector
   predictor value mvpLX.       The decoding apparatus may derive a motion vector mvLX
15 using a motion vector predictor value mvpLX and Equation 3.
            [159] [Equation 3]
             mvLX[0] = mvdLX[0] + mvpLX[0]
             mvLX[l] = mvdLX[l] + mvpLX[l]
            [160] Here, mvLX[0], mvdLX[0] and mvpLX[O] are x components of an LX
   motion vector information (i.e. x components of mvLX, mvdLX and mvpLX), and
20 mvLX[l], mvdLX[l] and mvpLX[1] are y components of the LX motion vector
   information (i.e. y components of mvLX, mvdLX and mvpLX).
            [161] The decoding apparatus may derive a reconstructed signal (S840).       For
   instance, the decoding apparatus may add the residual signal to a signal of a previous
                                             - 37 -

   frame (i.e. predicted signal) to generate the reconstructed signal.   The decoding apparatus
   may add a prediction signal of the previous frame obtained by motion compensation using
   a derived motion vector and the decoded residual signal for the current PU, thereby
   generating the reconstructed signal.
 5          [162] Although the methods have been described with a series of stages or blocks
   based on the flowcharts in the aforementioned embodiments, the present invention is not
   limited to the foregoing sequence of the stages.       Some stages may be carried out in
   different order from described above or at the same time.      Also, it will be understood by
   those skilled in the art that the stages illustrated in the flowcharts are not exclusive,
10 additional stages may be included in the flowchart, or one or more stages may be deleted
   from the flowcharts without affecting the scope of the present invention.
            [163] While a few exemplary embodiments have been shown and described with
   reference to the accompanying drawings, it will be apparent to those skilled in the art that
   various modifications and variations can be made from the foregoing descriptions without
15 departing from the essence of the present invention.       The exemplary embodiments are
   provided not to restrict the concept of the present invention but to illustrate the present
   invention and do not limit the scope of the present invention.     The scope of the invention
   is defined by the appended claims, and all differences within the scope will be construed
   as being included within the appended claims of the present invention.
20
                                               - 38 -

     [CLAIMS]
           1. A method of decoding a video signal having a current block to be decoded with
   a decoding apparatus, comprising:
           obtaining index information relating to the current block by entropy-decoding an
 5 input bitstream, the index information being signaled, through the input bitstream, to
   select a collocated picture from a plurality of previously decoded pictures, the collocated
   picture being representative of a picture including a collocated block which is used to
   derive a temporal motion vector candidate of the current block,
           obtaining a spatial motion vector candidate from a spatial neighboring block of the
10 current block;
           selecting, based on the index information, the collocated picture relating to the
   temporal motion vector candidate of the current block;
           obtaining the temporal motion vector candidate from the collocated block included
   in the selected collocated picture of the current block, the collocated block being included
15 in a collocated picture, the collocated picture being selected based on a reference index
   which is extracted from the video signal;
           generating a motion vector candidate list including the spatial motion vector
   candidate and the temporal motion vector candidate;
           deriving a motion vector predictor based on the motion vector candidate list and a
20 candidate index of the current block, the candidate index specifying one of motion vector
   candidates included in the motion vector candidate list;
           deriving a motion vector of the current block using the motion vector predictor and
   a motion vector difference;
           obtaining prediction samples of the current block using the motion vector; and
25         decoding the current block by using the prediction samples.
                                              - 39 -

           2. The method of claim 1, wherein the collocated picture has a temporal order
   different from a current picture including the current block.
 5         3. The method of claim 2, wherein the collocated block is representative of a block
   corresponding to a position as the current block.
           4. The method of claim 1, wherein the spatial neighboring block includes at least
   one of a left neighboring block and a top neighboring block.
10
           5. The method of claim 4, wherein the motion vector candidates in the motion
   vector candidate list are arranged in priority order.
           6. The method of claim 5, wherein the motion vector candidates are arranged in
15 sequence of the spatial motion vector candidate and the temporal motion vector candidate.
                                               - 40 -

         <removed-apn> <removed-date>
  ),*


         <removed-apn> <removed-date>
),*
       
     

<removed-date>
                                         ),*
<removed-apn>
                                                              
                                                                 
                    
                                                              
                                                                             
                                                              
                                                                       
                                       
                                                                 
                 +RUL]RQWDOVFDQQLQJ                     9HUWLFDOVFDQQLQJ
                                                              
                                                                 
                      
                                                                    
                                                       
                                                                          
                                          
                                                           
                                                           
                     =LJ]DJVFDQQLQJ              8SULJKWGLDJRQDOVFDQQLQJ

                   <removed-apn> <removed-date>
          ),*
1[1 1[1    1[1 1[1
          

                                                         <removed-apn>   <removed-date>
                                     ),*
                                                               
                                                   
                                     
                                                     
                 
                                     
                                                     
     Q/[1       Q5[1       1[Q8        1[Q'
                                                    

<removed-date>
                                    ),*
<removed-apn>                                [                    [
                                                  
                                                                    
                                                       
                                                               
                                                                  
                                                                  
                                                      
                                                                  [
                                                                 
              [
                                 [                 [
                    

<removed-apn>   <removed-date>
                  ),*

<removed-apn>   <removed-date>
                   ),*

