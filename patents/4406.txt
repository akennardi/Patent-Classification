                         ABSTRACT OF THE DISCLOSURE
        Disclosed are techniques that provide a "best" picture taken within a few
seconds of the moment when a capture command is received (e.g., when the "shutter"
button is pressed). In some situations, several still images are automatically (that is,
without the user's input) captured. These images are compared to find a "best" image
that is presented to the photographer for consideration. Video is also captured
automatically and analyzed to see if there is an action scene or other motion content
around the time of the capture command. If the analysis reveals anything interesting,
then the video clip is presented to the photographer. The video clip may be cropped to
match the still-capture scene and to remove transitory parts. Higher-precision horizon
detection may be provided based on motion analysis and on pixel-data analysis.

                                              1
                             ENHANCED IMAGE CAPTURE
                  CROSS-REFERENCE TO RELATED APPLICATIONS
[0001]        The present application claims priority to U.S.          Provisional Patent
Application 62/001,327, filed on May 21, 2014, which is incorporated herein by
reference in its entirety.
[0002]        The present application is related to U.S. Patent Applications (Motorola
Docket Numbers CS42367, CS42702, CS42703, CS42704, and CS42741), filed on an
even date herewith.
                                    TECHNICAL FIELD
[0003]        The present disclosure is related generally to still-image and video capture
and, more particularly, to digital image processing.
                                      BACKGROUND
[0004]        On average, people discard a large number of the pictures they take as
unsatisfactory. In many cases, this is because the main subject is blinking, moving
(i.e., is too blurry), or not smiling at the moment of image capture. In other cases, the
photographer is inadvertently moving the image-capture device at the capture moment
(e.g., due to an unsteady hand or to an involuntary rotation of the device). Some
pictures are discarded because the image-capture settings are inappropriate (e.g., the
settings do not accommodate a low-light situation).
       BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS
[0005]        While the appended claims set forth the features of the present techniques
with particularity, these techniques, together with their objects and advantages, may
be best understood from the following detailed description taken in conjunction with
the accompanying drawings of which:
[0006]        Figure 1A is an overview of a representative environment in which the
              present techniques may be practiced;
[0007]        Figure 1B is an overview of a representative network that supports certain
              of the present techniques;

                                               2
[0008]       Figure 2 is a flowchart of a representative method for selecting and
             presenting a "best" captured still image;
[0009]       Figure 3 is a flowchart of a representative method for capturing an
             "interesting" video;
[0010]       Figure 4 is a flowchart of a representative method for selecting a "best"
             captured still image and for capturing an "interesting" video;
[0011]       Figure 5 is a flowchart of a representative method for a remote server that
             assists an image-capture devices;
[0012]       Figure 6 is a flowchart of representative methods for notifying a user that a
             "better" still image or an "interesting" video is available;
[0013]       Figure 7 is a flowchart of a representative method for detecting a horizon
             in a captured image and then using the detected horizon; and
[0014]       Figure 8 is a schematic showing various components of a representative
             image-capture device or server.
                                DETAILED DESCRIPTION
[0015]       Turning to the drawings, wherein like reference numerals refer to like
elements, techniques of the present disclosure are illustrated as being implemented in
a suitable environment. The following description is based on embodiments of the
claims and should not be taken as limiting the claims with regard to alternative
embodiments that are not explicitly described herein.
[0016]       The inventors believe that photographers would like, in addition to getting
the best possible photographs, more than one picture to capture the moment, and, in
some cases, a few seconds of video associated with a still picture. This later should be
accomplished without the photographer having to spend the time to switch between
still-capture mode and video-capture mode.
[0017]       Aspects of the presently disclosed techniques provide a "best" picture
taken within a few seconds of the moment when a capture command is received (e.g.,

                                            3
when the "shutter" button is pressed). Also, several seconds of video are captured
around the same time and are made available to the photographer. More specifically,
in some embodiments, several still images are automatically (that is, without the
user's input) captured. These images are compared to find a "best" image that is
presented to the photographer for consideration. Video is also captured automatically
and analyzed to see if there is an action scene or other motion content around the time
of the capture command. If the analysis reveals anything interesting, then the video
clip is presented to the photographer. The video clip may be cropped to match the
still-capture scene and to remove transitory parts. In further embodiments, better low
light images are provided by enhancing exposure control. Higher-precision horizon
detection may be provided based on motion analysis.
[0018]       For a more detailed analysis, turn first to Figure 1A. In this example
environment 100, a photographer 102 (also sometimes called the "user" in this
discussion) wields his camera 104 to take a still image of the "scene" 106. In this
example, the photographer 102 wants to take a snapshot that captures his friend 108.
[0019]       The view that the photographer 102 actually sees is depicted as 110,
expanded in the bottom half of Figure 1A. Specifically, when the photographer 102
pushes a "capture" button (also called the "shutter" for historical reasons), the camera
104 captures an image and displays that captured image in the viewfinder display 112.
So far, this should be very familiar to anyone who has ever taken a picture with a
smartphone or with a camera that has a large viewfinder display 112. In the example
of Figure 1A, however, the camera 104 also displays a "notification icon" 114 to the
photographer 102. While the detailed functioning supporting this icon 114 is
discussed at length below, in short, this icon 114 tells the photographer 102 that the
camera 104 believes that it has either captured a "better" still image than the one
displayed in the viewfinder display 112 or that it has captured a video that may be of
interest to the photographer 102.
[0020]       Figure lB introduces a network 116 (e.g., the Internet) and a remote server
118. The discussion below shows how these can be used to expand upon the sample
situation of Figure 1A. Figure lB also visually makes the point that the "camera" 104

                                            4
need not actually be a dedicated camera: It could be any image-capture device
including a video camera, a tablet computer, smartphone, and the like. For clarity's
sake, the present discussion continues to call the image-capture device 104 a
  cameraa"
[0021]      Figure 2 presents methods for specific techniques that enhance still-image
capture. In step 200, the camera 104 captures a number of still images. Consider, for
example, the photographer 102 putting the camera 104 into "viewfinder" mode. In
this mode, the camera's viewfinder 112 displays the image "seen" by the camera 104.
The photographer 102 may explicitly command the camera 104 to enter this mode, or
the camera 104 can automatically enter this mode when it determines that this mode is
desired (e.g., by monitoring the camera's current position and observing the behavior
of the photographer 102).
[0022]      In any case, the camera 104 automatically (that is, while still in viewfinder
mode and not in response to an explicit command from the photographer 102)
captures a number of still images, e.g., five per second over a period of a couple of
seconds. These captured still images are stored by the camera 104.
[0023]      In taking so many images, memory storage often becomes an issue. In
some embodiments, the images are stored in a circular buffer (optional step 202)
holding, say, ten seconds of still images. Because the capacity of the circular buffer is
finite, the buffer may be continuously refreshed with the latest image replacing the
earliest one in the buffer. Thus, the buffer stores a number of captured still images
ranging in time from the newest image back to the oldest, the number of images in the
buffer depending upon the size of the buffer. In some embodiments, the selection
process (see the discussion of step 208 below) is performed continuously on the set of
images contained in the circular buffer. Images that are not very good (as judged by
the techniques discussed below) are discarded, further freeing up space in the circular
buffer and leaving only the "best" images captured over the past, say, three seconds.
Even in this case, the metadata associated with discarded images are kept for
evaluation.

                                            5
[0024]      Note that the capture rate of images in step 200 may be configurable by the
photographer 102 or may depend upon an analysis of the photographer's previous
behavior or even upon an analysis of the captured images themselves. If, for example,
a comparison of one image to another indicates a significant amount of movement in
the captured scene, then maybe the camera 104 is focused on a sporting event, and it
should increase its capture rate. The capture rate could also depend upon the resources
available to the camera 104. Thus, if the camera's battery is running low, then it may
reduce the capture rate to conserve energy. In extreme cases, the technique of
automatic capture can be turned off when resources are scarce.
[0025]      At step 204 (generally while the camera 104 continues to automatically
capture still images), the photographer 102 gives a capture command to the camera
104. As mentioned above, this can result from the photographer 102 pressing a shutter
button on the camera 104. (In general, the capture command can be a command to
capture one still image or a command to capture a video.)
[0026]      (For purposes of the present discussion, when the camera 104 receives the
capture command, it exits the viewfinder mode temporarily and enters the "capture"
mode. Once the requested still image (or video as discussed below) is captured, the
camera 104 generally re-enters viewfinder mode and continues to automatically
capture images per step 200.)
[0027]      Unlike in the technique of step 200, traditional cameras stay in the
viewfinder mode without capturing images until they receive a capture command.
They then capture the current image and store it. A camera 104 acting according to
the present techniques, however, is already capturing and storing images (steps 200
and 202) even while it is still in the viewfinder mode. One way of thinking about the
present techniques is to consider the capture command of step 204 not to be a
command at all but rather to be an indication given by the photographer 102 to the
camera 104 that the photographer 102 is interested in something that he is seeing in
the viewfinder display 112. The camera 104 then acts accordingly (that is, it acts
according to the remainder of the flowchart of Figure 2).

                                             6
[0028]      Step 206 is discussed below in conjunction with the discussion of step 214.
[0029]      In step 208, the camera 104 reviews the images it has captured (which may
include images captured shortly before or shortly after the capture command is
received) and selects a "best" one (or a "best" several in some embodiments). (In
some embodiments, this selection process is performed on partially processed, or
"raw," images.) Many different factors can be reviewed during this analysis. As
mentioned above, the capture command can be considered to be an indication that the
photographer 102 is interested in what he sees. Thus, a very short time interval
between the capture command and the time that a particular image was captured
means that that particular image is likely to be of something that the photographer 102
wants to record, and, thus, this time interval is a factor in determining which image is
"best."
[0030]      Various embodiments use various sets of information in deciding which of
the captured images is "best." In addition to temporal proximity to the photographer's
capture    command,     some    embodiments      use   motion-sensor    data   (from   an
accelerometer, gyroscope, orientation, or GPS receiver on the camera 104) (e.g., was
the camera 104 moving when this image was captured?), face-detection information
(face detection, position, smile and blink detection) (i.e., easy-to-detect faces often
make for good snapshots), pixel-frame statistics (e.g., statistics of luminance: gradient
mean, image to image difference), activity detection, data from other sensors on the
camera 104, and scene analysis. Further information, sometimes available, can
include a stated preference of the photographer 102, past behavior of the photographer
102 (e.g., this photographer 102 tends to keep pictures with prominent facial images),
and a privacy setting (e.g., do not keep pictures with a prominent face of a person who
is not in a list of contacts for the camera 104). Also often available are camera 104
metadata and camera-status information. All such data can be produced in the camera
104 and stored as metadata associated with the captured images.
[0031]      These metadata may also include reduced resolution versions of the
captured images which can be used for motion detection within the captured scene.
Motion detection provides information which is used for "best" picture selection (and

                                             7
analysis of captured video, see discussion below), as well as other features which
improve the image-capture experience.
[0032]      The statistics and motion-detection results can also be used by an exposure
procedure to improve captured-image quality in low light by, for example, changing
exposure parameters and flash lighting. When there is motion in low light and strobe
lighting is available from the camera 104, the strobe may be controlled such that
multiple images can be captured with correct exposures and then analyzed to select
the best exposure.
[0033]      However the "best" captured image is selected, that best image is
presented to the photographer 102 is step 210. There are several possible ways of
doing this. Many embodiments are intended to be completely "transparent" from the
photographer's perspective, that is, the photographer 102 simply "snaps" the shutter
and is presented with the selected best image, whether or not that is actually the image
captured at the time of the shutter command.
[0034]      Consider again the situation of Figure 1A. When the photographer 102
presses the shutter button (step 204), the viewfinder display 112 is as shown in Figure
1A. Clearly, the photographer 102 wants a picture of the face of his friend 108. The
system can review the captured images from, say a second before to a second after the
capture command is received, analyze them, and then select the best one. Here, that
would be an image that is in focus, in which the friend 108 is looking at the camera
104, has her eyes open, etc. That best image is presented to the photographer 102
when he presses the shutter button even if the image captured at the exact time of the
shutter press is not as good.
[0035]      A slightly more complicated user interface presents the photographer 102
with the image captured when the shutter command was received (as is traditional)
and then, if that image is not the best available, presents the photographer 102 with an
indication (114 in Figure 1A) that a "better" image is available for the photographer's
consideration. Again considering the situation of Figure 1A, maybe his friend 108
blinks at the time of the capture command. That 'blinking" image is presented to the

                                            8
photographer 102, but the indication 114 is lit to show that other, possibly better,
images are available for the photographer's review.
[0036]      Other variations on the user interface are possible. The choice of which to
use in a given situation can be based on settings made by the photographer 102, on an
analysis of the photographer's past behavior (e.g., is he a "snapshot tourist," or does
he act more like an experienced photographer?), and on analysis of the captured
scene.
[0037]      In optional step 212, the selected image is further processed, if necessary,
and copied to a more permanent storage area.
[0038]      In some embodiments, the metadata associated with the captured images
(possibly including what the photographer 102 eventually does with the images) are
sent (step 214) to a remote server device (118 of Figure 1B). The work of the remote
server 118 is discussed in greater detail below with reference to Figure 5, but briefly,
the remote server 118 analyzes the information, potentially from multiple image
capture devices 104, looking for trends and for "best practices." It then encapsulates
what it has learned and sends recommendations to cameras 104 (step 206). The
cameras 104 are free to use these recommendations when they select images in step
208.
[0039]      Figure 3 presents other methods for enhancing image-capture, this time for
video images. The method of Figure 3 can be performed separately from, or in
conjunction with, the methods of Figure 2.
[0040]      In step 300, the camera 104 captures video while the camera 104 is in
viewfinder mode (that is, as described above, while the camera 104 has not received
an explicit command to capture video). As with still-image capture, parameters of the
video capture can be altered to reflect the resources (e.g., battery, memory storage)
available on the camera 104.
[0041]      In some embodiments, the captured video is, at this point, simply a time
sequence of "raw," unprocessed images. (These raw images can be further processed

                                            9
as necessary later: See the discussion of step 312 below.) The storage issues
mentioned above for still images are exacerbated for video, so, again, a circular buffer
is recommended for storing the video as it is captured (step 302). The latest video
images (also called "frames") replace the oldest ones so that at any time, the circular
buffer has, for example, the last twenty seconds of captured video.
[0042]      Optionally, a capture command is received in step 304. As discussed
above, this is not treated as an actual command, but rather as an indication given by
the photographer 102 to the camera 104 that the photographer 102 is interested in
something that he is seeing in the viewfinder display 112.
[0043]      Whether a capture command has been received or not, the captured video
is continuously analyzed (step 308) to see if it is "interesting."            While the
photographer 102 can indicate his interest by pressing the shutter, other information
can be used in addition to (or instead of) that, such as activity detection, intra-frame
and inter-frame motion, and face detection. For example, a sudden surge of activity
combined with a clearly recognizable face may indicate an interesting situation. As
with still-image capture, photographer 102 preferences, past behavior, and privacy
settings can also be used in a machine-learning sense to know what this photographer
102 finds interesting.
[0044]      If a segment (also called a "clip") of captured video has been found to be
potentially interesting (e.g., if an "interest score" for a video clip is above a set
threshold), then the photographer 102 is notified of this in step 308. The photographer
102 may then review the indicated video clip to see if he too finds it to be of interest.
If so, then the video clip is further processed as necessary (e.g., by applying video
compression techniques) and copied into longer-term storage (step 312).
[0045]      As a refinement, the limits of the interesting video clip can be determined
using the same analysis techniques described above along with applying motion
sensor data. For example, the starting point of the clip can be set shortly before
something interesting begins to occur.

                                           10
[0046]      Also as with the still-image embodiments, metadata can be sent to the
remote server 118 (step 314). Recommendations and refined operational parameters,
based on analysis performed by the remote server 118, can be received (step 306) and
used in the analysis of step 308.
[0047]      Note that from the description above, in some embodiments and in some
situations, the camera 104 captures and presents video without ever leaving the
viewfinder mode. That is, the camera 104 views the scene, delimits video clips of
interest, and notifies the photographer 102 of these video clips without ever receiving
any explicit command to do so. In other embodiments, these video-capture and
analysis techniques can be explicitly invoked or disabled by the photographer 102.
[0048]      As mentioned above in the introduction to the discussion of Figure 3, the
still-image capture-enhancement techniques of Figure 2 can be combined with the
video-image capture-enhancement techniques of Figure 3. Figure 4 presents such a
combination with some interesting refinements.
[0049]      Consider once again the scenario of Figure 1A. The camera 104 is in
viewfinder mode, capturing both still images (step 400, as per step 200 of Figure 2)
and video (step 408, as in step 300 of Figure 3). In the proper circumstances, the
system presents both the best captured still image (step 406) and interesting video
(step 410) for the photographer's consideration (possibly using the time of the capture
command of step 402 to select and analyze the captured images and frames).
[0050]      Even though still images and video frames can be captured at the same
time, the refinement of Figure 4 applies image-stabilization techniques to the captured
video but not to the captured still images (step 412). This provides both better video
and better stills than would any known "compromise" system that does the same
processing for both stills and video.
[0051]      In another refinement, the selection of the best still image (step 406) can
depend, in part, on the analysis of the video (step 410) and vice versa. Consider a
high-motion sports scene. The most important scenes may be best determined from
analyzing the video because that will best show the action. From this, the time of the

                                              11
most interesting moment is determined. That determination may alter the selection
process of the best still image. Thus, a still image taken at the moment when a player
kicks the winning goal may be selected as the best image, even though other factors
may have to be compromised (e.g. the player's face is not clearly visible in that
image). Going in the other direction, a video clip may be determined to be interesting
simply because it contains an excellent view of a person's face even though that
person is not doing anything extraordinary during the video.
[0052]       Specifically, all of the metadata used in still-image selection can be used in
combination with all of the metadata used in video analysis and delimitation. The
combined metadata set can then be used to both select the best still image and to
determine whether or not a video clip is interesting.
[0053]       The methods of Figure 4 can also include refinements in the use of the
remote server 118 (steps 404 and 414). These refinements are discussed below in
reference to Figure 5.
[0054]       Methods of operation of the remote server 118 are illustrated in Figure 5.
As discussed above, the server 118 receives metadata associated with still-image
selection (step 500; see also step 214 of Figure 2 and step 414 of Figure 4). The same
server 118 may also receive metadata associated with analyzing videos to see if they
are interesting (step 504; see also step 314 of Figure 3 and step 414 of Figure 4). The
server 118 can analyze these two data sets separately (step 508) and provide still
image selection recommendations (step 510) and video-analysis recommendations
(step 510) to various image-capture devices 104.
[0055]       In some embodiments, however, the remote server 118 can do more. First,
in addition to analyzing metadata, it can further analyze the data themselves (that is,
the actual captured still images and video) if that content is made available to it by the
image-capture devices 104 (steps 502 and 506). With the metadata and the captured
content, the server 118 can perform the same kind of selection and analysis performed
locally by the image-capture devices 104 themselves (see step 208 of Figure 2; steps
308 and 310 of Figure 3; and steps 406 and 410 of Figure 4). Rather than simply

                                             12
providing a means for second-guessing the local devices 104, the server 118 can
compare its own selections and interest scores against those locally generated and thus
refine its own techniques to better match those in the general population of image
capture devices 104.
[0056]       Further, the image-capture device 104 can tell the remote server 118 just
what the photographer 102 did with the selected still images and the video clips
thought to be interesting (steps 502 and 506). Again, the server 118 can use this to
further improve its recommendation models. If, for example, photographers 102 very
often discard those still images selected as best by the techniques described above,
then it is clear that those techniques may need to be improved. The server 118 may be
able to compare an image actually kept by the photographer 102 against the image
selected by the system and, by analyzing over a large population set, learn better how
to select the "best" image.
[0057]       Going still further, the remote server 118 can analyze the still-image
selection    metadata    (and, if available,    the still images themselves    and the
photographer's ultimate disposition of the still images) together with the video
analysis metadata (and, if available, the video clips themselves and the photographer's
ultimate disposition of the captured video). This is similar to the cross-pollination
concept discussed above with respect to Figure 4: That is, by combining the analysis
of still images and video, the server 118 can further improve its recommendations for
both selecting still images and for analyzing video clips. The particular methodologies
usable here are well known from the arts of pattern analysis and machine learning.
[0058]       In sum, if the remote server 118 is given access to information about the
selections and analyses of multiple image-capture devices 104, then from working
with that information, the server 118 can provide better recommendations, either
generically or tailored to particular photographers 102 and situations.
[0059]       Figure 6 presents methods for a user interface applicable to the presently
discussed techniques. Much of the user-interface functionality has already been
discussed above, so only a few points are discussed in any detail here.

                                             13
[0060]      In step 600, the camera 104 optionally enters the viewfinder mode wherein
the camera 104 displays what it sees in the viewfinder display 112. As mentioned
above with reference to Figure 2, the photographer 102 may explicitly command the
camera 104 to enter this mode, or the camera 104 can automatically enter this mode
when it determines that this mode is desired.
[0061]      In a first embodiment of step 602, the photographer 102 presses the shutter
button (that is, submits an image-capture command to the camera 104), the camera
104 momentarily enters the image-capture mode, displays a captured image in the
viewfinder display 112, and then re-enters viewfinder mode. In a second embodiment,
the photographer puts the camera 104 into another mode (e.g., a "gallery" mode)
where it displays already captured images, including images automatically captured.
[0062]      As discussed above, the displayed image can either be one captured
directly in response to an image-capture command or could be a "better" image as
selected by the techniques discussed above. If there is a captured image that is better
than the one displayed, then the photographer 102 is notified of this (step 604). The
notification can be visual (e.g., by the icon 114 of Figure 1A), aural, or even haptic. In
some cases, the notification is a small version of the better image itself. If the
photographer 102 clicks on the small version, then the full image is presented in the
viewfinder display 112 for his consideration. While the camera 104 is in gallery
mode, the photographer 102 can be notified of which images are "better" by
highlighting them in some way, for example by surrounding them with a distinctive
border or showing them first.
[0063]      Meanwhile, a different user notification can be posted if the techniques
above capture a video clip deemed to be interesting. Again, several types of
notification are possible, including a small still from the video (or even a presentation
of the video itself).
[0064]      Other user interfaces are possible. While the techniques described above
for selecting a still image and for analyzing a video clip are quite sophisticated, they
allow for a very simple user interface, in some cases an interface completely

                                            14
transparent to the photographer 102 (e.g., just show the best captured still image when
the photographer 102 presses the shutter button). More sophisticated user interfaces
are appropriate for more sophisticated photographers 102.
[0065]      Figure 7 presents a refinement that can be used with any of the techniques
described above. A first image (a still or a frame of a video) is captured in step 700.
Optionally, additional images are captured in step 702.
[0066]      In step 704, the first image is analyzed (e.g., looking for horizontal or
vertical lines). Also, motion-sensor data from the camera 104 are analyzed to try to
determine the horizon in the first image.
[0067]      Once the horizon has been detected, it can be used as input when selecting
other images captured close in time to the first image. For example, the detected
horizon can tell how level the camera 104 was held when an image was captured, and
that can be a factor in determining whether that image is better than another. Also, the
detected horizon can be used when post-processing images to rotate them into level or
to otherwise adjust them for involuntary rotation.
[0068]      Figure 8 shows the major components of a representative camera 104 or
server 118. The camera 104 could be, for example, a smartphone, tablet, personal
computer, electronic book, or dedicated camera. The server 118 could be a personal
computer, a compute server, or a coordinated group of compute servers.
[0069]      The central processing unit ("CPU") 800 of the camera 104 or server 118
includes one or more processors (i.e., any of microprocessors, controllers, and the
like) or a processor and memory system which processes computer-executable
instructions to control the operation of the device 104, 118. In particular, the CPU 800
supports aspects of the present disclosure as illustrated in Figures 1 through 7,
discussed above. The device 104, 118 can be implemented with a combination of
software, hardware, firmware, and fixed-logic circuitry implemented in connection
with processing and control circuits, generally identified at 802. Although not shown,
the device 104, 118 can include a system bus or data-transfer system that couples the
various components within the device 104, 118. A system bus can include any

                                             15
combination of different bus structures, such as a memory bus or memory controller,
a peripheral bus, a universal serial bus, and a processor or local bus that utilizes any of
a variety of bus architectures.
[0070]      The camera 104 or server 118 also includes one or more memory devices
804 that enable data storage (including the circular buffers described in reference to
Figures 2 through 4), examples of which include random-access memory, non-volatile
memory (e.g., read-only memory, flash memory, erasable programmable read-only
memory, and electrically erasable programmable read-only memory), and a disk
storage device. A disk storage device may be implemented as any type of magnetic or
optical storage device, such as a hard disk drive, a recordable or rewriteable disc, any
type of a digital versatile disc, and the like. The device 104, 118 may also include a
mass-storage media device.
[0071]      The memory system 804 provides data-storage mechanisms to store device
data 812, other types of information and data, and various device applications 810. An
operating system 806 can be maintained as software instructions within the memory
804 and executed by the CPU 800. The device applications 810 may also include a
device manager, such as any form of a control application or software application.
The utilities 808 may include a signal-processing and control module, code that is
native to a particular component of the camera 104 or server 118, a hardware
abstraction layer for a particular component, and so on.
[0072]      The camera 104 or server 118 can also include an audio-processing system
814 that processes audio data and controls an audio system 816 (which may include,
for example,     speakers). A visual-processing       system 818 processes graphics
commands and visual data and controls a display system 820 that can include, for
example, a display screen 112. The audio system 816 and the display system 820 may
include any devices that process, display, or otherwise render audio, video, display, or
image data. Display data and audio signals can be communicated to an audio
component or to a display component via a radio-frequency link, S-video link, High
Definition Multimedia Interface, composite-video link, component-video link, Digital
Video Interface, analog audio connection, or other similar communication link,

                                           16
represented by the media-data ports 822. In some implementations, the audio system
816 and the display system 820 are components external to the device 104, 118.
Alternatively (e.g., in a cellular telephone), these systems 816, 820 are integrated
components of the device 104, 118.
[0073]      The camera 104 or server 118 can include a communications interface
which includes communication transceivers 824 that enable wired or wireless
communication. Example transceivers 824 include Wireless Personal Area Network
radios compliant with various Institute of Electrical and Electronics Engineers
("IEEE") 802.15 standards, Wireless Local Area Network radios compliant with any
of the various IEEE 802.11 standards, Wireless Wide Area Network cellular radios
compliant with 3rd Generation Partnership Project standards, Wireless Metropolitan
Area Network radios compliant with various IEEE 802.16 standards, and wired Local
Area Network Ethernet transceivers.
[0074]      The camera 104 or server 118 may also include one or more data-input
ports 826 via which any type of data, media content, or inputs can be received, such
as user-selectable inputs (e.g., from a keyboard, from a touch-sensitive input screen,
or from another user-input device), messages, music, television content, recorded
video content, and any other type of audio, video, or image data received from any
content or data source. The data-input ports 826 may include Universal Serial Bus
ports, coaxial-cable ports, and other serial or parallel connectors (including internal
connectors) for flash memory, storage disks, and the like. These data-input ports 826
may be used to couple the device 104, 118 to components, peripherals, or accessories
such as microphones and cameras.
[0075]      Finally, the camera 104 or server 118 may include any number of "other
sensors" 828. These sensors 828 can include, for example, accelerometers, a GPS
receiver, compass, magnetic-field sensor, and the like.
[0076]      The remainder of this discussion presents details of choices and procedures
that can be used in certain implementations. Although quite specific, these details are
given so that the reader can more fully understand the broad concepts discussed

                                             17
above. These implementation choices are not intended to limit the scope of the
claimed invention in any way.
[0077]      Many techniques can be used to evaluate still images in order to select the
"best" one (step 208 of Figure 2). For images that contain faces, one embodiment
calculates an image score based on sharpness and exposure and calculates a separate
score for facial features.
[0078]      First, facial-recognition techniques are applied to the captured images to
see if many of them contain faces. If so, then the scene being captured is evaluated as
a "face" scene. If the scene is not a face scene, then the sharpness/exposure score is
used by itself to select the best image. For a face scene, on the other hand, if the
images available for evaluation (that is, the set of all captured images that are
reasonably    close    in    time  to   the  capture command)      have  very   similar
sharpness/exposure scores (e.g., the scores are equal within a similarity threshold
which can be specific to the hardware used), then the best image is selected based
purely on the face score.
[0079]      For a face scene when the set of images have significant differences in
their sharpness/exposure scores, then the best image is the one that has the highest
combination score based on both the sharpness/exposure score and the face score. The
combination score may be a sum or weighted sum of the two scores:
        picture,.,(i) = mFEscore(i) + total ,.(i)
[0080]      The sharpness/exposure score can be calculated using the mean of the
Sobel gradient measure for all pixels in the image and the mean pixel difference
between the image being analyzed and the immediately preceding image. Luminance
only data are used in these calculations. The frame-gradient metric and frame
difference metric are calculated as:
                                H
                      1
        mSobel =                )  0.5 * (abs(Sobel x(ij)) + abs(Sobel-y(ij))
                           i=1 j=1

                                           18
                        W   H
        mDiff =             ) abs(Y(ij, t) - Y_,(i,j))
where:
        W= image width;
        H= image height;
        Sobelx = The result of convolution of the image with the Sobel Gx operator:
               G, =     2   0   2]; and
                      1-1   0   1
        Sobely = The result of convolution of the image with the Sobel Gy operator:
                       1     2     1
               G,=     0     0     0 .1
           The1             -2     -1
[0081] The sharpness/exposure score is calculated for each image (i) in the circular
image buffer of N images around the capture moment using the Sobel value and its
minimum:
        mFEscore(i) = (mSobel(i) - min(mSobel) + 1) * (1 -                       ~)
                                          N                              200
The mFEscore is set to 0 for any image if the mean of all pixel values in the image is
not within a normal exposure range or if the focus state indicates that the image is out
of-focus. The sharpness/exposure score values for the set of available images are then
normalized to a range of, say, 0 to 100 to be used in conjunction with face scores,
when a face scene is detected.
[0082]     The face score is calculated for the images when at least one face is
detected. For each face, the score consists of a weighted sum of detected-smile score,
open-eyes score, and face-orientation score. For example:

                                            19
        Smile: Values range from 1 to 100 with large values for a wide smile, small
       values for no smile.
       Eyes Open: Values range from 1 to 100 with small values for wide-open eyes,
       large values for closed eyes (e.g., a blink). Values are provided for each eye
       separately. A separate blink detector may also be used.
       Face Orientation (Gaze): An angle from 0 for a frontal look to +/- 45 for a
       sideways look.
The procedure uses face-detection-engine values and creates normalized scores for
each of the face parameters as follows:
        Smile Score: Use the smile value from the engine; then normalize to a 1 to 100
       range for the set of N available images as follows:
                              smile (i) - min(smile)
               smile(i) =
                            max(smile) - min(smile)
                              N                N
       Eyes-Open Score: Detect the presence of a blink or half-opened eyes using the
       blink detector and a change-of-eyes parameters between consecutive frames;
       score 0 for images when a blink or half-open eye is detected. For the rest of
       the images, a score is calculated using the average of the values for both eyes
       and normalizing to the range in a manner similar to that described for a smile.
       The maximum score is obtained when the eyes are widest open over the N
       images in the analysis.
       Face-Orientation Score (Gaze): Use a maximum score for a frontal gaze and
       reduce the score when the face is looking sideways.
For each face in the image, a face score is calculated as a weighted sum:
       face.       =  a * smile +fl* eyes +r *gaze
[0083]     If there are more faces than one in an image, then an average or weighted
average of all face scores can be used to calculate the total face score for that image.

                                          20
The weights used to calculate total face score could correlate to the face size, such
tat larger faces have higher score contributions to the total face score. In another
embodiment, weights correlate with face priority determined through position or by
some face-recognition engine. For an image i with M faces. the total faces score then
may be calculated as:
         total...(z} =NitEst , w, * face..(j}
                                    J1 Ij
100841      As discussed above, the face score can then be combined (as appropriate)
with the sharpness/exposure score. and the image with the highest score is selected as
the 'best" image. As a refinement in some embodiments, the selected image is then
compared against the "captured" image (that is, the image captured closest in time to
the time of the capture command). If these images are too similar, then only the
captured image is presented to the user. This consideration is generally applicable
because studies have shown that photographers do not prefer the selected "best"
image when its differences from the captured image are quite small
100851      As with selecting a "best" image. many techniques can be applied to
determine whether or not a captured video is "interesting.'     Generally, the video
analysis procedure rns in real time, constantly marking video frames as interesting or
not. Also, the video analysis determines where the-interesting video clip begins and
ends. Some metrits useful in video analysis include region of interest, motion vectors
("MVs"), device motion, face information, and frame statistics. These metrcs are
calculated per frame and associated with the frame.
100861      In some embodiments, a device-motion detection procedure combines data
frm a gyroscope, accelerometer, and magnetometer to calculate device movement
and device position, possibly using a complementary filter or Kahnan filter. The
results are categorized as follows:
        NO MOTION means that the device is either not moving or is experiencing
        only a small level of handshake;

                                              21
        INTENTIONALMOTION means that the device has been intentional moved
        (e.g., the photographer is panning); and
        UNINTENTIONALMOTION               means that the device has experienced large
        motion that was not intended as input to the image-capture system (e.g., the
        device was dropped, pulled out of a pocket, etc.).
By comparing consecutive values of the calculated position, the device's motion in
three spatial axes is characterized:
        if(delta position of all 3-axis < NOMO VEMENT_THRESHOLD)
                 device motion state = NOMOTION
        if(delta position of one axis < INTENTIONALMOTIONTHRESHOLD             &&
        delta position of other two axis < NOMO VEMENTTHRESHOLD &&
        occurs over a sequence offrames)
                 device motion state = INTENTIONALMOTION
        if(delta position of any axis> UNINTENTIONALMOTION            THRESHOLD)
                 device motion state = UNINTENTIONALMOTION
The device-motion state is then stored in association with the image.
[0087]      Motion estimation finds movement within a frame (intra-frame) as
opposed to finding movement between frames (inter-frame). A block-based motion
estimation scheme uses a sum of absolute differences ("SAD") as the primary cost
metric. Other embodiments may use object tracking. Generic motion-estimation
equations include:
                       ,N-1   N-1
        SAD(i,j) = I                s(x, y,l) - s(x +i,y +j, k -  )[
        s(x,y,1)whe-re 0!      x, y,!  N - I

                                             22
                 [rx,ry] = argmin{SAD~i,j)]
where:
           (x,,)safinctionspecifyingpixel location;
         (Z) candidateframe:
         (k)   referenceframe; and
         (revv) is the motion-vector displacement with respect to (ij)
The motion-estimation procedure compares each NxN candidate block against a
reference frame in the past and calculates the pixel displacement of the candidate
block. At each displacement position, SAD is calculated. The position that produces
the minimum SAD value represents the position with the lowest distortion (based on
the SAD cost metric).
100881       Once the raw vector are calculated for each NxN block, the vectors are
filtered to obtain the ntba-flame motion. In one exemplary method:
         Motion is estimated with predicted motion vectors;
        The median filter is applied to the motion vectors;
         Motion vectors are additionally filtered for the following reasons:
                   MVI > a static-motion threshold; or
                 IIMVI > adynamic-motion threshold; or
                 Collocated zero SAD > mean zero SAD (of all blocks); or
                 Block SAD <a large-SAD threshold; or
                 Luma variance > a low-block-activity threshold;

                                           23
       Create a mask region (eg, inscribe a       rnaximal regular diamond in the
       rectangular frame and then inscribe a maximal regular rectangular (the inner
       rectangle") in the diamond); and
       Calculate:
                DiamondCount = num(MV in the diamond region))/num(MV in the
                frame): and
                InnerRectangle Count      num(MV in the inner rectangle))/numn(MV
                in the diamond region).
100891      Each frame of video is characterized as "interesting" (or not) based on
metrics such as internal movement in the frame, luma-exposure values, device
motion, Sobel-gradient scores, and face motion. These medics are weighted to
account for the priority of each merc.
       Internal     Frame     Motion:    Calculated        from     Diamond Count      and
        Inner RectangleCount ratios;
       Luarn Exposure: Calculated from pixel data and weighted less for over or
        under exposed images;
        Sobel-Gradient Scores: Calculated from pixel data and weighted less for Sobel
       scores that are far from the temporal average of Sobel scores for each frame;
       Device      Motion:   Uses device-motion         states  and    weighted   less  for
        UNINTENTIONAL MOTION:
       Face Motion: Motion vectors are calculated from detected positions for each
        face. Weighted less for larger motion vectors for each face.
putting these together:
                                     N
       tnotionfracrm, score - Yw(rec         IetricUi)
                                     10

                                               24
[0090]       If the motionframescore exceeds a threshold, then the frame is included
in a "sequence calculation." This sequence calculation sums up the number of frames
that have interesting information and compares that to a sequence-score threshold. If
the sequence-score is greater than the threshold, then the scene is marked as an
interesting video clip and is permanently stored (step 312 of Figure 3).
[0091]       Before a video clip is stored, the start and stop points are calculated. Based
on device motion, the first level of delimiters are applied. The procedure finds the
segment in the video where the device was marked as NOMOTION and marks the
start and stop points. As a secondary check, the procedure also examines intra-frame
motion in each frame and marks those sub-segments within the segment that have no
camera motion to indicate when interesting motion occurred in the video. The first
frame with interesting intra-frame motion is the new start of the video clip, and the
last frame after capture in the video with interesting motion ends the video clip. In
some embodiments, the clip is extended to capture a small amount of time before and
after the interesting section.
[0092]       Horizon detection (see Figure 7 and accompanying text) processes image
frames and sensor data to find the frame with the most level horizon. If none of the
images contain a 0 degree (within a threshold) horizon line, then the image is rotated
and cropped to create an image with a level horizon. Vertical lines can be used in
detecting the horizon as well.
[0093]       In some embodiments, the following procedure is performed continuously,
as each frame arrives. For each image:
         Associate an angle position from the motion sensors with the image;
         Apply a Gaussian blur filter followed by an edge-detection filter on the image
         (e.g., use a Canny detection filter);
         Apply image processing to find lines in the image (e.g., use a Hough Line
         Transform). For each line found:

                                               25
                 Calculate the angle of the line with reference to 0 degrees orientation
                 of the device (i.e., horizontal); and
                 Keep lines that are:
                         Within some angle threshold; and
                         Within some length threshold;
        Find the longest line (called the "maximal line"), the start and end positions of
        the maximal line, and the angle of the maximal line. (It is useful to store line
        information in polar and Cartesian coordinates and in a linear equation.)
At this point in the procedure, each image contains metadata corresponding to the
following parameters: the length of the maximal line, the maximal line's angle with
respect to the horizon, the linear equation of the maximal line, and the device
orientation (i.e., the angle of the device with respect to the image plane derived from
the motion sensors).
[0094]      For each series of images, remove from consideration those images where
the absolute difference (device orientation angle minus angle of the maximal line) is
greater than a threshold. This allows physical motion-sensor information to be used in
conjunction with pixel information to determine the angle.
[0095]      Find the "region of interest" for each image. To do this, extend the
maximal line in the image to two boundaries of the image. The region of interest is
the smallest rectangle bounded on the right and left sides of the image that contains
the maximal line.
[0096]      Next find the "reference region" by finding the area of greatest overlap
among the regions of interest of the relevant images. This helps verify that each
maximal line is actually the same horizon line but captured at different angles in the
different images. Remove from consideration any images whose maximal lines fall
outside of the reference region.

                                            26
[0097]      Finally, for the relevant images, select that image whose maximal line in
the reference region has an angle closest to 0 degree orientation (that is, closest to
horizontal). Use that as the detected horizon. If necessary, that is, if the angle of the
selected image is greater than some threshold, the rotate the image using the
calculated angle and crop and upscale the image.
[0098]      In view of the many possible embodiments to which the principles of the
present discussion may be applied, it should be recognized that the embodiments
described herein with respect to the drawing figures are meant to be illustrative only
and should not be taken as limiting the scope of the claims. Therefore, the techniques
as described herein contemplate all such embodiments as may come within the scope
of the following claims and equivalents thereof.
[0099]      Throughout this specification and the claims which follow, unless the
context requires otherwise, the word "comprise", and variations such as "comprises"
and "comprising", will be understood to imply the inclusion of a stated integer or step
or group of integers or steps but not the exclusion of any other integer or step or group
of integers or steps.
[0100]      The reference to any prior art in this specification is not, and should not be
taken as, an acknowledgement or any form of suggestion that the referenced prior art
forms part of the common general knowledge in Australia.

                                           27
                                        CLAIMS
We claim:
1.     A method comprising:
               automatically capturing, by an image-capture device, video before
       receiving an image capture command;
                receiving, by the image-capture device and from a server remote from
       the image capture device, at least one recommendation based on analysis of
       metadata from another image capture device;
               continuously analyzing, by the image-capture device and based on the
       received at least one recommendation, the captured video to identify a
       segment of video determined to be potentially interesting to a user; and
               presenting, responsive to the identification of the segment of video
       determined to be potentially interesting to the user and by the image-capture
       device on a user interface of the image-capture device, a notification that the
       identified segment of video determined to be interesting to the user is
       available.
2.     The method of claim 1 wherein the continuously analyzing is based, at least in
      part, on activity detection, intra-frame motion estimation, inter-frame motion
       estimation, face detection, motion-sensing, camera-state sensing, or scene
       analysis.
3.     The method of claim 1 wherein the continuously analyzing is based, at least in
       part, on a stated preference of a user of the image-capture device, past
       behavior of the user of the image-capture device, or a privacy setting.
4.     The method of claim 1 further comprising receiving, by the image-capture
       device and from the server, at least one refined operational parameter based on
       analysis of the metadata from the another image capture device,
               wherein the continuously analyzing is further based, at least in part, on
       the received at least one refined operational parameter.

                                         28
5. The method of claim 1, wherein
           the method further comprises receiving the image capture command,
   and
           the continuously analyzing is based, at least in part, on a time of the
   image capture command.
6. The method of claim 5 wherein parameters associated with the capturing of the
   video before receiving the image capture command are based, at least in part,
   on levels of resources available to the image-capture device.
7. The method of claim 1 further comprising:
           storing, in a circular buffer, a most recent, finite portion of the captured
   video for analysis, the video captured before receiving the image capture
   command;
           wherein the stored, finite portion of the captured video is continuously
   refreshed, a latest frame of the captured video replacing an oldest frame of the
   finite portion of the captured video in the circular buffer.
8. The method of claim 7 further comprising:
           in response to the continuously analyzing identifying a segment of
   video with an interest score above a threshold, and responsive to a user
   command confirming interest in the identified segment of the video by the
   user, copying the identified segment of the video to long-term storage outside
   of the circular buffer.
9. The method of claim 1 further comprising:
           delimiting a segment of video, the delimiting based, at least in part, on
   the analyzing of the captured video or motion-sensor data.

                                         29
10. An image-capture device comprising:
            an image-capture system configured to automatically capture video
    before receiving an image capture command;
            a user interface;
            a communications interface configured to receive, from a server remote
    from the image-capture device, at least one recommendation based on analysis
    of metadata from another image-capture device; and
            a processing system operatively connected to the image-capture
    system, to communications interface, and to the user interface, the processing
    system configured to:
                    continuously analyze, based on the received at least one
    recommendation, the captured video captured to identify a segment of video
    with an interest score above a threshold; and
                    present, responsive to an identified segment of video with an
    interest score above the threshold and via the user interface, a notification that
    the identified segment of video with an interest score above the threshold is
    available.
11. The image-capture device of claim 10 wherein the device is selected from a
    camera, a cellphone, or a tablet computer.
12. The image-capture device of claim 10 wherein parameters associated with the
    capture of the video before receiving the image capture command are based on
    levels of resources available to the image-capture device.
13. The image-capture device of claim 10 wherein the processing system is
    configured to continuously analyze based, at least in part, on activity
    detection, intra-frame motion estimation, inter-frame motion estimation, face
    detection, motion-sensing, camera-state sensing, or scene analysis.

                                          30
14. The image-capture device of claim 10 wherein the processing system is
    configured to continuously analyze based, at least in part, on a stated
    preference of a user of the image-capture device, past behavior of the user of
    the image-capture device, or a privacy setting.
15. The image-capture device of claim 10:
             wherein the communications interface is further configured to receive,
    from the server, at least one refined operational parameter based on analysis of
    the metadata from the another image capture device, and
             wherein the continuously analyzing is further based, at least in part, on
    the received at least one refined operational parameter.
16. The image-capture device of claim 10 wherein
             the user interface is configured to receive the image capture command,
    and
             the processing system is configured to continuously analyze based, at
    least in part, on a time of the image capture command.
17. The image-capture device of claim 10 further comprising:
             a circular buffer configured to store a most recent, finite portion of the
    captured video for analysis, the video captured before receiving the image
    capture command;
             wherein the stored, finite portion of the captured video is continuously
    refreshed, a latest frame of the captured video replacing an oldest frame of the
    finite portion of the captured video in the circular buffer.
18. The image-capture device of claim 10 further comprising:
             long-term storage outside of the circular buffer;
             wherein the processing system is further configured to:
                      in response to identifying a segment of video with an interest
    score above the threshold, and in response to a user command confirming
    interest in the identified segment of the captured video by the user, copy the

                                      31
    identified segment of the captured video to the long-term storage outside of
    the circular buffer.
19. The image-capture device of claim 10 wherein the processing system is further
    configured to:
                          delimit a segment of video based, at least in part, on the
    continuously analyzing of the captured video or motion-sensor data.

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
