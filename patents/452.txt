SYSTEM AND METHOD FOR LOCAL THREE DIMENSIONAL VOLUME
RECONSTRUCTION USING A STANDARD FLUOROSCOPE
ABSTRACT
        A system (100) for constructing fluoroscopic-based three dimensional volumetric data from
two dimensional fluoroscopic frames of fluoroscopic video. A fluoroscopic imaging device (110)
rotatable about a target area of a patient and configured to acquire a fluoroscopic video of the target
area comprising frames of the target area. The frames being captured during rotation of the imaging
device about the target area. A computing device (125) configured to determine a pose of the
fluoroscopic imaging device for each frame of the fluoroscopic video;
      construct, from the fluoroscopic video and the pose determined for each frame of the
fluoroscopic video, fluoroscopic-based three dimensional volumetric data of the target area; and
      sharpen or intensify at least one of a portion of the fluoroscopic-based three dimensional
volumetric data or at least one frame to identify soft tissue objects using the fluoroscopic-based
three dimensional volumetric data.

                                                    1
SYSTEM AND METHOD FOR LOCAL THREE DIMENSIONAL VOLUME
RECONSTRUCTION USING A STANDARD FLUOROSCOPE
CROSS-REFERENCE TO RELATED APPLICATIONS
        This application is a divisional application of Australian Patent Application No.
<removed-apn>, filed on 5 August 2016, the contents of which are hereby incorporated in its entirety.
This application claims priority to U.S. Provisional Application Serial Nos. 62/201,750 and
15/224,812, the entire contents of which are incorporated by reference herein.
BACKGROUND
Technical Field
[0001] The present disclosure relates to a system, apparatus, and method of navigation and position
confirmation for surgical procedures. More particularly, the present disclosure relates to a system
and method for constructing a fluoroscopic-based three dimensional volume from two dimensional
fluoroscopic images captured using a standard fluoroscopic imaging device.
Description of Related Art
[0002] There are several commonly applied methods for treating various maladies affecting organs
including the liver, brain, heart, lung and kidney. Often, one or more imaging modalities, such as
magnetic resonance imaging, ultrasound imaging, computed tomography (CT), as well as others are
employed by clinicians to identify areas of interest within a patient and ultimately targets for
treatment.
[0003] An endoscopic approach has proven useful in navigating to areas of interest within a patient,
and particularly so for areas within luminal networks of the body such as the lungs. To enable the
endoscopic, and more particularly the bronchoscopic, approach in the lungs, endobronchial
navigation systems have been developed that use previously acquired MRI data or CT image data to
generate a three dimensional rendering or volume of the particular body part such as the lungs. In

                                                     2
particular, previously acquired images, acquired from an MRI scan or CT scan of the patient, are
utilized to generate a three dimensional or volumetric rendering of the patient.
[0004] The resulting volume generated from the MRI scan or CT scan is then utilized to create a
navigation plan to facilitate the advancement of a navigation catheter (or other suitable device)
through a bronchoscope and a branch of the bronchus of a patient to an area of interest.
Electromagnetic tracking may be utilized in conjunction with the CT data to facilitate guidance of
the navigation catheter through the branch of the bronchus to the area of interest. In certain
instances, the navigation catheter may be positioned within one of the airways of the branched
luminal networks adjacent to, or within, the area of interest to provide access for one or more
medical instruments.
[0005] Thus, in order to generate a navigation plan, or in order to even generate a three dimensional
or volumetric rendering of the patient's anatomy, such as the lung, a clinician is required to utilize
an MRI system or CT system to acquire the necessary image data for construction of the three
dimensional volume. An MRI system or CT-based imaging system is extremely costly, and in
many cases not available in the same location as the location where a navigation plan is generated
or where a navigation procedure is carried out.
[0006] A fluoroscopic imaging device is commonly located in the operating room during navigation
procedures. The standard fluoroscopic imaging device may be used by a clinician to visualize and
confirm the placement of a tool after it has been navigated to a desired location. However, although
standard fluoroscopic images display highly dense objects such as metal tools and bones as well as
large soft-tissue objects such as the heart, the fluoroscopic images have difficulty resolving small
soft-tissue objects of interest such as lesions. Further, the fluoroscope image is only a two
dimensional projection. In order to be able to see small soft-tissue objects in three dimensional
space, an X-ray volumetric reconstruction is needed. Several solutions exist that provide three
dimensional volume reconstruction of soft-tissues such as CT and Cone-beam CT which are
extensively used in the medical world. These machines algorithmically combine multiple X-ray
projections from known, calibrated X-ray source positions into three dimensional volume in which
the soft-tissues are visible.

                                                    3
[0007] In order to navigate tools to a remote soft-tissue target for biopsy or treatment, both the tool
and the target should be visible in some sort of a three dimensional guidance system. The majority
of these systems use some X-ray device to see through the body. For example, a CT machine can
be used with iterative scans during procedure to provide guidance through the body until the tools
reach the target. This is a tedious procedure as it requires several full CT scans, a dedicated CT
room and blind navigation between scans. In addition, each scan requires the staff to leave the
room. Another option is a Cone-beam CT machine which is available in some operation rooms and
is somewhat easier to operate, but is expensive and like the CT only provides blind navigation
between scans, requires multiple iterations for navigation and requires the staff to leave the room.
[0008] Accordingly, there is a need for a system that can achieve the benefits of the CT and Cone
beam CT three dimensional image guidance without the underlying costs, preparation requirements,
and radiation side effects associated with these systems. It is an object of the present invention to at
least partially satisfy this need.
SUMMARY
[0009] The present disclosure is directed to a system and method for constructing three dimensional
volumetric data in which small soft-tissue objects are visible from a video stream (or plurality of
images) captured by a standard fluoroscopic imaging device available in most procedure rooms.
The fluoroscopic-based constructed three dimensional volumetric data may be used for guidance,
navigation planning, improved navigation accuracy, navigation confirmation, and treatment
confirmation.
[0010] Soft tissue objects are not visible in standard fluoroscopic images and video because they
are obscured by dense objects such as dense tissue. The present disclosure is directed to a system
and method for creating a three-dimensional pseudo-volume in which the obscuring objects are
filtered based on the three dimensional position, and then projected back into two dimensions. In
one aspect, multiple fluoroscopic images, each captured at different angles, are utilized to construct
the three dimensional pseudo-volume data. The present disclosure describes a system and method
which is capable of constructing the three dimensional pseudo-volume data utilizing fluoroscopic
images captured from a short range of angles relative to a patient or target region of a patient.

                                                    4
Additionally, the present disclosure is also directed to a system and method for improving a
previously created three dimensional rendering utilizing two dimensional images, or video, captured
by a standard fluoroscopic imaging device.
[0011] As described in greater detail below, one aspect of the present disclosure is to determine
three dimensional positions of features in the fluoroscopic video, such as three dimensional catheter
position, three dimensional target tissue (for example a lesion) position, etc. In order to accomplish
this, the pose of the fluoroscopic imaging device for each frame must be determined or known. If
an external angle measurement device coupled to the fluoroscopic imaging device is utilized, then
the angles and pose of the imaging device is known for each frame. However, when external
measurement devices are not utilized other techniques are employed to determine the poses of the
fluoroscopic imaging device for each frame, as described in greater detail below. For example,
previously acquired CT scan data may be registered to the fluoroscopic video in order to
algorithmically find the pose of the fluoroscope for each frame of the captured video. Alternatively,
by tracking a few visible markers (two dimensional visible features) in the fluoroscopic video, the
pose and three dimensional positions may be solved together by using some structure from motion
technique. In some aspects, easier techniques in which the poses are known (by an angle
measurement device) may be utilized and only the three dimensional features' positions need to be
solved. In order to correct for movements, at least one marker (or a surgical device such as a
catheter tip) may be utilized.
[0012] Multiple fluoroscope two dimensional images can be processed algorithmically to create
pseudo three dimensional volumetric data, similar to Cone-beam CT, but with varying arbitrary
angle range and fluoroscope poses, where the fluoroscope is rotated manually. The angle range
may be very small (~30') which may result in poor three dimensional reconstruction quality. The
algorithm used is an iterative accelerated projection/back-projection method which unlike the
analytic algorithms (Radon transform, FDK...) doesn't assume any predetermined angle range or
angle rotation rate. In order to overcome the poor three dimensional reconstruction, instead of
displaying the raw three dimensional reconstructed data to the user, the three dimensional
reconstruction may be cropped around the area of interest (also referred to herein as the "FluoroCT
Blob"). The cropped three dimensional data is then reprojected into two dimensional virtual
fluoroscope images in which local soft-tissue features are visible. The reprojected two dimensional

                                                     5
images are of good quality (compared to the poor three dimensional reconstruction), especially if
the projections are done from the same fluoroscope poses as were seen in the video.
[0013] In order to reconstruct the three dimensional data, the pose of the fluoroscopic imaging
device must be determined for each two dimensional fluoroscope frame in the video, relative to
some fixed coordinate system. The pose of the fluoroscopic imaging device for each frame
captured may be determined using any of the methods described below. For example the pose may
be determined using an external measurement device coupled to the fluoroscopic imaging device.
[0014] Additionally, or alternatively, the pose may be determined using an external measurement
device and a single marker or a catheter tip. Specifically, in some cases, the fluoroscope (or C-arm)
may jitter during rotation, in which case some stabilization is needed. Unfortunately, due to
filtering, the angle measurement device may still report smooth angles throughout the video,
ignoring the high-frequencies which are present in the actual camera poses. In this case, a single
two dimensional marker can be tracked throughout the video and used to stabilize the camera poses,
or to increase their accuracy at the area of the marker. Since the marker will usually be located at
the region of interest, this increases the camera pose accuracy in this region, and thus improves the
three dimensional reconstruction quality. This method can also be used to compensate for patient
body movement such as breathing during the video. Instead of using the camera pose as reported by
the angle measurement device, a compensated camera pose is computed using the tracked two
dimensional marker, such that all camera poses will be correct relative to it. The single marker used
can be the tip of a tool or a catheter which is currently inserted to the patient.
[0015] Additionally, or alternatively, the pose may be determined via registration of the
fluoroscopic video to previously acquired CT data. Specifically, a previously acquired CT of the
patient may be available. In this case, each frame of the fluoroscope video can be registered to a
virtual fluoroscopic frame of the CT (camera pose is searched in CT space until the virtual
fluoroscopic image, corresponding to the camera pose, matches the one seen in the video). In this
way, the camera pose is realized using image-based features matching.
[0016] Additionally, or alternatively, the camera pose may be determined using structure from
motion techniques. Specifically, if numerous two dimensional features can be tracked throughout

                                                     6
the two dimensional video frames, from beginning to end, then these two dimensional features can
be used to realize camera poses (together with three dimensional feature positions) for each frame.
These features can be artificial markers which were introduced to the patient during procedure.
[0017] Filtering the obscuring tissue from the tissue of interest can be done by cropping at a
distance from the center of the generated three dimensional pseudo-volume data, cropping at a
distance from the tool/catheter, or registering to previously obtained three dimensional volume data
(CT) and using it to know which objects to filter.
[0018] Once the soft-tissue (such as a target lesion) is visible in the three dimensional reconstructed
data (or in the two dimensional enhanced FluoroCT images), all three dimensional information in
fluoroscope coordinates can be obtained. When working with the raw three dimensional
reconstructed data the three dimensional information is obtained directly from the data.
Alternatively, when working with the two dimensional enhanced FluoroCT images, 2-angles
markings are needed in order to realize three dimensional positions (triangulation).
[0019] The obtained three dimensional data may be utilized for confirmation of tool to soft-tissue
target three dimensional relation. For example, the data may be used to determine whether the tool
reached the target, the orientation of the tool relative to the target, the distance between the tool and
the target, or whether the target falls within an ablation zone of the tool.
[0020] Additionally, or alternatively, the three dimensional data may be utilized for correction of
navigation. For example, the three dimensional positions can be transformed from fluoroscope
coordinates into navigation system coordinates to improve navigation system accuracy at the region
of interest. In one aspect, when utilized in an EMN system, fluoroscope coordinates can be
transformed to antenna coordinates by assuming that the C-arm is perfectly perpendicular to the
antenna and matching the catheter tip, seen in the fluoroscope video, to the catheter position in
antenna at the time the video was taken. Fluoroscopic to/from antenna registration can also be
achieved by computing the angle between the C-arm and the antenna using the Earth's magnetic
field, attaching an EMN sensor to the fluoroscopic imaging device, or aligning a known two
dimensional feature of the antenna.

                                                     7
[0021] Aspects of the present disclosure are described in detail with reference to the figures
wherein like reference numerals identify similar or identical elements. As used herein, the term
"distal" refers to the portion that is being described which is further from a user, while the term
"proximal" refers to the portion that is being described which is closer to a user.
[0022] According to one aspect of the present disclosure, a system for constructing fluoroscopic
based three dimensional volumetric data from two dimensional fluoroscopic images is provided.
The system includes a computing device configured to facilitate navigation of a medical device to a
target area within a patient and a fluoroscopic imaging device configured to acquire a fluoroscopic
video of the target area about a plurality of angles relative to the target area. The computing device
is configured to determine a pose of the fluoroscopic imaging device for each frame of the
fluoroscopic video and to construct fluoroscopic-based three dimensional volumetric data of the
target area in which soft tissue objects are visible using a fast iterative three dimensional
construction algorithm. In aspects, the system is configured to determine the pose of the
fluoroscopic imaging device by knowing the angle range of movement of the device and computing
the relative rotational speed of the device along the range.
[0023] The medical device may be a catheter assembly including an extended working channel
configured to be positioned within a luminal network of the patient or the medical device may be a
radio-opaque marker configured to be placed within the target area. The radio-opaque marker is at
least partially visible in the fluoroscopic video acquired.
[0024] The computing device may be further configured to create virtual fluoroscopic images of the
patient from previously acquired CT volumetric data and register the generated virtual fluoroscopic
images with the acquired fluoroscopic video. The pose of the fluoroscopic imaging device for each
frame of the fluoroscopic video may be determined based on the registration between the
fluoroscopic video and the virtual fluoroscopic images.
[0025] The computing device may further be configured to detect frames missing from the
fluoroscopic video and supplement the detected missing frames with corresponding virtual
fluoroscopic images. The fluoroscopic-based three dimensional volumetric data may be
constructed based on the fluoroscopic video and the corresponding virtual fluoroscopic images. In

                                                     8
one aspect, the fluoroscopic-based three dimensional volumetric data may be registered with
previously acquired CT data using image-based techniques such as "mutual information." The
fluoroscopic-based three dimensional volumetric data may be registered globally to the previously
acquired CT data or locally, at the proximity of the target area of interest. A deep-learning based
approach may be utilized in which the computing device "sees" many examples of suitable and
non-suitable registrations and learns how to register the very two different modalities.
[0026] Additionally, the computing device may be further configured to track the two dimensional
position or orientation of the medical device navigated to the target region throughout the
fluoroscopic video. The computing device may be further configured to reconstruct positions of the
medical device throughout the fluoroscopic video using a structure-from-motion technique. The
pose of the fluoroscopic imaging device for each frame of the fluoroscopic video may be
determined based on the reconstructed positions. Additionally, or alternatively, the pose of the
fluoroscopic imaging device for each frame of the fluoroscopic video may be determined based on
an external angle measuring device. The external angle measuring device may include an
accelerometer, a gyroscope, or a magnetic field sensor coupled to the fluoroscopic imaging device.
Additionally, in aspects, the computing device may be configured to synchronize the captured
frames of the target area and compensate for shifts in the fluoroscopic imaging device or patient
movement to correct construction of the fluoroscopic-based three dimensional volumetric data.
Additionally, or alternatively, the computing device may be configured to crop a region of interest
from the fluoroscopic-based three dimensional volumetric data, project the cropped region of
interest onto the captured frames, and sharpen or intensify at least one of the region of interest or
the captured frame to identify soft tissue objects.
[0027] In yet another aspect of the present disclosure a method for constructing fluoroscopic-based
three dimensional volumetric data from two dimensional fluoroscopic images is provided. The
method includes navigating a medical device to a target area within a patient, acquiring a
fluoroscopic video of the target area about a plurality of angles relative to the target area using a
fluoroscopic imaging device, determining a pose of the fluoroscopic imaging device for each frame
of the fluoroscopic video, and constructing fluoroscopic-based three dimensional volumetric data of
the target area in which soft tissue objects are visible using a fast iterative three dimensional
construction algorithm. The medical device may be a catheter assembly including an extended

                                                     9
working channel configured to be positioned within a luminal network of the patient or the medical
device may be a radio-opaque marker configured to be placed within the target area. The radio
opaque marker is at least partially visible in the fluoroscopic video acquired.
[0028] The method may further include creating virtual fluoroscopic images of the patient from
previously acquired CT volumetric data, and registering the fluoroscopic video with the virtual
fluoroscopic images, wherein determining the pose of the fluoroscopic imaging device for each
frame of the fluoroscopic video is based on the registration between the fluoroscopic video and the
virtual fluoroscopic images. The method may further include detecting frames missing from the
fluoroscopic video and supplementing the detected missing frames with corresponding virtual
fluoroscopic images. Additionally, in aspects of the disclosure, the method may further include
tracking the two dimensional position or orientation of the medical device navigated to the target
region throughout the fluoroscopic video.
[0029] The positions of the medical device throughout the fluoroscopic video may be reconstructed
using a structure-from-motion technique. The pose of the fluoroscopic imaging device for each
frame of the fluoroscopic video may be determined based on the reconstructed positions.
Additionally, or alternatively, the pose of the fluoroscopic imaging device for each frame of the
fluoroscopic video may be determined based on an external angle measuring device. The external
angle measuring device may include an accelerometer, a gyroscope, or a magnetic field sensor
coupled to the fluoroscopic imaging device. Additionally, in aspects, the method may include
synchronizing the captured frames of the target area and compensating for shifts in the fluoroscopic
imaging device or patient movement to correct construction of the fluoroscopic-based three
dimensional volumetric data. Additionally, or alternatively, the method may include cropping a
region of interest from the fluoroscopic-based three dimensional volumetric data, projecting the
cropped region of interest onto the captured frames, and sharpening or intensifying at least one of
the region of interest or the captured frame to identify soft tissue objects.
BRIEF DESCRIPTION OF THE DRAWINGS
[0030] Various aspects and embodiments of the present disclosure are described hereinbelow with
references to the drawings, wherein:

                                                    10
[0031] Fig. 1 is a perspective view of one illustrative embodiment of an electromagnetic navigation
(EMN) system incorporating a fluoroscopic imaging device in accordance with the present
disclosure;
[0032] Fig. 2 illustrates a fluoroscopic imaging device model;
[0033] Fig. 3A is a flow chart of a method for constructing a three dimensional volume using a
plurality of radio-opaque markers;
[0034] Fig. 3B is an illustration of an example of frames of a fluoroscopic video captured by a
fluoroscopic imaging device showing markers and an extended working channel of a catheter
assembly positioned within a target region of a patient in accordance with the instant disclosure;
[0035] Fig. 4 is a flow chart of a method for constructing a three dimensional volume using either a
single radio-opaque marker or the tip of an extended working channel of a catheter assembly;
[0036] Fig. 5 is a flow chart of a method for constructing a three dimensional volume using either a
single radio-opaque marker or the tip of an extended working channel of a catheter assembly in
conjunction with an angle measurement device;
[0037] Fig. 6 is a flow chart of a method for three dimensional model construction in accordance
with the instant disclosure;
[0038] Fig. 7 is an illustration of a frame of an original video captured by a fluoroscopic imaging
device, an image of the frame after being ramp filtered, and the resulting three dimensional volume;
and
[0039] Fig. 8 is an illustration of a three dimensional construction generated according to a given
angle range of fluoroscopic images/video.
DETAILED DESCRIPTION

                                                   11
[0040] The present disclosure is directed to a system and method for constructing local three
dimensional volumetric data, in which small soft-tissue objects are visible, from a video stream
captured by a standard fluoroscopic imaging device available in most procedure rooms. The
constructed fluoroscopic-based local three dimensional volumetric data may be used for guidance,
navigation planning, improved navigation accuracy, navigation confirmation, and treatment
confirmation.
[0041] Fig. 1 depicts an Electromagnetic Navigation (EMN) system 100 configured for reviewing
CT image data to identify one or more targets, planning a pathway to an identified target (planning
phase), navigating an extended working channel (EWC) 12 of a catheter assembly to a target
(navigation phase) via a user interface, and confirming placement of the EWC 12 relative to the
target. One such EMN system is the ELECTROMAGNETIC NAVIGATION
BRONCHOSCOPY* system currently sold by Medtronic PLC. The target may be tissue of interest
identified by review of the CT image data during the planning phase. Following navigation, a
medical instrument, such as a biopsy tool or other tool, may be inserted into the EWC 12 to obtain a
tissue sample from the tissue located at, or proximate to, the target.
[0042] As shown in Fig. 1, EWC 12 is part of a catheter guide assembly 40. In practice, the EWC
12 is inserted into bronchoscope 30 for access to a luminal network of the patient "P." Specifically,
EWC 12 of catheter guide assembly 40 may be inserted into a working channel of bronchoscope 30
for navigation through a patient's luminal network. A locatable guide (LG) 32, including a sensor
44 is inserted into the EWC 12 and locked into position such that the sensor 44 extends a desired
distance beyond the distal tip of the EWC 12. The position and orientation of the sensor 44 relative
to the reference coordinate system, and thus the distal portion of the EWC 12, within an
electromagnetic field can be derived. Catheter guide assemblies 40 are currently marketed and sold
by Medtronic PLC under the brand names SUPERDIMENSION* Procedure Kits, or EDGETM
Procedure Kits, and are contemplated as useable with the present disclosure. For a more detailed
description of the catheter guide assemblies 40, reference is made to commonly-owned U.S. Patent
Publication No. 2014/0046315, filed on March 15, 2013, by Ladtkow et al, U.S. Patent No.
7,233,820, and U.S. Patent No. 9,044,254, the entire contents of each of which are hereby
incorporated by reference.

                                                    12
[0043] EMN system 100 generally includes an operating table 20 configured to support a patient
"P," a bronchoscope 30 configured for insertion through the patient's "P's" mouth into the patient's
"P's" airways; monitoring equipment 120 coupled to bronchoscope 30 (e.g., a video display, for
displaying the video images received from the video imaging system of bronchoscope 30); a
tracking system 50 including a tracking module 52, a plurality of reference sensors 54 and a
transmitter mat 56; and a computing device 125 including software and/or hardware used to
facilitate identification of a target, pathway planning to the target, navigation of a medical
instrument to the target, and confirmation of placement of an EWC 12, or a suitable device
therethrough, relative to the target.
[0044] A fluoroscopic imaging device 110 capable of acquiring fluoroscopic or x-ray images or
video of the patient "P" is also included in this particular aspect of system 100. The images, series
of images, or video captured by the fluoroscopic imaging device 110 may be stored within the
fluoroscopic imaging device 110 or transmitted to computing device 125 for storage, processing,
and display. Additionally, the fluoroscopic imaging device 110 may move relative to the patient
"P" so that images may be acquired from different angles or perspectives relative to the patient "P"
to create a fluoroscopic video. In one aspect of the present disclosure, fluoroscopic imaging device
110 includes an angle measurement device 111 which is configured to measure the angle of the
fluoroscopic imaging device 110 relative to the patient "P." Angle measurement device 111 may be
an accelerometer. Fluoroscopic imaging device 110 may include a single imaging device or more
than one imaging device. In embodiments including multiple imaging devices, each imaging device
may be a different type of imaging device or the same type. Further details regarding the imaging
device 110 are described in U.S. Patent No. 8,565,858, which is incorporated by reference in its
entirety herein.
[0045] Computing device 125 may be any suitable computing device including a processor and
storage medium, wherein the processor is capable of executing instructions stored on the storage
medium. The computing device 125 may further include a database configured to store patient
data, CT data sets including CT images, fluoroscopic data sets including fluoroscopic images and
video, navigation plans, and any other such data. Although not explicitly illustrated, the computing
device 125 may include inputs, or may otherwise be configured to receive, CT data sets,
fluoroscopic images/video and other data described herein. Additionally, computing device 125

                                                      13
includes a display configured to display graphical user interfaces. Computing device 125 may be
connected to one or more networks through which one or more databases may be accessed.
[0046] With respect to the planning phase, computing device 125 utilizes previously acquired CT
image data for generating and viewing a three dimensional model of the patient's "P's" airways,
enables the identification of a target on the three dimensional model (automatically, semi
automatically, or manually), and allows for determining a pathway through the patient's "P's"
airways to tissue located at and around the target. More specifically, CT images acquired from
previous CT scans are processed and assembled into a three dimensional CT volume, which is then
utilized to generate a three dimensional model of the patient's "P's" airways. The three
dimensional model may be displayed on a display associated with computing device 125, or in any
other suitable fashion. Using computing device 125, various views of the three dimensional model
or enhanced two dimensional images generated from the three dimensional model are presented.
The enhanced two dimensional images may possess some three dimensional capabilities because
they are generated from three dimensional data. The three dimensional model may be manipulated
to facilitate identification of target on the three dimensional model or two dimensional images, and
selection of a suitable pathway through the patient's "P's" airways to access tissue located at the
target can be made. Once selected, the pathway plan, three dimensional model, and images derived
therefrom, can be saved and exported to a navigation system for use during the navigation phase(s).
One such planning software is the ILOGIC* planning suite currently sold by Medtronic PLC.
[0047] With respect to the navigation phase, a six degrees-of-freedom electromagnetic tracking
system 50, e.g., similar to those disclosed in U.S. Patent Nos. 8,467,589, 6,188,355, and published
PCT Application Nos. WO 00/10456 and WO 01/67035, the entire contents of each of which are
incorporated herein by reference, or other suitable positioning measuring system, is utilized for
performing registration of the images and the pathway for navigation, although other configurations
are also contemplated. Tracking system 50 includes a tracking module 52, a plurality of reference
sensors 54, and a transmitter mat 56. Tracking system 50 is configured for use with a locatable
guide 32 and particularly sensor 44. As described above, locatable guide 32 and sensor 44 are
configured for insertion through an EWC 12 into a patient's "P's" airways (either with or without
bronchoscope 30) and are selectively lockable relative to one another via a locking mechanism.

                                                   14
[0048] Transmitter mat 56 is positioned beneath patient "P." Transmitter mat 56 generates an
electromagnetic field around at least a portion of the patient "P" within which the position of a
plurality of reference sensors 54 and the sensor element 44 can be determined with use of a tracking
module 52. One or more of reference sensors 54 are attached to the chest of the patient "P." The
six degrees of freedom coordinates of reference sensors 54 are sent to computing device 125 (which
includes the appropriate software) where they are used to calculate a patient coordinate frame of
reference. Registration, as detailed below, is generally performed to coordinate locations of the
three dimensional model and two dimensional images from the planning phase with the patient's
"P's" airways as observed through the bronchoscope 30, and allow for the navigation phase to be
undertaken with precise knowledge of the location of the sensor 44, even in portions of the airway
where the bronchoscope 30 cannot reach. Further details of such a registration technique and their
implementation in luminal navigation can be found in U.S. Patent Application Pub. No.
2011/0085720, the entire content of which is incorporated herein by reference, although other
suitable techniques are also contemplated.
[0049] Registration of the patient's "P's" location on the transmitter mat 56 is performed by
moving LG 32 through the airways of the patient's "P." More specifically, data pertaining to
locations of sensor 44, while locatable guide 32 is moving through the airways, is recorded using
transmitter mat 56, reference sensors 54, and tracking module 52. A shape resulting from this
location data is compared to an interior geometry of passages of the three dimensional model
generated in the planning phase, and a location correlation between the shape and the three
dimensional model based on the comparison is determined, e.g., utilizing the software on
computing device 125. In addition, the software identifies non-tissue space (e.g., air filled cavities)
in the three dimensional model. The software aligns, or registers, an image representing a location
of sensor 44 with a the three dimensional model and two dimensional images generated from the
three dimension model, which are based on the recorded location data and an assumption that
locatable guide 32 remains located in non-tissue space in the patient's "P's" airways. Alternatively,
a manual registration technique may be employed by navigating the bronchoscope 30 with the
sensor 44 to pre-specified locations in the lungs of the patient "P", and manually correlating the
images from the bronchoscope to the model data of the three dimensional model.

                                                   15
[0050] Following registration of the patient "P" to the image data and pathway plan, a user interface
is displayed in the navigation software which sets for the pathway that the clinician is to follow to
reach the target. One such navigation software is the ILOGIC* navigation suite currently sold by
Medtronic PLC.
[0051] Once EWC 12 has been successfully navigated proximate the target as depicted on the user
interface, the locatable guide 32 may be unlocked from EWC 12 and removed, leaving EWC 12 in
place as a guide channel for guiding medical instruments including without limitation, optical
systems, ultrasound probes, marker placement tools, biopsy tools, ablation tools (i.e., microwave
ablation devices), laser probes, cryogenic probes, sensor probes, and aspirating needles to the target.
[0052] Having described the components of system 100 depicted in Fig. 1, the following
description of Figs. 2-8 provides an exemplary workflow of using the components of system 100,
including the fluoroscopic imaging device 110, to construct local three dimensional volumetric data
of a desired region of interest using the fluoroscopic imaging device 110 of system 100. The
systems and methods described herein may be useful for visualizing a particular target region of a
patient utilizing imaging devices which are commonly located within a surgical setting during EMN
procedures, thereby obviating the need for subsequent MRI or CT scans.
[0053] Turning now to Fig. 2, a fluoroscopic imaging device 110 model is illustrated. The
fluoroscopic imaging device 110 includes an X-ray source 201 and a detector 203. The detector
203 defines a plurality of two dimensional pixels p. Each two dimensional pixel pi is associated
with a single X-ray beam li, traversing three dimensional space from the X-ray source 201 to the
detector 203. The detector 203 size D and the source-detector distance SDD determine the Field-of
View angle using the following formula:
[0054]             0 = 2tan-' (        ).
[0055] Different fluoroscopes differ primarily by detector size and source-detector distance. In the
fluoroscopic data, pixels are not normalized to a specific scale. Their brightness depends on the
gain/exposure used by the fluoroscope and on other objects in the scene which the X-rays traverse
between the source and the detector such as the table, the surgical device, etc. In the CT data, each

                                                     16
voxel is measured in Hounsfield units. Hounsfield units are measured with respect to the brightness
observed of water and air using the following formula:
                    HU = 1000      M -water
[0056]
                                  Mwater-Mair
[0057] y are attenuation coefficients. They range from 0 to infinity and measure how difficult it is
for an X-ray beam 1i to traverse through matter of the three dimensional volume 205. "Thicker"
matter has a larger p. HU scales attenuation coefficients such that air is placed at -1000, water at 0
and thicker matter goes up to infinity.
[0058] Using the Beer-Lambert law, each two dimensional pixel pi in the fluoroscope's detector
203 is given by:
[0059]              pi,; = Ioexp (- fli p(I)d l).
[0060] p (x, y, z) is the attenuation coefficient of the three dimensional volume 205 at position
(x, y, z). I is the X-ray source 201 energy (higher energy produces brighter image). For
simplicity, assume I = 1. Taking the logarithm, each two dimensional pixel pi is represented by:
[0061]              log(pi,j) = - f, p.jy(1) dl.
[0062] This is true for each two dimensional detector pixel pi which provides a linear equation on
the three dimensional attenuation coefficients. If many such equations are utilized (to solve for
each two dimensional pixel pi of the detector 203) then the attenuation coefficients may be solved
and the three dimensional volume data of the three dimensional volume 205 can be constructed.
[0063] Fluoroscope to CT - Discretization:
[0064] The three dimensional volume 205 can be divided into a discrete grid, with voxels sitting at
(xk, yk, zk). Thus, the equation for solving each two dimensional pixel pi can then be written as:
[00651              log(pi,)       Ek  Wk"'I(Xk,  Yk, Zk).

                                                    17
[0066] The left hand side of the equation above is the observed two dimensional pixel pi of the
detector 203 and the right hand side of the equation above is a weighted sum of the attenuation
coefficients (that is to be solved) with wkj which are determined by the known fluoroscopic
imaging device position of the X-ray source 201.
[0067] In order to be able to solve for the volumetric attenuation coefficient values enough linear
equations are needed, that is, enough two dimensional observations of different two dimensional
pixels p are required. Standard detectors usually have 1024x 1024 pixels, where each pixel is
represented by a single equation. Therefore, many two dimensional observations are required to be
solved for (many two dimensional observed pixels) to reconstruct a three dimensional volume (to
solve for many voxels). That is many two dimensional pixels are required to solve for the many
voxels. In order for the weights in the equations to be known, the fluoroscopic imaging device X
ray source 201 configuration (position, orientation, field of view) must be known.
[0068] In use, the three dimensional volume 205 is a portion of a patient's body. A fluoroscopic
video which is comprised of multiple fluoroscope images (as frames of the video) are taken from
many different locations relative to the patient, for example a 1800 rotation about the patient, to
acquire multiple observations of two dimensional pixels p at different positions relative to the
patient. As will be described in greater detail below, the position of the fluoroscopic imaging
device relative to the patient at a given time can be determined using multiple techniques, including
structure-from-motion analysis of radio-opaque markers placed within the patient (Figs. 3A-3B), a
registration between the captured fluoroscopic images/video and generated virtual fluoroscopic
images (Fig. 4), or by an external angle measurement device such as an accelerometer, gyroscope,
or magnetic field sensor (Fig. 5). In one aspect, the system may correct for patient movements
when measuring angles when a marker is utilized which moves with the movement of the body.
Specifically, all of the two dimensional fluoroscopic images may be synched to the same three
dimensional position based on the position of the placed marker in each of the images.
[0069] Turning now to Figs. 3A-3B and 4-6, methods for constructing a local three dimensional
volume of a target region using a standard fluoroscopic imaging device (such as the fluoroscopic
imaging device 110 of Fig. 1) in conjunction with a system such as the system described in Fig. 1

                                                    18
will now be described with particular detail. Although the methods illustrated and described herein
are illustrated and described as being in a particular order and requiring particular steps, any of the
methods may include some or all of the steps and may be implemented in any order not specifically
described.
[0070] The fluoroscopic-based three dimensional volume generated by any of the methods
described below can be incorporated into system 100 for multiple purposes. For example, the
fluoroscopic-based three dimensional volume can be registered with the previously generated three
dimensional volumetric data that was utilized for navigation of the medical instrument. The system
100 may utilize the registration between the fluoroscopic-based three dimensional volume and the
previously acquired three dimensional volumetric data to update the calculated position of the
sensor 44 (Fig. 1) which has been placed in the body of the patient. More particularly, the three
dimensional model of a patient's lungs, generated from previously acquired CT scans, may not
provide a basis sufficient for accurate guiding of medical instruments to a target during an
electromagnetic navigation procedure. In certain instances, the inaccuracy is caused by
deformation of the patient's lungs during the procedure relative to the lungs at the time of the
acquisition of the previously acquired CT data. This deformation (CT-to-Body divergence) may be
caused by many different factors, for example: sedation vs. no sedation, bronchoscope changing
patient pose and also pushing the tissue, different lung volume because CT was in inhale while
navigation is during breathing, different bed, day, etc. Thus, another imaging modality is necessary
to visualize targets and/or a terminal bronchial branch, and enhance the electromagnetic navigation
procedure by correcting the navigation during the procedure, enabling visualization of the target,
and confirming placement of the surgical device during the procedure. For this purpose, the system
described herein processes and converts image data captured by the fluoroscopic imaging device
110, as will be described in detail below. This fluoroscopic image data may be utilized to identify
such targets and terminal bronchial branches or be incorporated into, and used to update, the data
from the CT scans in an effort to provide a more accurate/correction of the electromagnetic
navigation procedure.
[0071] Additionally, users may visually confirm that the placement of the navigated medical
instrument is positioned in a desired location relative to a target tissue within a target area.
Additionally, the fluoroscopic-based three dimensional volume can be utilized to visualize the

                                                      19
target area in three dimensions after a procedure is performed. For example, the fluoroscopic-based
three dimensional volume can be utilized to visualize the target area after markers are placed within
the target area, after a biopsy is taken, or after a target is treated.
[0072] With particular reference to Figs. 3A-3B, a method for constructing a three dimensional
volume using a plurality of radio-opaque markers placed proximate the target will now be described
and referred to as method 300. Method 300 begins at step 301 where a marker placement device is
navigated to a target area utilizing an electromagnetic navigation system, such as the EMN system
100 (Fig. 1) described above. The navigation of the marker placement device to the target area may
be accomplished using a previously created navigation plan which includes routes created during
the planning phase. In step 303, radio-opaque markers are placed within the target area. In one
example, four radio-opaque markers are utilized. However, less than four or more than four radio
opaque markers may be used.
[0073] In step 305, with the radio-opaque markers placed in the target area, the fluoroscopic
imaging device is positioned such that all of the radio-opaque markers placed in step 303 are
visible. That is, step 305 includes aligning the fluoroscopic imaging device such that it can rotate
300 around the markers with all of the markers visible. In step 307, the fluoroscopic imaging
device is used to capture a video of about a 300 rotation of the imaging device 110 about the patient,
and thus around the markers (rotation from -15' to + 150). By rotating up to 150 (on each side) from
the centered angle, it can be ensured that the markers will remain in the images/frames for the entire
rotation video and that the imaging device will not hit the patient or the bed. Fig. 3B illustrates six
frames fl-f6 of the captured video. Each of frames fl-f6 is an image of the fluoroscopic video
showing the different position and orientation of each radio-opaque marker ml-m4 at different
points in time of the video, where the fluoroscopic imaging device is positioned at a different angle
relative to the patient at each given time.
[0074] In step 309, the two-dimensional position of each radio-opaque marker is tracked throughout
the entire video. In step 311, the marker positions are constructed in three dimensional using
structure-from-motion techniques and the pose of the fluoroscopic imaging device is obtained for
each video frame. Structure-from-motion is a method for reconstructing three dimensional
positions of points, along with fluoroscopic imaging device locations (camera poses) by tracking

                                                    20
these points in a two dimensional continuous video. In step 311, by introducing markers into the
patient, the positions and orientations of those markers can be tracked along a continuous
fluoroscope rotation video, their three dimensional position in space can be reconstructed, and the
corresponding fluoroscopic imaging device locations can be determined. With the fluoroscopic
imaging device locations determined through analysis of the video, the fluoroscopic imaging device
locations can be used to solve for the three dimensional data.
[0075] In step 313, a local three dimensional volume is constructed. Specifically, a fast iterative
reconstruction algorithm (Fig. 6) is used to reconstruct a local three dimensional volume at the area
of the markers which correspond to the local anatomy and which soft-tissues are visible. Step 313
may include reconstructing a global three dimensional volume from the acquired two dimensional
fluoroscopic data and cropping the global three dimensional volume at the area of the target to
create a "FluoroCT Blob" volume. This cropped volume may be displayed to the user in the form
of raw three dimensional data or as two dimensional reprojected images. In this cropped volume,
all anatomy in the target area, including the target tissue, will be visible. The reprojected image can
be intensified (which does not include the distant dense obscuring objects) by stretching the darker
value to be black and the brighter value to be white to increase differentiation and also sharpen in
either three dimension before projection or in the projected images, such that the soft tissue is
visually identified.
[0076] As described above, the fluoroscopic-based three dimensional volume can be incorporated
into system 100 for multiple purposes. For example, the fluoroscopic-based three dimensional
volume can be registered with the previously generated three dimensional volumetric data that was
utilized for navigation of the medical instrument. The system 100 may utilize the registration
between the fluoroscopic-based three dimensional volume and the previously acquired three
dimensional volumetric data to update the calculated position of the sensor 44 (Fig. 1).
Additionally, users may visually confirm that the placement of the navigated medical instrument is
positioned in a desired location relative to a target tissue within a target area.
[0077] Turning now to Fig. 4, a method for constructing a three dimensional volume using either a
single radio-opaque marker placed proximate the target or the distal portion of a navigated tool,
such as the tip of the extended working channel of the catheter assembly positioned proximate the

                                                   21
target will now be described and referred to as method 400. Although described as utilizing the tip
of the extended working channel of the catheter assembly, method 400 may utilize any tool to
achieve this function. For example, the tip of a navigated catheter, the tip of a biopsy tool, or the tip
of a treatment tool may be utilized. In one aspect, the tool is navigated transbronchially to the
target. In other aspects, the tool may be of a tool inserted into a patient percutaneously, for
example, a transthoracic navigation of a treatment device such as an ablation device.
[0078] Method 400 begins at step 401 where the extended working channel is navigated to a target
area utilizing an electromagnetic navigation system, such as the EMN system 100 (Fig. 1) described
above. The navigation of the EWC to the target area is accomplished using a previously created
navigation plan which includes routes created during the planning phase. Method 400 may
optionally include the additional step of navigating a marker placement device, via the EWC, to the
target area to place a single radio-opaque marker within the region of the target (step 403). In one
aspect, step 401 includes percutaneously inserting a tool to the target area.
[0079] After the EWC or tool is in position, or after the radio-opaque marker is placed, the
fluoroscopic imaging device is positioned such that the navigated tip of the EWC or tool (and/or the
placed radio-opaque marker) is visible within the field of view of the fluoroscopic imaging device.
That is, step 405 includes aligning the fluoroscopic imaging device such that it can rotate 300
around the marker with the marker visible and/or around the tip of the EWC or tool with the tip of
the EWC or tool visible. In step 407, the fluoroscopic imaging device is used to capture a video of
about a 300 rotation of the imaging device 110 about the patient, and thus around the marker and/or
tip of the EWC or tool (rotation from -15 0 to + 15 0). By rotating up to 150 (on each side) from the
centered angle, it can be ensured that the marker and/or tip of the EWC or tool will remain in the
images/frames for the entire rotation video and that the imaging device will not hit the patient or the
bed. Step 407 may include capturing a video of about a 300 rotation around the distal portion of the
EWC (and the radio-opaque marker, if placed). If a 300 rotation video is captured, then one angle
(in the middle of the range) is enough. That is, two projections with 300 between them are enough
to confirm or correct three dimensional relation of tools to soft tissue.
[0080] In step 409, the two-dimensional position of the distal portion of the EWC or tool (and/or
the radio-opaque marker, if placed) is tracked throughout the entire captured video.

                                                     22
[0081] In step 411, virtual fluoroscopic images are created from previously acquired CT data. The
previously acquired CT data is typically the CT data used during the planning phase to plan a
navigation path to the target. In step 411, the CT data is manipulated to create a computer model of
fluoroscopic images of the patient. The location of the target in the virtual fluoroscopic images
corresponds to the location of the target identified by the clinician during the planning phase. The
virtual fluoroscopic images generated by the system, based off of the previously acquired CT data,
depict the field of view that would be captured by a fluoroscopic imaging device. Additionally,
each of the virtual fluoroscopic images has a virtual fluoroscopic imaging device pose.
[0082] In step 413, each video frame of the fluoroscopic video captured in step 407 is registered to
the previously acquired CT data by matching each of the fluoroscopic video frames to the virtual
fluoroscopic images. In step 415, the fluoroscopic imaging device pose of each video frame of the
captured fluoroscopic video is determined based on the registration of step 413. That is, once a
fluoroscopic frame is matched to a virtual fluoroscopic image, the virtual fluoroscopic imaging
device pose of the virtual fluoroscopic image can be associated with the corresponding fluoroscopic
frame.
[0083] In step 417, the origin of the fluoroscopic imaging device pose determined in step 415 is
corrected by using the tracked position of the distal portion of the EWC or tool (and/or the radio
opaque marker, if placed), which is used to compensate for movement of the patient, such as
movement caused by breathing. In step 419, a local three dimensional volume is constructed.
Specifically, a fast iterative reconstruction algorithm (Fig. 6) is used to reconstruct a local three
dimensional volume at the area of the target lesion which corresponds to the local anatomy and
which soft-tissues are visible. Step 319 may include reconstructing a global three dimensional
volume from the acquired two dimensional fluoroscopic data and cropping the global three
dimensional volume at the area of the target to create a "FluoroCT Blob" volume. This cropped
volume may be displayed to the user in the form of raw three dimensional data or as two
dimensional reprojected images. In this cropped volume, all anatomy in the target area, including
the target tissue, will be visible. The reprojected image can be intensified (which does not include
the distant dense obscuring objects) by stretching the darker value to be black and the brighter value
to be white to increase differentiation and also sharpen in either three dimension before projection
or in the projected images, such that the soft tissue is visually identified.

                                                   23
[0084] Method 400 may also include the additional step (step 421) of completing the fluoroscopic
video captured in step 407 to include virtual fluoroscopic images that are generated by the system,
which are representative of fluoroscopic imaging device poses that are outside the range of
fluoroscopic imaging device poses captured in the fluoroscopic video. Specifically, in aspects, the
previously generated CT volumetric data of the patient which is used to create a navigation plan
may also be utilized by system 100 to generate virtual fluoroscopic images of the patient. The
generated virtual fluoroscopic images are fluoroscopic-like images which display a view to the user
of what a fluoroscopic image of a patient should look like if captured at a given angle by a
fluoroscopic imaging device. In step 421, the virtual fluoroscopic images may be used to fill any
gaps in the captured fluoroscopic video (captured in step 407). This may include, for example,
replacement of images, such as frames, of the captured video that are skewed or damaged.
Additionally, or alternatively, this may include, for example, supplementing the captured
fluoroscopic video (captured in step 407) with virtual fluoroscopic images that are representative of
fluoroscopic images that are outside the range of angles included in the fluoroscopic video. For
example, if the fluoroscopic video included a sweep of about a 300 range about the patient, virtual
fluoroscopic images that are outside the 30' range could be incorporated into the video to generate a
fluoroscopic video that has a range of greater than 30'.
[0085] Method 300 (Fig. 3A) and method 400 (Fig. 4) are both used to construct three dimensional
CT volumetric data using fluoroscopic video without knowing the fluoroscopic imaging device
poses of each of the frames of the fluoroscopic video. To this end, each of methods 300 and 400
require steps of determining the fluoroscopic imaging device pose of each of the frames of the
fluoroscopic video utilizing image-based techniques. In contrast, and as described in greater detail
below, method 500 (Fig. 5) is a method for constructing three dimensional CT volumetric data
where the fluoroscopic imaging device pose of each of the frames of the acquired fluoroscopic
video are determined using a pose/angle measurement device, which may include an accelerometer,
a gyroscope, or magnetic field detector to detect the position/pose of the fluoroscopic imaging
device relative to the patient.
[0086] Method 500 is a method for constructing a three dimensional volume using either a single
radio-opaque marker placed proximate the target or the tip of the extended working channel of the
catheter assembly positioned proximate the target, in conjunction with a fluoroscope angle

                                                     24
measurement device. Method 500 begins at step 501 where fluoroscope calibration is performed
using a fluoroscope calibration jig to compute the canonical fluoroscope projection parameters and
geometry. This is done once per fluoroscope device, in a setup phase by a technician. The
calibration jig is used to determine, in an automated process, both the projection parameters of the
fluoroscope (field of view angle) as well as the geometry of the C-arm: position relative to rotation
axis. These parameters are sometimes given in technical drawings per fluoroscope device, but may
also be found using our calibration jig. In step 503, the previously acquired CT volumetric data is
imported into the system along with the previously generated navigation plan.
[0087] In step 505, the extended working channel is navigated to a target area utilizing an
electromagnetic navigation technique using a system such as the EMN system 100 (Fig. 1)
described above. The navigation of the EWC to the target area is accomplished using the
previously created navigation plan which includes routes created during the planning phase.
Method 500 may optionally include the additional step (step 507) of navigating a marker placement
device, via the EWC, to the target area to place a single radio-opaque marker within the region of
the target.
[0088] After the EWC or tool is in position, or after the radio-opaque marker is placed, method 500
proceeds to step 509 which includes aligning the fluoroscopic imaging device such that it can rotate
30' around the marker with the marker visible and/or the tip of the EWC or the tool such that the tip
of the EWC or tool is visible. In step 511, the fluoroscopic imaging device is used to capture a
video of about a 30' rotation of the imaging device 110 about the patient, and thus around the
marker or tip of the EWC or tool (rotation from -15' to + 150). By rotating up to 15' (on each side)
from the centered angle, it can be ensured that the marker or tip of the EWC or tool will remain in
the images/frames for the entire rotation video and that the imaging device will not hit the patient or
the bed. If a 30' rotation video is captured, then one angle (in the middle of the range) is enough.
That is, two projections with 30' between them are enough to confirm or correct three dimensional
relation of tools to soft tissue. In step 513, the two-dimensional position and orientation of the
distal end of the EWC (and/or the radio-opaque marker, if placed) is tracked throughout the entire
captured video.

                                                   25
[0089] In step 515, the calibration data from step 501 is utilized in combination with measurements
from an external angle measurement device to compute the fluoroscopic imaging device location
(origin-less) in world coordinates. Specifically, the calibration jig is used, as described above, to
automatically find the projection parameters and the C-arm geometry of the specific fluoroscope
device by using optimization methods. Once these parameters are known, the angle taken from the
angle measurement device, that is, the angle of the detector of the fluoroscope, determines a unique
three dimensional pose for the detector. It cannot be located in any other place in space other than
the once explained by the given angle and the setup parameters.
[0090] In step 517, a local three dimensional volume is constructed. Specifically, a fast iterative
reconstruction algorithm (Fig. 6) is used to reconstruct a local three dimensional volume at the area
of the target tissue which corresponds to the local anatomy and which soft-tissues are visible. Step
517 may include reconstructing a global three dimensional volume from the acquired two
dimensional fluoroscopic data and cropping the global three dimensional volume at the area of the
target to create a "FluoroCT Blob" volume. This cropped volume may be displayed to the user in
the form of raw three dimensional data or as two dimensional reprojected images. In this cropped
volume, all anatomy in the target area, including the target tissue, will be visible. The reprojected
image can be intensified (which does not include the distant dense obscuring objects) by stretching
the darker value to be black and the brighter value to be white to increase differentiation and also
sharpen in either three dimension before projection or in the projected images, such that the soft
tissue is visually identified.
[0091] CT reconstruction methods will now be described. CT reconstruction methods can be
divided into Analytic methods (Radon, FDK...) and Algebraic methods (ART, SART...). Analytic
methods assume a very specific configuration, such as a full 1800 rotation, and reconstruct the CT
volume in a single iteration (using some exact formulas). Algebraic methods are more flexible but
slower and treat the problem as a large equation system which is solved iteratively (using some
gradient descent method). All methods use projection (three dimensional to two dimensional) and
back-projection (two dimensional to three dimensional).
[0092] With respect to projection, since each detector pixel is essentially a weighted sum of three
dimensional voxels along a ray, the detector image can be viewed as a two dimensional projection

                                                   26
of the three dimensional volume from some fluoroscopic imaging device location. If the three
dimensional volume data is already available, then the fluoroscope images can be reproduced by
projecting it from the known fluoroscopic imaging device locations. A three dimensional
reconstruction is considered good if its two dimensional projections resemble the observed
fluoroscope images it was created from.
[0093] With respect to back-projection, at each voxel, the back-projection determines which rays
traversed through a particular voxel and sums them together. In order to make this determination,
the fluoroscopic imaging device locations must be known. If the back-projection operator were
applied to the fluoroscope images of the captured video, then a three dimensional volume could be
constructed, but the constructed three dimensional volume would be very blurry and inaccurate
because while the true center voxel is summed many times, many irrelevant voxels surrounding the
true center voxel are also summed many times. After reconstructing a three dimensional volume
using one method or another, the quality of the reconstruction is evaluated by taking the
reconstructed three dimensional data, projecting it into two dimensional frames (which may be
virtual-fluoroscopic frames) and comparing those two dimensional frames to the original
fluoroscopic two dimensional frames from which the three dimensional data was created. A goal of
the volume reconstruction algorithm is to find three dimensional data which explains the two
dimensional observations, such that if the three dimensional data were to be projected back into two
dimensional frames, those frames would look like the original, real, fluoroscopic frames. When the
product is blurry, it means that the projections are blurry and do not match the real images. In order
to address this issue, the method provides for a ramp-filter and correction iteration.
[0094] Turning now to Fig. 6, a method for reconstructing a local three dimensional volume
utilizing a fast iterative algorithm will now be described and referred to as method 600. The three
dimensional volume reconstruction algorithm (for example method 600) consists of multiple
iterations of projection - back-projection. The goal of the algorithm is to find three dimensional
data which explains the two dimensional observations of the fluoroscopic imaging device. If it
succeeds in finding such data then the three dimensional data found is assumed to be the three
dimensional anatomy of the patient. In each iteration the current three dimensional data found is
projected into two dimensions, which should look like the original fluoroscope video, then the two
dimensional errors are backprojected back into the three dimensional data, and in such a manner the

                                                    27
three dimensional data is updated. This is repeated several times until the three dimensional data
converges and the process stops. In order to speed up the process, some filtering is done to the two
dimensional projected images before backprojecting them back into three dimensions. The ramp
filter is just an example filter which proves to be efficient in speeding up the convergence process.
This filter is applied when reconstructing standard CTs with the classic Radon transform. With the
classic approach, this process is done in a single iteration: Filtering (ramp-filter for example), Back
projection. In this method, this is repeated iteratively in several steps.
[0095] Method 600 begins in step 601 where the equation begins with an initial volume V (can just
be zeros). In step 603, the volume V is projected from known fluoroscopic imaging device
locations into images   Qi.  For example, projection may be done, not of the fluoroscopic images, but
of the fluoroscopic images filtered by a ramp function. The iterations with residuals, described
below, may undergo a ramp-filter before back-projections. In step 605, the residuals Ri = Pi - Qi
are computed where Pi are the observed projections from the captured fluoroscope video. In step
606, Ri is convolved with the ramp-filter as in Radon. In step 607, it is determined whether Ri is
below a predetermined threshold. If Ri is below the predetermined threshold (yes in step 607), then
method 600 is complete. If Ri is not below the predetermined threshold (no in step 607), then
method 600 proceeds to step 609. In step 609, Ri is back-projected into E, the correction volume.
In step 611, volume V is set to V + E (V = V + E) and method 600 reverts to step 603 where the
volume V (now V + E) is projected from known fluoroscopic imaging device locations into images
Qi.
[0096] Turning now to Figs. 7 and 8. Fig. 7 illustrates a frame of an original video captured by a
fluoroscopic imaging device 700, an image of the frame after being ramp-filtered 703, and an image
of the resulting three dimensional volume 705. Fig. 8 is an illustration of a three dimensional
construction at angles 801-88, where 801 is 30 degrees, 803 is 60 degrees, 805 is 90 degrees, 807 is
120 degrees, 809 is 150 degrees, and 811 is 180 degrees.
[0097] From the foregoing and with reference to the various figure drawings, those skilled in the art
will appreciate that certain modifications can also be made to the present disclosure without
departing from the scope of the same. For example, although the systems and methods are
described as usable with an EMN system for navigation through a luminal network such as the

                                                   28
lungs, the systems and methods described herein may be utilized with systems that utilize other
navigation and treatment devices such as percutaneous devices. Additionally, although the above
described system and method is described as used within a patient's luminal network, it is
appreciated that the above-described systems and methods may be utilized in other target regions
such as the liver. Further, the above-described systems and methods are also usable for
transthoracic needle aspiration procedures.
[0098] Detailed embodiments of the present disclosure are disclosed herein. However, the
disclosed embodiments are merely examples of the disclosure, which may be embodied in various
forms and aspects. Therefore, specific structural and functional details disclosed herein are not to
be interpreted as limiting, but merely as a basis for the claims and as a representative basis for
teaching one skilled in the art to variously employ the present disclosure in virtually any
appropriately detailed structure.
[0099] As can be appreciated a medical instrument such as a biopsy tool or an energy device, such
as a microwave ablation catheter, that is positionable through one or more branched luminal
networks of a patient to treat tissue may prove useful in the surgical arena and the present disclosure
is directed to systems and methods that are usable with such instruments and tools. Access to
luminal networks may be percutaneous or through natural orifice using navigation techniques.
Additionally, navigation through a luminal network may be accomplished using image-guidance.
These image-guidance systems may be separate or integrated with the energy device or a separate
access tool and may include MRI, CT, fluoroscopy, ultrasound, electrical impedance tomography,
optical, and/or device tracking systems. Methodologies for locating the access tool include EM, IR,
echolocation, optical, and others. Tracking systems may be integrated to an imaging device, where
tracking is done in virtual space or fused with preoperative or live images. In some cases the
treatment target may be directly accessed from within the lumen, such as for the treatment of the
endobronchial wall for COPD, Asthma, lung cancer, etc. In other cases, the energy device and/or
an additional access tool may be required to pierce the lumen and extend into other tissues to reach
the target, such as for the treatment of disease within the parenchyma. Final localization and
confirmation of energy device or tool placement may be performed with imaging and/or
navigational guidance using a standard fluoroscopic imaging device incorporated with methods and
systems described above.

                                                    29
[0100] While several embodiments of the disclosure have been shown in the drawings, it is not
intended that the disclosure be limited thereto, as it is intended that the disclosure be as broad in
scope as the art will allow and that the specification be read likewise. Therefore, the above
description should not be construed as limiting, but merely as exemplifications of particular
embodiments. Those skilled in the art will envision other modifications within the scope and spirit
of the disclosure herein.

                                                    30
CLAIMS:
1.       A system for constructing fluoroscopic-based three dimensional volumetric data from two
dimensional fluoroscopic frames of fluoroscopic video, comprising:
       a fluoroscopic imaging device rotatable about a target area of a patient and configured to
acquire a fluoroscopic video of the target area comprising frames of the target area, the frames
being captured during rotation of the imaging device about the target area; and
       a computing device configured to:
       determine a pose of the fluoroscopic imaging device for each frame of the fluoroscopic video;
       construct, from the fluoroscopic video and the pose determined for each frame of the
fluoroscopic video, fluoroscopic-based three dimensional volumetric data of the target area; and
       sharpen or intensify at least one of a portion of the fluoroscopic-based three dimensional
volumetric data or at least one frame to identify soft tissue objects using the fluoroscopic-based
three dimensional volumetric data.
2.     The system according to claim 1, wherein the computing device is further configured to create
virtual fluoroscopic images of the patient from previously acquired CT volumetric data and the pose
of the fluoroscopic imaging device for each frame of the fluoroscopic video is determined based on
a registration between the fluoroscopic video and the virtual fluoroscopic images.
3.     The system according to claim 2, wherein the computing device is further configured to:
       detect frames missing from the fluoroscopic video; and
       supplement the detected missing frames with corresponding virtual fluoroscopic images,
whereby the fluoroscopic-based three dimensional volumetric data is constructed based on the
fluoroscopic video and the corresponding virtual fluoroscopic images.
4.     The system according to any one of claims 1, 2 or 3, wherein the computing device is further
configured to track a two dimensional position of a medical device navigated to the target area in
each frame throughout the fluoroscopic video.

                                                    31
5.     The system according to claim 1, wherein the computing device is further configured to
reconstruct positions of a medical device throughout the fluoroscopic video using a structure-from
motion technique and the pose of the fluoroscopic imaging device for each frame of the
fluoroscopic video is determined based on the reconstructed positions.
6.     The system according to claim 1, wherein the pose of the fluoroscopic imaging device for
each frame of the fluoroscopic video is determined based on an external angle measuring device.
7.     The system according to any preceding claim, wherein the computing device is further
configured to:
       synchronize the captured frames of the target area; and
       compensate for shifts in the fluoroscopic imaging device or patient movement to correct
construction of the fluoroscopic-based three dimensional volumetric data.
8.    A method for constructing fluoroscopic-based three dimensional volumetric data from two
dimensional fluoroscopic frames of fluoroscopic video, comprising:
       acquiring a fluoroscopic video of a target area comprising frames of the target area using a
fluoroscopic imaging device, the frames being captured during rotation of the imaging device about
the target area;
       determining a pose of the fluoroscopic imaging device for each frame of the fluoroscopic
video;
       constructing, from the fluoroscopic video and the pose determined for each frame of the
fluoroscopic video, fluoroscopic-based three dimensional volumetric data; and
       sharpening or intensifying at least one of a portion of the fluoroscopic-based three
dimensional volumetric data or at least one frame to identify soft tissue objects using the
fluoroscopic-based three dimensional volumetric data.
9.     The method according to claim 8, further comprising:
       creating virtual fluoroscopic images from previously acquired CT volumetric data; and
      registering the fluoroscopic video with the virtual fluoroscopic images, wherein determining
the pose of the fluoroscopic imaging device for each frame of the fluoroscopic video is based on the

                                                   32
registration between the fluoroscopic video and the virtual fluoroscopic images, and further
comprising:
       detecting frames missing from the fluoroscopic video; and
       supplementing the detected missing frames with corresponding virtual fluoroscopic images.
10.    The method according to claim 8 or 9, wherein determining the pose of the fluoroscopic
imaging device for each frame of the fluoroscopic video is based on an external angle measuring
device.
11.    The method according to any one of claims 8 to 10, further comprising:
       synchronizing the captured frames of the target area; and
       compensating for shifts in the fluoroscopic imaging device or patient movement to correct
construction of the fluoroscopic-based three dimensional volumetric data.
12.    The method according to claim 8 or 9, further comprising tracking a two dimensional position
of a medical device navigated to the target area in each frame throughout the fluoroscopic video.
13.    The method according to claim 8 or 9, further comprising reconstructing positions of a
medical device throughout the fluoroscopic video using a structure-from-motion technique, wherein
determining the pose of the fluoroscopic imaging device for each frame of the fluoroscopic video is
based on the reconstructed positions.
14.    The method according to claim 12, wherein the medical device is at least one of an extended
working channel of a catheter assembly navigatable to the target area, a radio-opaque marker
navigatable to the target area, wherein the radio-opaque marker is at least partially visible in the
fluoroscopic video acquired, or a tool configured to percutaneously access the target area.
                                             Covidien LP
                                 Patent Attorneys for the Applicant
                                       SPRUSON & FERGUSON

         <removed-apn>   <removed-date>
    N
    ch
               0
               0
T   N
                            1/9

               <removed-apn>         <removed-date>
         201
FIG. 2                                 2/9
                            ~~~

<removed-date>
                                          3/9
              300~
                                                                 ~301
                         NAVIGATE TO REGION OF INTEREST/TARGET
                          AREA USING EMN OR OTHER TECHNIQUE
<removed-apn>
                                                                     303
                        PLACE MULTIPLE MARKERS (RADIO-OPAQUE)
                        WITHIN THE REGION OF THE TARGET/AROUND
                                        TARGET
                                                                     305
                     ALIGN THE FLUOROSCOPE C-ARM SUCH THAT IT CAN
                      ROTATE 30 AROUND THE MARKERS WITH ALL OF
                                  THE MARKERS VISIBLE
                                                                     307
                       RECORD VIDEO OF ABOUT A 30 FLUOROSCOPE
                            ROTATION AROUND THE MARKERS
                                                                     309
                       TRACK TWO-DEMENSIONAL POSITION OF EACH
                         MARKER THROUGHOUT THE ENTIRE VIDEO
                                                                     311
                        RECONSTRUCT THE MARKER POSITIONS IN 3D
                       USING STRUCTURE FROM MOTION AND OBTAIN,
                        FOR EACH VIDEO FRAME, THE FLUOROSCOPIC
                       POSE IN LOCAL MARKERS COORDINATE SYSTEM
                                                                     313
                     USE A FAST ITERATIVE 3D VOLUME RECONSTRUCTION
                      ALGORITHM (PROJECTION, BACK -PROJECTION TO
                      RECONSTRUCT A LOCAL 3D VOLUME AT THE AREA
                       OF THE MARKERS WHICH CORRESPOND TO THE
                         LOCAL ANATOMY AND WHICH SOFT-TISSUES
                                        ARE VISIBLE
                                      FIG. 3A

<removed-date>
                                    4/9
<removed-apn>
                                          r     N   C~')   CI'
                       N   ~   ~
              N
              L.L                   L.L
                                                                 W
                                                                 M
                                                                 C3
                                                                 1.~.
                                          ~--   N   M      ~
                      N
                    - N   M   ~-         ~- N      C'7 ~

<removed-date>
                                                5/9                                 ~aoo
                                                                             401
                 NAVIGATE REGION OF INTEREST/TARGET AREA USING EMN
                                        CATHETER
                -----------------------------~---------------------------f 403
              -
                   PLACE A SINGLE MARKER (RADIO-OPAQUE) IN REGION
<removed-apn>
                                     AROUND TARGET
                                                                       ~405
               ALIGN FLUOROSCOPE C-ARM SUCH THAT IT CAN ROTATE 30
               AROUND THE TOOL (AND/OR MARKER) WITH THE TIP OF THE
                             TOOL (AND/OR MARKER VISIBLE)
                                                                           407
                  RECORD VIDEO OF ABOUT 30 ROTATION AROUND THE I
                         PLACED MARKER AND/OR TIP OF TOOL
                                                                          :~.
              TRACK TWO DIMENSIONAL POSITION OF THE PLACED MARKER
                 AND/OR TIP OF TOOL THROUGHOUT THE ENTIRE VIDEO
                                                                             411
                    CREATE VIRTUAL-FLUOROSCOPIC IMAGES FROM              I
                     PREOPERATIVE CT-BASED VOLUMETRIC DATA
                                                                             413
               REGISTER VIDEO FRAMES OF THE CAPTURED FLUOROSCOPIC
                  VIDEO TO THE PREOPERATIVE VOLUME BY MATCHING
               THE VIDEO FRAMES TO THE VIRTUAL-FLUOROSCOPIC IMAGES
                                                                             415
               DETERMINE CAMERA POSE OF EACH VIDEO FRAME BASED ON
                             REGISTRATION/MATCHING
                                                                              -------~ 421
                                                                 ------------
                                                                   USE VIRTUAL FLUORO
                                                                   IMAGES TO COMPUTE
                                                                   ANY MISSING ANGLES
                                                               ~  FROM  CAPTURED VIDEO
                                                      X417
              CORRECT THE ORIGIN OF THE DETERMINED CAMERA POSE BY
                USING THE TRACKED MARKER AND/OR TIP OF TOOL AND
                     COMPENSATE FOR MOVEMENT (BREATHING)
                                                                         419
               USE FAST ITERACTIVE 3D RECONSTRUCTION ALGORITHM
               (PROJECTION, BACK-PROJECTION) TO RECONSTRUCT A
               LOCAL 3D VOLUME AT THE AREA OF THE SINGLE MARKER
               (OR TIP OF TOOL) WHICH CORRESPONDS TO THE LOCAL
              ANATOMY AND IN WHICH SOFT-TISSUE OBJECTS ARE VISIBLE
                                                                                   FIG. 4

<removed-date>
                                               6/9
              500-~
                                                                                     501
                      PERFORM FLUOROSCOPE CALIBRATION USING A FLUOROSCOPE
                      CALIBRATION JIG TO COMPUTE THE CANONICAL FLUOROSCOPE
                                         PROJECTION MATRIX
<removed-apn>
                                                                                     503
                      IMPORT PREVIOUSLY ACQUIRED PREOPERATIVE CT VOLUMETRIC I
                                     DATA AND NAVIGATION PLAN
                                                                                     505
                                  NAVIGATE TOOL TO TARGET AREA
                                                    ------------------------------~ 507
                  --------------------------------~
                        PLACE A SINGLE MARKER IN REGION AROUND TARGET
                                                                                ~-509
                     ALIGN FLUOROSCOPE C-ARM SUCH THAT IT CAN ROTATE 30
                   AROUND THE TIP OF THE TOOL (AND/OR THE MARKER) WITH THE
                            TIP OF THE TOOL (AND/OR MARKER) VISIBLE
                                                                                     511
                       RECORD A VIDEO OF ABOUT A 30 FLUOROSCOPE ROTATION        I
                            AROUND THE MARKER (AND/OR TIP OF TOOL)
                                                                                     513
                      TRACK TWO DIMENSIONAL POSITION OF THE MARKER (AND/OR I
                              TIP OF TOOL) THROUGHOUT ENTIRE VIDEO
                                                                                     515
                          USE THE CALIBRATION DATA IN COMBINATION WITH
                      MEASUREMENTS FROM AN EXTERNAL ANGLE MEASUREMENT
                           DEVICE TO COMPUTE THE CAMERA ORIENTATION
                              (ORIGIN-LESS) IN WORLD COORDINATES
                                                                                     517
                  USE A FAST ITERATIVE 3D VOLUME RECONSTRUCTION ALGORITHM
                   (PROJECTION, BACK-PROJECTION) TO RECONSTRUCT A LOCAL
                    3D VOLUME AT THE AREA OF THE SINGLE MARKER (AND/OR
                   CATHETER TIP) WHICH CORRESPONDS TO THE LOCAL ANATOMY
                         AND IN WHICH SOFT-TISSUE OBJECTS ARE VISIBLE
                                             FIG. 5

<removed-date>
                                              7/9
                                                                        ~soo
                                                                601
<removed-apn>
                         BEGIN WITH INITIAL VOLUME V
                            (CAN JUST BE ZEROS)
                                                                603
              PROJECT V FROM THE KNOWN CAMERA LOCATIONS INTO
                                  IMAGES Qi
                                                                605
                COMPUTE THE RESIDUALS Ri=Pi- Qi WHERE Pi ARE
                    THE OBSERVED PROJECTIONS FROM THE                          611
                            FLUOROSCOPIC VIDEO
                                                               X606   SETV=V+E
                CONVOLVE Ri WITH THE RAMP-FILTER AS IN RADON
                                               607                        ~ ~609
                                    R~            NO       BACK-PROJECT Ri INTO E,
                               < THRE~HOLD                 THE CORRECTION VOLUME
                                       YES
                                    END
                                             FIG. 6

<removed-date>
                               8/9
                      i
<removed-apn>
                           0
                          ..
              0
                                     ti
                      _~
                                     C
                                     ~
                   ~`~':~
                      ~--_
                 ~`.~ .~
               ~`~~
              0
              0
              ti

<removed-date>
                        9/9
              T
              T
<removed-apn>
                   0 a
                   ~
                   e p
                    4
                   o ~
                  :~~`?
                    O O
              O    ~p         W
                              V
              M
              O
                  <
                  ~
              T   '
                  /
                  ~~
                  Z

