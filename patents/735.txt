                                          ABSTRACT
The present disclosure is directed to a planning system for planning a surgical procedure. The
planning system includes a memory configured to store a plurality of images and a controller
configured to render the plurality of images in three dimensions. The controller also
automatically segments the plurality of images to demarcate a target area and automatically
determines a treatment plan based on the target area. A display is configured to display the
rendered plurality of images and the target area.

WO 2013/177051                                                                   PCT/US2013/041842
                              TREATMENT PLANNING SYSTEM
    [0001A] The present application is a divisional of Australian Patent Application No.
    2013266600, the content of which is incorporated herein by reference in its entirety.
    Australian Patent Application No. 2013266600 is related to PCT Application No. PCT/
    US2013/041842 (WO 2013/17705 1), the content of which is incorporated herein by
    reference in its entirety.
     BACKGROUND
     1.       Technical Field
     [0001]           The present disclosure relates to planning a surgical procedure, More
     specifically, the present disclosure is directed to the use of a planning system to
     determine a treatment plan by segmenting a plurality of images of a patient.
     2.       Backgroundof the Related Art
     [00021           Electrosurgical devices have become widely used. Electrosurgery
     involves the application of thermal and/or electrical energy to cut, dissect, ablate,
     coagulate, cauterize, seal or otherwise treat biological tissue during a surgical procedure.
     Electrosurgery is typically performed using a handpiece including a surgical device (e.g,,
     end effector or ablation probe) that is adapted to transmit energy to a tissue site during
     electrosurgical procedures, a remote electrosurgical generator operable to output energy,
     and a cable assembly operatively connecting the surgical device to the remote generator.
      [00031           Treatment of certain diseases requires the destruction of malignant tissue
     growths, e.g., tumors. In the treatment of diseases such as cancer, certain types of turnor
     cells have been found to denature at elevated temperatures that are slightly lower than
     temperatures normally injurious to healthy cells. Known treatment methods, such as
     hyperthermia therapy, typically involving heating diseased cells to temperatures above
      41* C while maintaining adjacent healthy cells below the temperature at which
      irreversible cell destruction occurs. These methods may involve applying
      electromagnetic radiation to heat, ablate and/or coagulate tissue. There are a number of
                                                    -1-

WO 2013/177051                                                                   PCT/US2013/041842
    different types of electrosurgical apparatus that can be used to perform ablation
    procedures.
    [0004]           Minimally invasive tumor ablation procedures for cancerous or benign
    tumors may be performed using two dimensional (2D) preoperative computed
    tomography (CT) images and an "ablation zone chart" which typically describes the
    characteristics of an ablation needle in an experimental, ex vivo tissue across a range of
    input parameters (power, time). Energy dose (power, time) can be correlated to ablation
    tissue effect (volume, shape) for a specific design. It is possible to control the energy
    dose delivered to tissue through microwave antenna design, for example, an antenna
    choke may be employed to provide a known location of microwave transfer from device
     into tissue. In another example, dielectric buffering enables a relatively constant
    delivery of energy from the device into the tissue independent of differing or varying
    tissue properties.
     [0005]          After a user determines which ablation needle should be used to effect
     treatment of a target, the user performs the treatment with ultrasound guidance.
     Typically, a high level of skill is required to place a surgical device into a target
     identified tinder ultrasound. Of primary importance is the ability to choose the angle and
     entry point required to direct the device toward the ultrasound image plane (e.g., where
     the target is being imaged).
     10006]          Ultrasound-guided intervention involves the use of real-time ultrasound
     imaging (transabdominal, intraoperative, etc.) to accurately direct surgical devices to
     their intended target. This can be performed by percutaneous application and/or
      intraoperative application. In each case, the ultrasound system will include a transducer
     that images patient tissue and is used to identify the target and to anticipate and/or follow
     the path of an instrument toward the target.
                                                    -2-

WO 2013/177051                                                                   PCT/US2013/041842
    [0007]           Ultrasound-guided interventions are commonly used today for needle
    biopsy procedures to determine malignancy of suspicious lesions that have been detected
    (breast, liver, kidney, and other soft tissues). Additionally, central-line placements are
    common to gain jugular access and allow medications to be delivered. Finally, emerging
    uses include tumor ablation and surgical resection of organs (liver, lung, kidney, and so
    forth). In the case of tumor ablation, after ultrasound-guided targeting is achieved a
    biopsy-like needle may be employed to deliver energy (RF, microwave, cryo, and so
    forth) with the intent to kill tumor. In the case of an organ resection, intimate knowledge
     of subsurface anatomy during dissection, and display of a surgical device in relation to
    this anatomy, is key to gaining successful surgical margin while avoiding critical
     structures.
     [0008]          In each of these cases, the ultrasound-guidance typically offers a two
     dimensional image plane that is captured from the distal end of a patient-applied
     transducer. Of critical importance to the user for successful device placement is the
     ability to visualize and characterize the target, to choose the instrument angle and entry
     point to reach the target, and to see the surgical device and its motion toward the target.
     Today, the user images the target and uses a high level of skill to select the instrument
     angle and entry point. The user must then either move the ultrasound transducer to see
     the instrument path (thus losing site of the target) or assume the path is correct until the
     device enters the image plane. Of primary importance is the ability to choose the angle
      and entry point required to direct the device toward the ultrasound image plane (e.g.,
      where the target is being imaged).
                                                    -3-.

                                                  4
SUMMARY
[0008A] It is an object of the present invention to substantially overcome or at least
ameliorate one or more of the above disadvantages.
[0008B] According to an aspect of the present disclosure, there is provided a planning system
comprising: a receiver configured to receive a plurality of images of a target area; a controller
configured to segment at least one vessel and at least one object in the plurality of images of
the target area, compute a proximity of the at least one vessel to the at least one object,
determine a treatment plan including an energy level and a treatment duration based on the
proximity of the at least one vessel to the at least one object; an input device configured to
adjust at least one of the energy level or the treatment duration of the treatment plan; and a
display configured to display a rendering of the plurality of images of the target area.
[0008C] According to an aspect of the present disclosure, there is provided a method of
creating a treatment plan, the method comprising: receiving a plurality of images of a target
area; segmenting at least one vessel and at least one object in the plurality of images of the
target area; computing a proximity of the at least one vessel to the at least one object;
determining a treatment plan including an energy level and a treatment duration based on the
proximity of the at least one vessel to the at least one object; adjusting at least one of the
energy level or the treatment duration of the treatment plan; and displaying a rendering of the
plurality of images of the target area.
[0009] This description may use the phrases "in an embodiment," "in embodiments," "in
some embodiments," or "in other embodiments," which may each refer to one or more of the
same or different embodiments in accordance with the present disclosure. For the purposes of
this description, a phrase in the form "A/B" means A or B. For the purposes of the
description, a phrase in the form "A and/or B" means "(A), (B), or (A and B)". For the
purposes of this description, a phrase in the form "at east one of A, B, or C" means "(A), (B),
(C), (A and B), (A and C), (B and C), or (A, B and C)".
[0010] As shown in the drawings and described throughout the following description, as is
traditional when referring to relative positioning on a surgical device, the term "proximal
13890358/P129525D1

                                                 4a
refers to the end of the apparatus that is closer to the user or generator, while the term "distal"
refers to the end of the apparatus that is farther away from the user or generator. The term
"user" refers to any medical professional (i.e., doctor, nurse, or the like) performing a medical
procedure involving the use of aspects of the present disclosure described herein.
[0011] As used in this description, the term "surgical device" generally refers to a surgical
tool that imparts electrosurgical energy to treat tissue. Surgical devices may include, but are
not limited to, needles, probes, catheters, endoscopic instruments, laparoscopic instruments,
vessel sealing devices, surgical staplers, etc. The term "electrosurgical energy" generally
refers to any form of electromagnetic, optical, or acoustic energy.
[0012] Electromagnetic (EM) energy is generally classified by increasing frequency or
decreasing wavelength into radio waves, microwaves, infrared, visible light, ultraviolet, X
rays and gamma-rays. As used herein, the term "microwave"
13890358/P129525D1

WO 2013/177051                                                                    PCT/US2013/041842
    generally refers to electromagnetic waves in the frequency range of 300 megahertz
    (MHz) (3 x 108 cycles/second) to 300 gigahertz (GHz) (3 x 1011 cycles/second). As used
    herein, the term "RF" generally refers to electromagnetic waves having a lower
    frequency than microwaves. As used herein, the term "ultrasound" generally refers to
    cyclic sound pressure with a frequency greater than the upper limit of human hearing.
    [00131           As used in this description, the term "ablation procedure" generally refers
    to any ablation procedure, such as microwave ablation, radio frequency (RF) ablation or
    microwave ablation-assisted resection. As it is used in this description, "energy
    applicator" generally refers to any device that can be used to transfer energy from a
    power generating source, such as a microwave or RF electrosurgical generator, to tissue.
     [00141           As they are used in this description, the terms "power source" and "power
    supply" refer to any source (e.g., battery) of electrical power in a form that is suitable for
    operating electronic circuits. As it is used in this description, "transmission line"
    generally refers to any transmission medium that can be used for the propagation of
    signals from one point to another. As used in this description, the terms "switch" or
    "switches" generally refers to any electrical actuators, mechanical actuators, electro
     mechanical actuators (rotatable actuators, pivotable actuators, toggle-like actuators,
     buttons, etc.), optical actuators, or any suitable device that generally fulfills the purpose
     of connecting and disconnecting electronic devices, or a component thereof, instruments,
     equipment, transmission line or connections and appurtenances thereto, or software.
     [00151           As used in this description, "electronic device" generally refers to a
     device or object that utilizes the properties of electrons or ions moving in a vacuum,
     gas, or semiconductor. As it is used herein, "electronic circuitry" generally refers to
     the path of electron or ion movement, as well as the direction provided by the device or
     object to the electrons or ions. As it is used herein, "electrical circuit" or simply
                                                    -5-

WO 2013/177051                                                                   PCT/US2013/041842
    "circuit" generally refers to a combination of a number of electrical devices and
    conductors that when connected together, form a conducting path to fulfill a desired
    function. Any constituent part of an electrical circuit other than the interconnections
    may be referred to as a "circuit element" that may include analog and/or digital
    components.
    [0016]          The term "generator" may refer to a device capable of providing energy.
    Such device may include a power source and an electrical circuit capable of modifying
    the energy outputted by the power source to output energy having a desired intensity,
    frequency, and/or waveform.
    [0017]          As it is used in this description, "user interface" generally refers to any
    visual, graphical, tactile, audible, sensory or other mechanism for providing information
    to and/or receiving information from a user or other entity, The term "user interface" as
    used herein may refer to an interface between a human user (or operator) and one or
    more devices to enable communication between the user and the device(s). Examples of
    user interfaces that may be employed in various embodiments of the present disclosure
    include, without limitation, switches, potentiometers, buttons, dials, sliders, a mouse, a
    pointing device, a keyboard, a keypad, joysticks, trackballs, display screens, various
    types of graphical user interfaces (GUIs), touch screens, microphones and other types of
    sensors or devices that may receive some form of human-generated stimulus and
    generate a signal in response thereto. As it is used herein, "computer" generally refers to
    anything that transforms information in a purposeful way.
     [0018]         The systems described herein may also utilize one or more controllers to
     receive various information and transform the received information to generate an
     output. The controller may include any type of computing device, computational circuit,
     or any type of processor or processing circuit capable of executing a series of
                                                    -6-

WO 2013/177051                                                                 PCT/US2013/041842
    instructions that are stored in a memory. The controller may include multiple processors
    and/or multicore central processing units (CPUs) and may include any type of processor,
    such as a microprocessor, digital signal processor, microcontroller, or the like. The
    controller may also include a memory to store data and/or algorithms to perform a series
    of instructions.
    [00191           Any of the herein described methods, programs, algorithms or codes may
    be converted to, or expressed in, a programming language or computer program. A
    "Programming Language" and "Computer Program" is any language used to specify
    instructions to a computer, and includes (but is not limited to) these languages and their
    derivatives: Assembler, Basic, Batch files, BCPL, C, C+, C++, Delphi, Fortran, Java,
    JavaScript, Machine code, operating system command languages, Pascal, Per], PLI,
    scripting languages, Visual Basic, metalanguages which themselves specify programs,
    and all first, second, third, fourth, and fifth generation computer languages. Also
     included are database and other data schemas, and any other meta-languages. For the
    purposes of this definition, no distinction is made between languages which are
     interpreted, compiled, or use both compiled and interpreted approaches. For the
    purposes of this definition, no distinction is made between compiled and source versions
     of a program. Thus, reference to a program, where the programming language could
     exist in more than one state (such as source, compiled, object, or linked) is a reference to
     any and all such states. The definition also encompasses the actual instructions and the
     intent of those instructions.
     [0020]           Any of the herein described methods, programs, algorithms or codes may
     be contained on one or more machine-readable media or memory. The term "memory"
     may include a mechanism that provides (e.g., stores and/or transmits) information in a
     form readable by a machine such a processor, computer, or a digital processing device.
                                                    -7-

WO 2013/177051                                                                   PCT/US2013/041842
    For example, a memory may include a read only memory (ROM), random access
    memory (RAM), magnetic disk storage media, optical storage media, flash memory
    devices, or any other volatile or non-volatile memory storage device. Code or
    instructions contained thereon can be represented by carrier wave signals, infrared
    signals, digital signals, and by other like signals.
    10021]           As it is used in this description, the phrase "treatment plan" refers to a
    selected ablation needle, energy level, and/or treatment duration to effect treatment of a
    target. The term "target" refers to a region of tissue slated for treatment, and may
    include, without limitation, tumors, fibroids, and other tissue that is to be ablated. The
    phrase "ablation zone" refers to the area and/or volume of tissue that will be ablated.
     [0022]          As it is used in this description, the phrase "computed tomography" (CT)
    or "computed axial tomography" (CAT) refer to a medical imaging method employing
    tomography created by computer processing. Digital geometry processing is used to
    generate a three-dimensional image of the inside of an object from a large series of two
    dimensional X-ray images taken around a single axis of rotation.
     100231          As it is used in this description, the term magnetic resonance imaging
     (MR), nuclear magnetic resonance imaging (NMRI), or magnetic resonance tomography
     (MRT) refer to a medical imaging technique used in radiology to visualize detailed
     internal structures. MRI makes use of the property of nuclear magnetic resonance
     (NMR) to image nuclei of atoms inside the body. An MRI machine uses a powerful
     magnetic field to align the magnetization of some atomic nuclei in the body, while using
     radio frequency fields to systematically alter the alignment of this magnetization, This
     causes the nuclei to produce a rotating magnetic field detectable by the scanner and this
     information is recorded to construct an image of the scanned area of the body.
                                                     -8-

WO 2013/177051                                                                   PCT/US2013/041842
    [00241          As it is used in this description, the term "three-dimensional ultrasound"
    or "3D ultrasound" refers to medical ultrasound technique providing three dimensional
    images.
    [0025]          As it is used in this description, the phrase "digital imaging and
    communication in medicine" (DICOM) refers to a standard for handling, storing,
    printing, and transmitting information relating to medical imaging. It includes a file
    format definition and a network communications protocol. The communication protocol
    is an application protocol that uses TCP/IP to communicate between systems. DICOM
    files can be exchanged between two entities that are capable of receiving image and
    patient data in DICOM format.
    [0026]          Any of the herein described systems and methods may transfer data
    therebetween over a wired network, wireless network, point to point communication
    protocol, a DICOM communication protocol, a transmission line, a removable storage
    medium, and the like.
     [00271         The systems described herein may utilize one or more sensors configured
    to detect one or more properties of tissue and/or the ambient environment. Such
    properties include, but are not limited to: tissue impedance, tissue type, tissue clarity,
    tissue compliance, temperature of the tissue orjaw members, water content in tissue, jaw
     opening angle, water motality in tissue, energy delivery, and jaw closure pressure.
     [00281         In an aspect of the present disclosure, a planning system is provided. The
     planning system includes a memory configured to store a plurality of images. The
     planning system also includes a controller configured to render the plurality of images in
     three dimensions, automatically segment the plurality of images to demarcate a target
     area, and automatically determine a treatment plan based on the target area. A display is
     provided to display the rendered plurality of images and the target area.
                                                    -9-

WO 2013/177051                                                                   PCT/US2013/041842
    [0029]           In the planning system, the controller performs a volumetric analysis to
    determine a treatment plan. The planning system may also include an input device
    configured to adjust the treatment plan. The display provides a graphical user interface.
    [00301           The controller may also segment at least one vessel and adjust the
    treatment plan based on the proximity of the vessel to the target or the controller may
    segment at least one organ and adjust the treatment plan based on a position of the target
    in relation to the organ.
    BRIEF DESCRIPTION OF THE DRAWINGS
     [0031]          The above and other aspects, features, and advantages of the present
    disclosure will become more apparent in light of the following detailed description when
    taken in conjunction with the accompanying drawings in which:
     [0032]          Figure 1 is a system block diagram of a planning and navigation system
    according to an embodiment of the present disclosure;
     [00331          Figures 2A and 2B are schematic diagrams of an ablation needle
     according to an embodiment of the present disclosure;
     [00341           Figure. 3 is a schematic diagram of a radiation pattern of the ablation
     needle of Figures 2A and 2B;
     [0035]           Figure 4 is a schematic diagram of a planning system according to an
     embodiment of the present disclosure;
     [00361           Figure 5 is a flowchart depicting overall operation of the planning system
     according to an embodiment of the present disclosure;
     [0037]           Figures 6 and 7 are schematic diagrams of graphical user interfaces used
     in the planning system in accordance with an embodiment of the present disclosure;
                                                   -10-

WO 2013/177051                                                                  PCT/US2013/041842
    [0038]          Figure 8 is a flowchart depicting an algorithm for image segmentation and
    inverse planning according to an embodiment of the present disclosure;
     [0039]         Figure 9 is a flowchart depicting an algorithm for segmenting a nodule
    according to an embodiment of the present disclosure;
     [0040]         Figures 1OA-IOB are graphical representations of relationships between
    ablation zones and energy delivery;
     [0041]         Figure 1IA is a schematic diagram of a relationship between a vessel and
    a target according to another embodiment of the present disclosure;
     [00421         Figure 11lB is a graphical representation of an alternate dosing curve
     according to another embodiment of the present disclosure;
     [0043]         Figures 12A-12C are schematic diagrams of a planning method according
    to another embodiment of the present disclosure;
     [0044]         Figure 13 is a schematic diagram of a navigation system according to an
     embodiment of the present disclosure;
     [0045]          Figures 14A and 14B are schematic diagrams of graphical user interfaces
     used in the navigation system of Figure 13;
     [00461          Figure 15 is a flowchart depicting a fiducial tracking algorithm according
     to an embodiment of the present disclosure;
     [0047]          Figures 16A and 16B depict an image taken by a camera and a corrected
     version of the image, respectively;
      [0048]         Figure 17 is a flowchart depicting an algorithm for finding white circles
     according to an embodiment of the present disclosure;
      [00491         Figures 18A-I 8C depict intermediate image results of the algorithm
     depicted in Figure 17;
                                                  -II1-

WO 2013/177051                                                                PCT/US2013/041842
    [0050]          Figure 19 is a flowchart depicting an algorithm for finding black circles
    and black regions according to an embodiment of the present disclosure;
    [00511          Figures 20A-20D depict intermediate image results of the algorithm
    depicted in Figure 19;
    [0052]          Figure 21A is a flowchart depicting a correspondence algorithm according
    to an embodiment of the present disclosure;
    [00531           Figure 21B is a flowchart depicting an algorithm for applying a topology
    constraint according to an embodiment of the present disclosure;
     [0054]          Figure 22A-22D are a schematic diagrams of fiducial models used in the
    algorithm of Figure 21A;
     [0055]          Figure 23 is a schematic diagram of an integrated planning and navigation
    system according to another embodiment of the present disclosure;
     [00561          Figure 24 is a schematic diagram of an integrated planning and navigation
     system according to yet another embodiment of the present disclosure;
     [0057]          Figures 25A and 25B are schematic diagrams of a navigation system
     suitable for use with the system of Figure 24; and
     [0058]          Figures 26-29 are schematic diagrams of graphical user interfaces used in
     the system of Figure 24 in accordance with various embodiments of the present
     disclosure.
     DETAILED DESCRIPTION
     10059]          Particular embodiments of the present disclosure are described
     hereinbelow with reference to the accompanying drawings; however, it is to be
     understood that the disclosed embodiments are merely examples of the disclosure and
     may be embodied in various forms. Well-known functions or constructions are not
                                                 -12-

WO 2013/177051                                                                  PCT/US2013/041842
    described in detail to avoid obscuring the present disclosure in unnecessary detail.
    Therefore, specific structural and functional details disclosed herein are not to be
    interpreted as limiting, but merely as a basis for the claims and as a representative basis
    for teaching one skilled in the art to variously employ the present disclosure in virtually
    any appropriately detailed structure. Like reference numerals may refer to similar or
    identical elements throughout the description of the figures.
    [0060]           Turning to the figures, Figure 1 depicts an overview of a planning and
    navigation system according to various embodiments of the present disclosure. As
    shown in Figure 1, pre-operative images 15 of a patient "P" are captured via an image
    capture device 10. Image capture device 10 may include, but is not limited to, a MRI
    device, a CAT device, or an ultrasound device that obtains two-dimensional (2D) or
    three-dimensional (3D) images. Image capture device 10 stores pre-operative images 15
    that are transferred to planning system 100. Pre-operative images 15 may be transferred
    to planning system 100 by uploading images 15 to a network, transmitting images 15 to
    planning system 100 via a wireless communication means, and/or storing images 15 on a
    removable memory that is inserted into planning system 100. In an embodiment of the
    present disclosure, pre-operative images 15 are stored in a DICOM format. In some
     embodiments, image capture device 10 and planning system 100 may be incorporated
     into a standalone unit.
     [00611          Planning system 100, which is described in more detail below, receives
     the pre-operative images 15 and determines the size of a target. Based on the target size
     and a selected surgical device, planning system 100 determines settings that include an
     energy level and a treatment duration to effect treatment of the target.
     [00621          Navigation system 200, which is described in more detail below, utilizes a
     fiducial pattern disposed on a medical imaging device (e.g., an ultrasound imaging
                                                  -13-

WO 2013/177051                                                                   PCT/US2013/041842
    device) to determine an intracorporeal position of an surgical device. The intracorporeal
    position of the surgical device is displayed on a display device in relation to an image
    obtained by the medical imaging device, Once the surgical device is positioned in the
    vicinity of the target, the user effects treatment of the target based on the treatment zone
    settings determined by the planning system,
     [0063]         In some embodiments, a user determines the treatment zone settings using
    planning system 100 and utilizes the treatment zone settings in effecting treatment using
     navigation system 200. In other embodiments, the planning system 100 transmits the
    treatment zone settings to navigation system 200 to automatically effect treatment of the
     target when the surgical device is in the vicinity of the target. Additionally, in some
     embodiments, planning system 100 and navigation system 200 are combined into a
     single standalone system. For instance, a single processor and a single user interface
     may be used for planning system 100 and navigation system 200, a single processor and
     multiple user interfaces may be used to for planning system 100 and navigation system
     200, or multiple processors and a single user interface may be used for planning system
     100 and navigation system 200.
     [0064]          Figure 2A shows an example of a surgical device in accordance with an
     embodiment of the present disclosure. Specifically, Fig. 2A shows a side view of a
     variation on an ablation needle 60 with an electrical choke 72 and Figure 2B shows a
     cross-section side view 2B-2B from Figure 2A. Ablation needle 60 shows radiating
     portion 62 electrically attached via feedline (or shaft) 64 to a proximally located coupler
     66. Radiating portion 62 is shown with sealant layer 68 coated over section 62.
     Electrical choke 72 is shown partially disposed over a distal section of feedline 64 to
     form electrical choke portion 70, which is located proximally of radiating portion 62.
                                                   -14-

WO 2013/177051                                                                 PCT/US2013/041842
    [0065]          To improve the energy focus of the ablation needle 60, the electrical
    choke 72 is used to contain field propagation or radiation pattern to the distal end of the
    ablation needle 60. Generally, the choke 72 is disposed on the ablation needle 60
    proximally of the radiating section, The choke 72 is placed over a dielectric material that
    is disposed over the ablation needle 60. The choke 72 is a conductive layer that may be
    covered by a tubing or coating to force the conductive layer to conform to the underlying
    ablation needle 60, thereby forming an electrical connection (or short) more distally and
    closer to the radiating portion 62. The electrical connection between the choke 72 and
    the underlying ablation needle 60 may also be achieved by other connection methods
    such as soldering, welding, brazing, crimping, use of conductive adhesives, etc.
    Ablation needle 60 is electrically coupled to a generator that provides ablation needle 60
    with electrosurgical energy.
     [0066]          Figure 3 is a cross-sectional view of an embodiment of the ablation needle
    60 shown with a diagrammatic representation of an emitted radiation pattern in
    accordance with the present disclosure.
     [0067]          Figures 4 to 12C describe the operation of planning system 100 in
    accordance with various embodiments of the present disclosure. Turning to Figure 4,
    planning system 100 includes a receiver 102, memory 104, controller 106, input device
     108 (e.g., mouse, keyboard, touchpad, touchscreen, etc.), and a display 110. During
     operation of the planning system 100, receiver 102 receives pre-operative images 15 in
     DICOM format and stores the images in memory 104. Controller 106 then processes
     images 15, which is described in more detail below, and displays the processed images
     on display 110. Using input device 108, a user can navigate through the images 15,
     select one of the images from images 15, select a seed point on the selected image, select
                                                   -15-

WO 2013/177051                                                                 PCT/US2013/041842
    an ablation needle, adjust the energy level, and adjust the treatment duration. The inputs
    provided by input device 108 are displayed on display I 10.
    [0068]           Figure 5 depicts a general overview of an algorithm used by planning
    system 100 to determine a treatment plan. As shown in Fig. 5, in step 120, images in a
    DICOM format are acquired via a wireless connection, a network, or by downloading the
    images from a removable storage medium and stored in memory 104. Controller 106
    then performs an automatic three dimensional (3D) rendering of the images 15 and
    displays a 3D rendered image (as shown in Figure 6) in step 122. In step 124, image
    segmentation is performed to demarcate specific areas of interest and calculate
    volumetrics of the areas of interest. As described below, segmentation can be user
    driven or automatic. In step 126, the controller performs an inverse planning operation,
    which will also be described in more detail below, to determine a treatment algorithm to
    treat the areas of interest. The treatment algorithm may include selection of a surgical
    device, energy level, and/or duration of treatment. Alternatively, a user can select the
    surgical device, energy level, and/or duration of treatment to meet the intentions of a
    treating physician that would include a "margin value" in order to treat the target and a
    margin of the surrounding tissue.
     [00691          Figures 6 and 7 depict graphical user interfaces (GUIs) that may be
    displayed on display 110. As shown in Figures 6 and 7, each GUI is divided into a
     number of regions (e.g., regions 132, 134, and 136) for displaying the rendered DICOM
     images. For example, region 132 shows an image of patient "P" along a transverse
     cross-section and region 134 shows an image of patient "P" along a coronal cross
     section. Region 136 depicts a 3D rendering of patient "P". In other embodiments, a
     sagittal cross-section may also be displayed on the GUL The GUI allows a user to select
     different ablation needles in drop down menu 131. The GUI also allows a user to adjust
                                                  -16-

WO 2013/177051                                                                    PCT/US2013/041842
    the power and time settings in regions 133 and 135, respectively. Additionally, the GUI
    has a number of additional tools in region 137 that include, but are not limited to, a
    planning tool that initiates the selection of a seed point, a contrast tool, a zoom tool, a
    drag tool, a scroll tool for scrolling through DICOM images, and a 3D Render tool for
    displaying the volume rendering of the DICOM dataset.
    [0070]          The flowchart of Figure 8 depicts the basic algorithm for performing the
    image segmentation step 124 and the inverse planning step 126. As shown in Figure 8, a
    user selects a seed point in step 140 (see Figure 6 where a cross hair is centered on the
    target "T" in regions 132 and 134). After the seed point is manually selected, planning
    system 100 segments a nodule to demarcate a volume of interest in step 142. In other
    embodiments, the seed point may be automatically detected based on the intensity values
    of the pixels.
    [0071]          Figure 9 depicts a flowchart of an algorithm used to segment a nodule.
    As shown in Figure 9, once a seed point is identified in step 151, the algorithm creates a
    Region of Interest (ROI) in step 152. For example, the ROI may encompass a volume of
    4cm 3, In step 153, a connected threshold filter applies a threshold and finds all the pixels
    connected to the seed point in the DICOM images stored in memory 104. For example,
    the threshold values may start at -400 Houndsfields Units (HU) and end at 100 HU when
    segmenting lung nodules.
    [0072]          In step 154, controller 106 applies a geometric filter to compute the size
    and shape of an object. The geometric filter enables the measurement of geometric
    features of all objects in a labeled volume. This labeled volume can represent, for
    instance, a medical image segmented into different anatomical structures. The
    measurement of various geometric features of these objects can provide additional
    insight into the image.
                                                  -17-

WO 2013/177051                                                                  PCT/US2013/041842
    [00731           The algorithm determines if a predetermined shape is detected in step
    155. If a predetermined shape is not detected, the algorithm proceeds to step 156 where
    the threshold is increased by a predetermined value. The algorithm repeats steps 153 to
    155 until a predetermined object is detected.
    [0074]           Once a predetermined object is detected, the algorithm ends in step 157
    and the planning system 100 proceeds to step 144 to perform volumetric analysis.
    During the volumetric analysis, the following properties of the spherical object may be
    calculated by controller 106: minimum diameter; maximum diameter; average diameter;
    volume; sphericity; minimum density; maximum density; and average density. The
    calculated properties may be displayed on display 110 as shown in region 139 of Figure
    7. The volumetric analysis may use a geometric filter to determine a minimum diameter,
    a maximum diameter, volume, elongation, surface area, and/or sphericity. An image
    intensity statistics filter may also be used in conjunction with the geometric filter in step
     144, The image intensity statistics filter calculates a minimum density, maximum
    density, and average density.
     [00751          In step 146, power and time settings are calculated for a demarcated
    target. Figure 10 depicts various graphs of the relation ship between energy deposited
    into tissue and the resulting ablation zone for a given time period. This relationship
    allows for inverse planning by considering the dimension and characteristics of a target
    tissue (i.e., tumors, fibroids, etc.) and the energy dose/antenna design of a specific
    ablation needle. Table 1 below shows an example of a relationship between ablation
    volume, power, and time for an ablation needle.
                                                   -18-

WO 2013/177051                                                                  PCT/US2013/041842
                                     Table 1
    Ablation Volume (cm 3)            Power (W)                     Time (s)
     6                                140
    22                                140                           3
     41                               140                           5
     31                               110                           5
     23                               80                            5
     [00761          Using the values in Table 1, a linear equation can be derived from the
     table to compute optimal power and time settings. For example, using a linear regression
     analysis, Table I provides the following equation:
     [0077]          Volume = 0.292381 * Power +8.685714 * Time - 44.0762                 (1)
     [0078]          which can be written as
      [00791         Power   = (Volume - 8.685714 * Time + 44.0762) / 0.292381.           (2)
      [00801         The desired volume can be calculated using the maximum diameter from
     the volumetric analysis plus a 1 centimeter margin as follows:
      [0081]         DesiredVolume = 4/3 * pi * DesiredRadius ^ 3          (3)
      [0082]         where the desired radius is calculated as follows:
      [0083]         DesiredRadius = MaxirumNoduleDiameter / 2 + Margin. (4)
      [0084]          Substituting the desired volume into equation (1) or (2) leaves two
      unknowns, power and time. Using equation (2) controller 106 can solve for power by
      substituting values for time. Controller 106 chooses the smallest value for time that
      maintains power below 70W, or some other predetermined value, so that the user can
      perform the procedure as quickly as possible while keeping power in a safe range.
                                                   -19-

WO 2013/177051                                                                  PCT/US2013/041842
     [0085]          Once the power and time are calculated 146, the power and time are
    displayed on display I 10 as shown in Figure 7 (see 133 and 135). A user can adjust the
    calculated power and/or time using controls 133 and 135, respectively, to adjust the
    treatment zone 138a and/or margin 138b.
     [00861          Memory 104 and/or controller 106 may store a number of equations that
     correspond to different surgical devices. When a user selects a different surgical devices
     in drop down menu 131, controller 106 can perform the same analysis described above to
     determine the smallest value for time that keeps the power below 70W or some other
     predetermined value.
     [00871          Although the above described procedure describes the use of a single seed
     point to determine a predetermined object, some targets may have an irregular shape that
     can not be treated by the predetermined treatment zone without causing damage to other
     tissue. In such instances, multiple seed points may be used to create an irregular shaped
     treatment plan using a single surgical device that is repositioned in a number of places or
     multiple surgical devices that may be used concurrently to treat an irregularly shaped
     region.
      [0088]         In other embodiments, memory 104 and/or controller 106 may store a
     catalog of surgical devices and treatment zone performance, which includes power, time,
     number of instruments, and spacing of instruments required to achieve treatment zones
     ex vivo or in vivo. Based on the results of the image segmentation and volumetric
     analysis, the controller may automatically select device types, numbers of devices,
     spacing of multiple devices, and/or power and time settings for each device to treat the
     ROI, Alternatively, a user can manually select device types, numbers of devices, spacing
      of multiple devices, power and/or time settings for each device to treat the ROI using the
      GUI to generate a treatment plan.
                                                 -20-

WO 2013/177051                                                                PCT/US2013/041842
    [0089]          In another embodiment according to the present disclosure, planning
    system 100 may also segment organs and other vital structures in addition to targets.
    Segmentation of organs and other structures, such as vessels, are used to provide a more
    advanced treatment plan, As described above with regard to Figure 10, treatment zones
    correlate to energy delivery in a regular fashion. Further, it is known that vessels greater
    than three (3) millimeters may negatively affect treatment zone formation. Segmentation
    of a vessel would allow the interaction between the vessels and the target to be
    estimated, including the vessel diameter (1D1) and distance (D2) (see Figure I IA)
    between the vessel and a proposed target, This interaction may be estimated manually
    by a user or automatically by controller 106, Using the vessel diameter Dl and the
    distance D2, planning system 100 may automatically suggest an alternate dose curve to
     be used for treatment purposes as shown in Figure 1lB. Alternatively, controller 106
    may provide a recommendation to the user via display 110 to move the treatment zone.
    Additionally, a different treatment zone projection could be displayed on display 110.
    Further, in the compute power and time settings step 146 of Figure 8, the controller could
     leverage different curves depending on the vessel's diameter and distance to the target
     area.
     [0090]          Figures 12A-12C depict an advanced treatment planning using organ
     segmentation. Segmentation of an organ allows for at least two advantages in planning a
     course of treatment. In a first instance, minimally invasive treatments are often chosen
     to be organ sparing. By segmenting the organ, controller 106 can calculate the organ
     volume 160 and subtract the determined ablation zone 162 to determine the volume of
     organ being spared 164 as shown in Figure 12A. If controller 106 determines that
     volume of organ being spared is too low, controller 106 may alert a user that an alternate
     treatment plan is needed or it may suggest an alternate treatment plan.
                                                  -21-

WO 2013/177051                                                                    PCT/US2013/041842
     [00911           Figures 12B and 12C depict a treatment plan for a target "T" located on
    the surface of an organ. Conventionally, treatment near an organ surface is often
    avoided or additional techniques may be required to separate the organ from other organs
    before treatment can be performed, In another embodiment in accordance with the
    present disclosure, after the organ is segmented, the position of a target "T" can also be
    determined. If the treatment zone 162 in the treatment plan projects outside the surface
    of the organ and the target "T" is located on the surface, controller 106 may alert the user
    that treatment zone 162 may affect other organs and/or structures in the vicinity of the
     target "T" and that the treatment plan needs to be altered. In another embodiment,
     controller 106 may automatically make recommendations to the user indicating the
     surgical device, energy level, duration of treatment. Controller 106 may also suggest a
     smaller treatment zone 162 as shown in Figure 12B or it may suggest moving the
     treatment zone 162 as shown in Figure 12C.
     [0092]           In other embodiments, after targets, tissues, organs, and other structures
     are segmented, known tissue properties can be attributed to these structures. Such tissue
     properties include, but are not limited to, electrical conductivity and permittivity across
     frequency, thermal conductivity, thermal convection coefficients, and so forth. The
     planning algorithm of Figure 8 may use the tissue properties attributed to the segmented
     tumors, tissues, organs, and other structures to solve the Pennes bioheat equation in order
     to calculate a dose required to ablate a selected target. Keys to successful
     implementation of this more comprehensive solution using the bioheat equation include:
     utilizing known tissue properties at steady-state to predict an initial spatial temperature
     profile, utilizing tissue properties as temperature rises to adjust spatial properties in
     accordance with temperature elevation, and utilizing tissue properties at liquid-gas phase
     transition.
                                                   -22-

WO 2013/177051                                                                  PCT/US2013/041842
    [0093]           Turning to Figure 13, a navigation system in accordance with an
    embodiment of the present disclosure is shown generally as 200. Generally, navigation
    system 200 incorporates a reference patch or fiducial patch 204 that is affixed to an
    ultrasound transducer 202. Fiducial patch 204 may be printed on ultrasound transducer
    202, attached to ultrasound transducer 202 via an adhesive, or removably coupled to
    ultrasound transducer 202. In some embodiments, the fiducial patch is disposed on a
    support structure that is configured to be removably affixed, e.g., "clipped onto", the
    housing of an ultrasound transducer, Ultrasound transducer 202 is coupled to an
    ultrasound generator 210 that generates acoustic waves. Ultrasound transducer 202 and
    ultrasound generator 210 may be incorporated into a standalone unit. Ultrasound
    transducer 202 emits the acoustic waves toward patient "P". The acoustic waves reflect
     off various structures in patient "P" and are received by ultrasound transducer 202.
    Ultrasound transducer 202 transm its the reflected acoustic waves to an ultrasound
     generator 210 that converts the reflected acoustic waves into a two dimensional (2D)
     image in real time. The 2D image is transmitted to a controller 212. Controller 212
     processes the 2D image and displays the 2D image as image 218 including target 220 on
     display 214. Image 218 is a real time representation of scan plane "S" which may
     include target "T".
     100941          The navigation system also incorporates a camera 208 affixed to an
     surgical device 206. The camera 208 captures an image of fiducial patch 204 in real time
     in order to determine the position of the surgical device 206 in relation to the scan plane
     "S". In particular, fiducial patch 204 has a defined spatial relationship to scan plane "S".
     This defined spatial relationship is stored in controller 212. Camera 208 also has a
     known spatial relationship to surgical device 206 that is stored in controller 212. In
     order to determine the spatial relationship between surgical device 206 and scan plane
                                                   -23-

WO 2013/177051                                                                   PCT/US2013/041842
    "S", camera 208 captures an image of fiducial patch 204 and transmits the image to
    controller 212. Using the image of the fiducial patch 204, controller 212 can calculate
    the spatial relationship between the surgical device 206 and the scan plane "S".
     [00951          After controller 212 determines the spatial relationship between the
    surgical device 206 and scan plane "S", controller 212 displays that relationship on
    display 214. As shown in Figure 13, display 214 includes an image 218 of scan plane
    "S" including a target image 220 of target "T". Additionally, controller 212
    superimposes a virtual image 206a of surgical device 206 in relation to image 218 to
    indicate the position of the surgical device 206 in relation to scan plane "S". Based on
    the angle and position of the ablation needle 206, controller 212 can calculate a trajectory
    of the surgical device 206 and display the calculated trajectory shown generally as 216.
    In some embodiments, a crosshair or target may be superimposed on image 218 to
     indicate where the surgical device 206 will intersect the scan plane "S". In other
     embodiments, the calculated trajectory 216 may be shown in red or green to indicate the
     navigation status. For instance, if surgical device 206 is on a path that will intersect
    target "T", calculated trajectory 216 will be shown in green. If surgical device 206 is not
     on a path that will intersect target "T", calculated trajectory 216 will be shown in red.
     [0096]          Controller 212 can also be controlled by a user to input the surgical device
     type, energy level, and treatment duration, The surgical device type, energy level, and
     treatment duration can be displayed on display 214 as shown in Figure 14A. When
     surgical device 206 intersects target "T", a virtual ablation zone 222 is projected onto
     image 218 as shown in Figure 14B. The energy level and treatment duration can then be
     adjusted by a user and the controller 212 will adjust the virtual ablation zone 222 to
     reflect the changes in the energy level and treatment duration.
                                                   -24-

WO 2013/177051                                                                 PCT/US2013/041842
     [0097]          The fiducial tracking system is described hereinbelow with reference to
    Figures 15-22. In the fiducial tracking system, controller 212 receives a fiducial image
    from camera 208. Controller 212 also includes camera calibration and distortion
    coefficients for camera 208, fiducial system models, and camera-antenna calibration data
    previously stored thereon. In other embodiments, camera calibration and distortion
    coefficients for camera 208, fiducial system models, and camera-antenna calibration data
    can be entered into controller 212 during a navigation procedure. Based on the fiducial
     image, camera calibration and distortion coefficients for camera 208, fiducial system
    models, and camera-antenna calibration data, controller 212 can output the position of
    ablation needle 206 to display 214 as well as diagnostic frame rate, residual error, and
    tracking status. In some embodiments, the distance between the camera 208 and the
    fiducial patch 204 may be in the range of about 5 to about 20 centimeters. In some
    embodiments, the distance between camera 208 and fiducial patch 204 may be in the
     range of about 1 to about 100 centimeters.
     [0098]          Figure 15 shows a basic flowchart.for the fiducial tracking algorithm
    employed by controller 212. As shown in Figure 15, an image flame is captured in step
    230. In step 231, controller 212 corrects for lens distortion using the camera calibration
     and distortion coefficients. Images captured by camera 208 may exhibit lens distortion
     as shown in Figure 16A. Thus, before an image can be used for further calculations, the
     image needs to be corrected for the distortion. Before camera 208 is used during a
     navigation procedure, camera 208 is used to take multiple images of a checkerboard
     pattern at various angles. The multiple images and various angles are used to create a
     camera matrix and distortion coefficients. Controller 212 then uses the camera matrix
     and distortion coefficients to correct for lens distortion.
                                                   -25-

WO 2013/177051                                                                    PCT/US2013/041842
     t0099]          In step 232, controller 212 finds the white circles in the image frame
     using the algorithm of Figure 17. As shown in Figure 17, the image frame received in
     step 241 (Figure 18A) is thresholded in step 243 using a dynamic threshold (see Figure
     18B). When using a dynamic threshold, after each valid frame, the dynamic threshold
    algorithm computes a new threshold for the next frame using the circles that were found
    in the valid frame. Using the circles that were found in the valid frame, controller 212
    calculates a new threshold based on equation (5) below:
     [0100]          threshold=(black circle intensityavemge + white circle intensityavemage)/ 2
    (5)
     [0101]          A predetermined threshold may be used to capture the initial valid frame
    which is then used to calculate a new threshold.
     [0102]          Alternatively, controller 212 may scan for an initial threshold by testing a
    range of threshold values until a threshold value is found that results in a valid frame.
    Once an initial threshold is found, controller 212 would use equation (5) for dynamic
    thresholding based on the valid frame.
     [0103]           In other embodiments, a fixed threshold may be used. The fixed
    threshold may be a predetermined number stored in controller 212 or it may be
    determined by testing the range of threshold values until a threshold value is found that
    results in a valid frame.
    [0104]           After a threshold and automatic gain control is applied to the image, a
    connected component analysis is performed in step 244 to find all the objects in the
    thresholded image. A geometric filter is applied to the results of the connected
    component analysis and the image frame in step 245. The geometric filter computes the
    size and shape of the objects and keeps only those objects that are circular and about the
                                                  -26-

WO 2013/177051                                                                 PCT/US2013/041842
    right size as shown in Figure 18C. Weighted centroids are computed and stored for all
    the circular objects.
    [0105]           Turning back to Figure 15, in addition to finding the white circles in step
    232, controller 212 also finds the black circles in step 233 using the algorithm depicted
    in Figure 19. The algorithm for finding the black circles is similar to the algorithm
    shown in Figure 17 for finding the white circles. In order to find the black circles, after
    an image frame is received in step 241 (see Figure 20A), controller 212 inverts the
    intensities of the image frame in step 242 as shown in Figure 20B. Then, as described
    above with regard to Figure 17, the image is thresholded as shown in Figure 20C and the
    connected component analysis is performed and geometric filter is applied to obtain the
    image shown in Figure 20D. The weighted centroids are computed and stored for all the
    black circles in step 248. Further, in step 245, controller 212 applies a geometric filter to
    determine the black regions in addition to the black circles in the image frame.
    Controller 212 stores the determined black regions in step 249.
     [0106]          In step 234 of Figure 15, controller 212 finds a correspondence between
    the fiducial image and fiducial models using the algorithm of shown in Figure 21A. In
    step 251 of Figure 21A, controller 212 uses a topology constraint to select the four white
    circles as shown in Figure 21B. As shown in Figure 21B, in step 261, controller 212
    obtains the black regions stored in step 249 of Figure 19 and obtains the white circles
    stored in step 246 of Figure 17. Controller 212 then selects a first black region in step
    263 and counts the number of white circles in the first black region in step 264.
     Controller 212 determines whether the number of circles in the selected black region
    matches a predetermined number of circles in step 265. If the number of circles does not
     match the predetermined number of circles, the algorithm proceeds to step 266 where the
     next black region is selected and the number of circles in the next black region is counted
                                                 -27-

WO 2013/177051                                                                   PCT/US2013/041842
     again in step 264. This process repeats until the number of circles counted in step 264
     matches the predetermined number of circles. Once the number of circles counted in
     step 264 matches the predetermined number of circles, the algorithm proceeds to step
     267 where the topology constraint algorithm is completed. In other embodiments,
     controller 212 selects the four white circles by selecting the four roundest circles.
     [0107]           After the four circles are chosen, they are arranged in a clockwise order
     using a convex hull algorithm in step 252. The convex hull or convex envelope for a set
     of points X in a real vector space V is the minimal convex set containing X. If the points
     are all on a line, the convex hull is the line segmentjoining the outermost two points. In
     the planar case, the convex hull is a convex polygon unless all points are on the same
     line. Similarly, in three dimensions the convex hull is in general the minimal convex
     polyhedron that contains all the points in the set. In addition, the four matching fiducials
     in the model are also arranged in a clockwise order.
     10108]           In step 253, a planar homography matrix is computed. After a planar
     homography matrix is calculated, the homography matrix is used to transform the
     fiducial models to image coordinates using the four corresponding fiducial models
     shown in Figure 22 to find the closest matching image fiducials (steps 254 and 255).
     Controller 212 also computes the residual error in step 256. The algorithm uses the
     resulting 3D transform to transform the 3D fiducial model into the 2D image. It then
     compares the distances between fiducials mapped into the 2D image with the fiducials
     detected in the 2D image. The residual error is the average distance in pixels. This error
     is used to verify accuracy and partly determine the red/green navigation status.
     Controller 212 then selects the model with the most matches and the smallest residual
     error. In order for a more accurate result, there has to be a minimum number of black
     fiducial matches (e.g., three).
                                                    -28-

WO 2013/177051                                                                  PCT/US2013/041842
     [01091          In step 235 of Figure I 5, camera pose estimation is performed. The
    camera pose estimation involves calculating a 3D transform between the camera and the
    selected model by iteratively transforming the model fiducials onto the fiducial image
    plane and minimizing the residual error in pixels. The goal is to find the global
    minimum of the error function. One problem that may occur is the occurrence of
    significant local minima (e.g., an antenna imaged from the left looks similar to an
    antenna imaged from the right) in the error function that needs to be avoided. Controller
    212 avoids the local minima by performing minimization from multiple starting points
     and choosing the result with the smallest error. Once the 3D transform is calculated, the
     controller can use the 3D transform to transform the coordinates of the surgical device
    206 to a model space and display the surgical device 206 as virtual surgical device 206a
     in display 214.
     [01101          Because object boundaries expand and contract under different lighting
     conditions, a conventional square corner fiducials location may change depending on
     lighting conditions. Fiducial patch 204 uses black and white circles, and, thus, is not
     hampered by this problem because the center of the circle always stays, the same and
     continues to work well for computing weighted centroids. Other contrasting images or
     colors are also contemplated.
     101111          In another embodiment of the present disclosure, and as shown in Figure
     23, a planning and navigation system 300 is provided. System 300 includes planning
     system 302 and navigation system 304 that are connected to a controller 306. Controller
     306 is connected to a display 308 that may include a single display screen or multiple
     display screens (e.g., two display screens). Planning system 302 is similar to planning
     system 100 and navigation system 304 is similar to navigation system 200. In system
     300, display 308 displays the planning operation and navigation operation described
                                                  -29-

WO 2013/177051                                                                PCT/US2013/041842
    hereinabove. The planning operation and the navigation operation may be displayed as a
    split screen arrangement on a single display screen, the planning operation and the
    navigation operation may be displayed on separate screens, or the planning operation and
    the navigation operation may be displayed the same screen and a user may switch
    between views. Controller 306 may import dose settings from the planning system and
    use the dose setting during a navigation operation to display the ablation zone
    dimensions.
     [0112]          In other embodiments of the present disclosure, CT navigation and
     software can be integrated with planning system 100. Turning to Figures 24, 25A, and
    25B a planning and navigation system is shown generally as 400. System 400 includes
     an image capturing device 402 that captures CT images of a patient "P" having an
     electromagnetic reference 428 and/or optical reference 438. The CT images are provided
     in DICOM format to planning system 404 that is similar to planning system 100.
    Planning system 400 is used to determine a treatment plan as described above and the
     treatment plan is provided to controller 408 and displayed as a planning screen 412 on
     display 410 as shown in Figure 26.
     [01131          Navigation system 406 may use an electromagnetic tracking system as
     shown in Figure 25A, an infrared tracking system or an optical tracking system as shown
     in Figure 25B. Turning to Figure 25A, a navigation system 420 includes an
     electromagnetic field generator 422, an surgical device 424 having an electromagnetic
     transducer 426, and an electromagnetic reference 428 disposed on the patient. The field
     generator 422 emits electromagnetic waves which are detected by electromagnetic
     sensors (not explicitly shown) on the surgical device 424 and electromagnetic reference
     428 and then used to calculate the spatial relationships between surgical device 424 and
     electromagnetic reference 428. The spatial relationships may be calculated by the field
                                                  -30-

WO 2013/177051                                                                  PCT/US2013/041842
     generator 422 or the field generator 422 may provide the data to controller 408 to
     calculate the spatial relationship between the ablation needle 424 and the electromagnetic
     reference 428.
     [0114]          Figure 25B depicts an alternate navigation system 430 that is similar to
     the navigation system described in Figure 13 above. In Figure 25B, an optical reference
     or fiducials 438 is placed on a patient. A camera 436 attached to surgical device 424
     takes an image of the fiducials 438 and transmits the image to controller 408 to
     determine a position of the ablation needle in relation to the fiducials 438.
     [0115]          After receiving data from navigation system 406, controller 408 may
     correlate the position of the surgical device 424 with the CT images in order to navigate
     the surgical device 424 to a target "T" as described below. In this case, the patient
     reference (of any type) may have radiopaque markers on it as well to allow visualization
     during CT. This allows the controller to connect the patient CT image coordinate system
     to the instrument tracking coordinate system.
     101161          Controller 408 and display 410 cooperate with each other to display the
     CT images on a navigation screen 440 as shown in Figure 27. As shown in Figure 27,
     display screen 440 includes a transverse view 442, coronal view 444, and sagittal view
     446. Each view includes a view of the target "T" and an ablation zone 452 (including a
     margin). The transverse view 442, coronal view 444 and sagittal view 446, ablation zone
     452 are all imported from planning system 404. Additionally, all planning elements
     (e.g., device selection, energy level, and treatment duration) are automatically transferred
     to the navigation screen 440. The navigation screen 440 is also a graphical user interface
     that allows a user to adjust the device selection, energy level, and treatment duration.
     [0117]          A navigation guide screen 448 is provided on display screen 440 to assist
     in navigating the ablation needle to the target "T". Based on the data received from the
                                                   -31-

WO 2013/177051                                                                   PCT/US2013/041842
     navigation system 406, the controller can determine if the surgical device 424 is aligned
    with target "T". If the surgical device 424 is not aligned with target "T", the circle 454
     would be off-centered from outer circle 453. The user would then adjust the angle of
     entry for the surgical device 424 until the center of circle 454 is aligned with the center
     of outer circle 453. In some embodiments, circle 454 may be displayed as a red circle
     when the center of circle 454 is not aligned with the center of outer circle 453 or circle
     454 may be displayed as a green circle when the center of circle 454 is aligned with the
     center of outer circle 453. Additionally, controller 408 may calculate the distance
     between the target "T" and the surgical device 424.
     [01181          In another embodiment depicted in Figure 28, controller 408
     superimposes a virtual surgical device 424a over a 3D rendered image and displays the
     combined image on screen 462. Similar to the method described above, a user can align
     the center of circle 453 with the center of circle 454 to navigate the surgical device 424
     to the target "T". Alternatively, the user can determine the position of surgical device
     424 in relation to the target "T" by viewing virtual surgical device 424a on screen 462 to
     navigate the surgical device 424 to the target "T".
     [0119]           Figure 29 depicts another embodiment of the present disclosure.
     Similarly to screen 462 above, in the embodiment of Figure 29, screen 472 depicts a
     virtual surgical device 424a in spatial relationship to previously acquired and rendered
     CT image. The CT image has been volume rendered to demarcate the target "T" as well
     as additional structures, vessels, and organs. By volume rendering the target "T", as well
     as the additional structures, vessels, and organs, the user can navigate the surgical device
     424 into the patient while also avoiding the additional structures, vessels, and organs to
     prevent unnecessary damage.
                                                   -32-

WO 2013/177051                                                                 PCT/US2013/041842
     [0120]         It should be understood that the foregoing description is only illustrative
    of the present disclosure. Various alternatives and modifications can be devised by those
    skilled in the art without departing from the disclosure. Accordingly, the present
    disclosure is intended to embrace all such alternatives, modifications and variances. The
    embodiments described vith reference to the attached drawing figures are presented only
    to demonstrate certain examples of the disclosure. Other elements, steps, methods and
    techniques that are insubstantially different from those described above and/or in the
    appended claims are also intended to be within the scope of the disclosure.
                                                 -33-

                                                  34
CLAIMS:
1. A planning system comprising:
        a receiver configured to receive a plurality of images of a target area;
        a controller configured to segment at least one vessel and at least one object in the
plurality of images of the target area, compute a proximity of the at least one vessel to the at
least one object, determine a treatment plan including an energy level and a treatment duration
based on the proximity of the at least one vessel to the at least one object;
        an input device configured to adjust at least one of the energy level or the treatment
duration of the treatment plan; and
        a display configured to display a rendering of the plurality of images of the target area.
2. The planning system of claim 1, wherein the controller is further configured to apply a
geometric filter to compute a size of the at least one object in the target area.
3. The planning system of claim 1, wherein the controller is further configured to segment at
least one organ in the plurality of images of the target area.
4. The planning system of claim 3, wherein the controller is further configured to compute a
position of the at least one object in relation to the at least one organ.
5. The planning system of claim 4, wherein the controller is further configured to adjust at
least one of the energy level or the treatment duration of the treatment plan based on the
position of the at least one object in relation to the at least one organ.
6. The planning system of claim 1, wherein the display provides a graphical user interface.

                                                 35
7. The planning system of claim 1, wherein the controller is further configured to:
        select a seed point;
        create a region of interest around the seed point;
        compare a first plurality of pixels in the region of interest to a predetermined
threshold;
        select a second plurality of pixels from the first plurality of pixels, wherein the second
plurality of pixels is connected to the seed point and is less than the predetermined threshold;
and
        apply a geometric filter to the second plurality of pixels.
8. The planning system of claim 7, wherein the controller is further configured to:
        determine if the second plurality of pixels forms a predetermined object, wherein if the
second plurality of pixels does not form the predetermined object, the predetermined
threshold is increased.
9. The planning system of claim 1, wherein the controller is further configured to:
        receive a selection of a surgical device; and
        calculate an energy level and a treatment duration based on the target area and the
selected surgical device.
10. The planning system of claim 1, wherein the plurality of images is one of computed
tomography images or ultrasound images.
11. A method of creating a treatment plan, the method comprising:

                                                 36
        receiving a plurality of images of a target area;
        segmenting at least one vessel and at least one object in the plurality of images of the
target area;
        computing a proximity of the at least one vessel to the at least one object;
        determining a treatment plan including an energy level and a treatment duration based
on the proximity of the at least one vessel to the at least one object;
        adjusting at least one of the energy level or the treatment duration of the treatment
plan; and
        displaying a rendering of the plurality of images of the target area.
12. The method of claim 11, further comprising applying a geometric filter to compute a size
of the at least one object in the target area.
13. The method of claim 11, further comprising segmenting at least one organ in the plurality
of images of the target area.
14. The method of claim 13, further comprising computing a position of the at least one
object in relation to the at least one organ.
15. The method of claim 14, further comprising adjusting at least one of the energy level or
the treatment duration of the treatment plan based on the position of the at least one object in
relation to the at least one organ.
16. The method of claim 11, wherein the display provides a graphical user interface.

                                                 37
17. The method of claim 11, further comprising:
        selecting a seed point;
        creating a region of interest around the seed point;
        comparing a first plurality of pixels in the region of interest to a predetermined
threshold;
        selecting a second plurality of pixels from the first plurality of pixels, wherein the
second plurality of pixels is connected to the seed point and is less than the predetermined
threshold; and
        applying a geometric filter to the second plurality of pixels.
18. The method of claim 17, further comprising:
        determining if the second plurality of pixels forms a predetermined object, wherein if
the second plurality of pixels does not form the predetermined object, the predetermined
threshold is increased.
19. The method of claim 11, further comprising:
        receiving a selection of a surgical device; and
        calculating an energy level and a treatment duration based on the target area and the
selected surgical device.
20. The method of claim 11, wherein the plurality of images is one of computed tomography
images or ultrasound images.
                                          Covidien LP
                 Patent Attorneys for the Applicant/Nominated Person
                                  SPRUSON & FERGUSON

<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
<removed-apn> <removed-date>
